<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Functions · PDMO.jl Documentation</title><meta name="title" content="Functions · PDMO.jl Documentation"/><meta property="og:title" content="Functions · PDMO.jl Documentation"/><meta property="twitter:title" content="Functions · PDMO.jl Documentation"/><meta name="description" content="Documentation for PDMO.jl Documentation."/><meta property="og:description" content="Documentation for PDMO.jl Documentation."/><meta property="twitter:description" content="Documentation for PDMO.jl Documentation."/><meta property="og:url" content="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/functions/"/><meta property="twitter:url" content="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/functions/"/><link rel="canonical" href="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/functions/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PDMO.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../S1_getting_started/">Getting Started</a></li><li><span class="tocitem">Algorithms</span><ul><li><a class="tocitem" href="../../S2_algorithms/AdaPDM/">AdaPDM</a></li><li><a class="tocitem" href="../../S2_algorithms/ADMM/">ADMM</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../S3_examples/DistributedOPF/">Distributed OPF</a></li><li><a class="tocitem" href="../../S3_examples/DualLasso/">Dual Lasso</a></li><li><a class="tocitem" href="../../S3_examples/DualSVM/">Dual SVM</a></li><li><a class="tocitem" href="../../S3_examples/FusedLasso/">Fused Lasso</a></li><li><a class="tocitem" href="../../S3_examples/LeastL1Norm/">Least L1 Norm</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../main/">Main</a></li><li><a class="tocitem" href="../formulations/">Formulations</a></li><li class="is-active"><a class="tocitem" href>Functions</a><ul class="internal"><li><a class="tocitem" href="#Abstract-Function-Interface"><span>Abstract Function Interface</span></a></li><li><a class="tocitem" href="#First-Order-Oracles"><span>First Order Oracles</span></a></li><li><a class="tocitem" href="#Functions-implemented-in-PDMO.jl"><span>Functions implemented in <code>PDMO.jl</code></span></a></li></ul></li><li><a class="tocitem" href="../mappings/">Mappings</a></li><li><a class="tocitem" href="../admm/">ADMM</a></li><li><a class="tocitem" href="../pdm/">AdaPDM</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>Functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Functions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/alibaba-damo-academy/PDMO.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/main/docs/src/S4_api/functions.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h1><p>This page documents the function types used in the PDMO optimization framework.</p><h2 id="Abstract-Function-Interface"><a class="docs-heading-anchor" href="#Abstract-Function-Interface">Abstract Function Interface</a><a id="Abstract-Function-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Abstract-Function-Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AbstractFunction" href="#PDMO.AbstractFunction"><code>PDMO.AbstractFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractFunction</code></pre><p>Abstract base type for all functions in the Bipartization optimization framework.</p><p>This type serves as the foundation for implementing mathematical functions that can be used in optimization algorithms. It defines a common interface for function evaluation,  gradients, and proximal operators.</p><p><strong>Interface Requirements</strong></p><p>To implement a new function type, you must:</p><ol><li><strong>Define a concrete struct</strong> that inherits from <code>AbstractFunction</code></li><li><strong>Implement function evaluation</strong> by overriding the call operator</li><li><strong>Implement trait methods</strong> to specify function properties</li><li><strong>Implement oracles</strong> for smooth/proximal functions as needed</li></ol><p><strong>Core Interface Methods</strong></p><p><strong>Function Evaluation</strong></p><pre><code class="language-julia hljs">(f::YourFunction)(x::NumericVariable, enableParallel::Bool=false) -&gt; Float64</code></pre><p><strong>Trait Methods (override as needed)</strong></p><pre><code class="language-julia hljs">isSmooth(::Type{YourFunction}) = true/false     # Has gradient?
isProximal(::Type{YourFunction}) = true/false   # Has proximal operator?
isConvex(::Type{YourFunction}) = true/false     # Is convex?
isSet(::Type{YourFunction}) = true/false        # Is indicator function?</code></pre><p><strong>Gradient Oracle (if isSmooth = true)</strong></p><pre><code class="language-julia hljs">gradientOracle!(grad::NumericVariable, f::YourFunction, x::NumericVariable, enableParallel::Bool=false)
gradientOracle(f::YourFunction, x::NumericVariable, enableParallel::Bool=false)</code></pre><p><strong>Proximal Oracle (if isProximal = true)</strong></p><pre><code class="language-julia hljs">proximalOracle!(y::NumericVariable, f::YourFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)
proximalOracle(f::YourFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)</code></pre><p><strong>Implementation Example</strong></p><pre><code class="language-julia hljs"># Define a simple quadratic function: f(x) = (1/2)||x||²
struct SimpleQuadratic &lt;: AbstractFunction
    # No fields needed for this simple case
end

# Specify traits
isSmooth(::Type{SimpleQuadratic}) = true
isConvex(::Type{SimpleQuadratic}) = true
isProximal(::Type{SimpleQuadratic}) = true

# Function evaluation
function (f::SimpleQuadratic)(x::NumericVariable, enableParallel::Bool=false)
    return 0.5 * sum(x.^2)
end

# Gradient oracle
function gradientOracle!(grad::NumericVariable, f::SimpleQuadratic, x::NumericVariable, enableParallel::Bool=false)
    grad .= x
end

# Proximal oracle
function proximalOracle!(y::NumericVariable, f::SimpleQuadratic, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)
    y .= x ./ (1.0 + gamma)
end</code></pre><p><strong>Built-in Function Types</strong></p><p>The framework provides many built-in function types:</p><ul><li><strong>Basic Functions</strong>: <code>Zero</code>, <code>AffineFunction</code>, <code>QuadraticFunction</code></li><li><strong>Norms</strong>: <code>ElementwiseL1Norm</code>, <code>FrobeniusNormSquare</code>, <code>MatrixNuclearNorm</code></li><li><strong>Indicators</strong>: <code>IndicatorBox</code>, <code>IndicatorBallL2</code>, <code>IndicatorSOC</code>, <code>IndicatorPSD</code></li><li><strong>Wrappers</strong>: <code>WrapperScalingTranslationFunction</code>, <code>WrapperScalarInputFunction</code></li><li><strong>User-defined</strong>: <code>UserDefinedSmoothFunction</code>, <code>UserDefinedProximalFunction</code></li></ul><p><strong>Design Principles</strong></p><ol><li><strong>Modularity</strong>: Each function type is self-contained</li><li><strong>Efficiency</strong>: Support for in-place operations and parallel computation</li><li><strong>Flexibility</strong>: Trait-based design allows algorithm selection</li><li><strong>Extensibility</strong>: Easy to add new function types</li><li><strong>Type Safety</strong>: Strong typing helps catch errors early</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L1-L94">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.NumericVariable" href="#PDMO.NumericVariable"><code>PDMO.NumericVariable</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NumericVariable</code></pre><p>Type alias for function arguments in the Bipartization framework.</p><p>This type represents the domain of functions, encompassing both scalar and array inputs. It provides a unified interface for working with optimization variables of different dimensions and structures.</p><p><strong>Definition</strong></p><pre><code class="language-julia hljs">NumericVariable = Union{Float64, AbstractArray{Float64, N} where N}</code></pre><p><strong>Supported Types</strong></p><ul><li><strong>Scalar</strong>: <code>Float64</code> - Single real number</li><li><strong>Vector</strong>: <code>Vector{Float64}</code> - 1D array of real numbers  </li><li><strong>Matrix</strong>: <code>Matrix{Float64}</code> - 2D array of real numbers</li><li><strong>Tensor</strong>: <code>Array{Float64, N}</code> - N-dimensional array of real numbers</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Scalar variable
x_scalar::NumericVariable = 3.14

# Vector variable
x_vector::NumericVariable = [1.0, 2.0, 3.0]

# Matrix variable
x_matrix::NumericVariable = [1.0 2.0; 3.0 4.0]

# 3D tensor variable
x_tensor::NumericVariable = rand(2, 3, 4)</code></pre><p><strong>Usage in Functions</strong></p><p>Functions that accept <code>NumericVariable</code> arguments can handle different input types:</p><pre><code class="language-julia hljs">function (f::MyFunction)(x::NumericVariable, enableParallel::Bool=false)
    if isa(x, Float64)
        # Handle scalar case
        return x^2
    else
        # Handle array case
        return sum(x.^2)
    end
end</code></pre><p><strong>Design Rationale</strong></p><ul><li><strong>Flexibility</strong>: Supports various optimization variable structures</li><li><strong>Type Safety</strong>: Ensures only numeric types are used</li><li><strong>Performance</strong>: Avoids unnecessary type conversions</li><li><strong>Compatibility</strong>: Works with Julia&#39;s array ecosystem</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L97-L153">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.isSmooth" href="#PDMO.isSmooth"><code>PDMO.isSmooth</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">isSmooth(f::AbstractFunction) -&gt; Bool
isSmooth(::Type{&lt;:AbstractFunction}) -&gt; Bool</code></pre><p>Trait checker for the smooth (differentiable) property.</p><p>Returns <code>true</code> if the function is smooth (differentiable) everywhere in its domain, <code>false</code> otherwise. Smooth functions can be used in gradient-based optimization algorithms like gradient descent, Newton&#39;s method, and quasi-Newton methods.</p><p><strong>Mathematical Background</strong></p><p>A function f is smooth if it is continuously differentiable, meaning:</p><ul><li>The gradient ∇f(x) exists at every point x in the domain</li><li>The gradient is continuous</li></ul><p><strong>Implementation Requirements</strong></p><p>If <code>isSmooth(f) = true</code>, then the function must implement:</p><ul><li><code>gradientOracle!(grad, f, x)</code>: In-place gradient computation</li><li><code>gradientOracle(f, x)</code>: Allocating gradient computation (has default implementation)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Check if a function is smooth
f = QuadraticFunction(Q, q, r)
isSmooth(f)  # Returns true

# Use in algorithm selection
if isSmooth(f)
    # Use gradient-based algorithm
    grad = gradientOracle(f, x)
    x_new = x - α * grad  # Gradient descent step
else
    # Use derivative-free algorithm
    x_new = proximalOracle(f, x, γ)  # If proximal is available
end</code></pre><p><strong>Built-in Smooth Functions</strong></p><ul><li><code>QuadraticFunction</code>: Gradient is linear</li><li><code>AffineFunction</code>: Gradient is constant</li><li><code>ComponentwiseExponentialFunction</code>: Gradient is exponential</li><li><code>Zero</code>: Gradient is zero</li><li><code>FrobeniusNormSquare</code>: Gradient is linear in matrix case</li></ul><p><strong>Non-smooth Functions</strong></p><ul><li><code>ElementwiseL1Norm</code>: Not differentiable at zero</li><li>Most indicator functions: Not differentiable on boundaries</li><li><code>MatrixNuclearNorm</code>: Not differentiable when singular values are zero</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L202-L251">source</a></section><section><div><pre><code class="language-julia hljs">isSmooth(f::WrapperScalingTranslationFunction)</code></pre><p>Check if the wrapped function is smooth (differentiable). Delegates to the original function&#39;s smoothness property.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L92-L97">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.isProximal" href="#PDMO.isProximal"><code>PDMO.isProximal</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">isProximal(f::AbstractFunction) -&gt; Bool
isProximal(::Type{&lt;:AbstractFunction}) -&gt; Bool</code></pre><p>Trait checker for the proximal operator capability.</p><p>Returns <code>true</code> if the function has an implemented proximal operator, <code>false</code> otherwise. Functions with proximal operators can be used in proximal algorithms like ADMM, forward-backward splitting, and Douglas-Rachford splitting.</p><p><strong>Mathematical Background</strong></p><p>The proximal operator of a function f is defined as:</p><pre><code class="nohighlight hljs">prox_{γf}(x) = argmin_z { f(z) + (1/(2γ))||z - x||² }</code></pre><p><strong>Implementation Requirements</strong></p><p>If <code>isProximal(f) = true</code>, then the function must implement:</p><ul><li><code>proximalOracle!(y, f, x, γ)</code>: In-place proximal operator</li><li><code>proximalOracle(f, x, γ)</code>: Allocating proximal operator (has default implementation)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Check if a function has proximal operator
f = ElementwiseL1Norm()
isProximal(f)  # Returns true

# Use in algorithm selection
if isProximal(f)
    # Use proximal algorithm
    result = proximalOracle(f, x, γ)
else
    # Use different algorithm approach
    result = gradientOracle(f, x)
end</code></pre><p><strong>Built-in Proximal Functions</strong></p><ul><li><code>ElementwiseL1Norm</code>: Soft thresholding</li><li><code>IndicatorBox</code>: Projection onto box constraints</li><li><code>IndicatorBallL2</code>: Projection onto L2 ball</li><li><code>Zero</code>: Identity operator</li><li>Many indicator functions: projections onto constraint sets</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L155-L199">source</a></section><section><div><pre><code class="language-julia hljs">isProximal(f::WrapperScalingTranslationFunction)</code></pre><p>Check if the wrapped function has a proximal operator. Delegates to the original function&#39;s proximal property.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L84-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.isConvex" href="#PDMO.isConvex"><code>PDMO.isConvex</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">isConvex(f::AbstractFunction) -&gt; Bool
isConvex(::Type{&lt;:AbstractFunction}) -&gt; Bool</code></pre><p>Trait checker for the convex property.</p><p>Returns <code>true</code> if the function is convex, <code>false</code> otherwise. Convex functions have many desirable properties for optimization, including the guarantee that any local minimum is also a global minimum.</p><p><strong>Mathematical Background</strong></p><p>A function f is convex if for all x, y in its domain and λ ∈ [0,1]:</p><pre><code class="nohighlight hljs">f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)</code></pre><p><strong>Properties of Convex Functions</strong></p><ul><li>Any local minimum is a global minimum</li><li>The set of global minimizers forms a convex set</li><li>First-order optimality conditions are sufficient</li><li>Many efficient optimization algorithms are guaranteed to converge</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Check if a function is convex
f = ElementwiseL1Norm()
isConvex(f)  # Returns true

# Use in algorithm selection
if isConvex(f)
    # Use convex optimization algorithm
    # Global optimality guarantees apply
else
    # Use general nonlinear optimization
    # Local optimality only
end</code></pre><p><strong>Built-in Convex Functions</strong></p><ul><li><strong>Norms</strong>: <code>ElementwiseL1Norm</code>, <code>FrobeniusNormSquare</code></li><li><strong>Indicators</strong>: All indicator functions of convex sets</li><li><strong>Basic</strong>: <code>Zero</code>, <code>AffineFunction</code></li><li><strong>Quadratic</strong>: <code>QuadraticFunction</code> (if positive semidefinite)</li><li><strong>Exponential</strong>: <code>ComponentwiseExponentialFunction</code></li></ul><p><strong>Non-convex Functions</strong></p><ul><li>Some user-defined functions</li><li><code>MatrixNuclearNorm</code> with different weights</li><li>Functions with non-convex constraints</li></ul><p><strong>Algorithm Implications</strong></p><ul><li>Convex functions enable global convergence guarantees</li><li>Specialized convex optimization algorithms can be used</li><li>Duality theory applies for convex functions</li><li>Efficient solution methods are available</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L254-L310">source</a></section><section><div><pre><code class="language-julia hljs">isConvex(f::WrapperScalingTranslationFunction)</code></pre><p>Check if the wrapped function is convex. Delegates to the original function&#39;s convexity property.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L100-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.isSet" href="#PDMO.isSet"><code>PDMO.isSet</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">isSet(f::AbstractFunction) -&gt; Bool
isSet(::Type{&lt;:AbstractFunction}) -&gt; Bool</code></pre><p>Trait checker for indicator functions of sets.</p><p>Returns <code>true</code> if the function is the indicator function of a set, <code>false</code> otherwise. Indicator functions are fundamental in constrained optimization and represent constraints as functions that are 0 inside the constraint set and ∞ outside.</p><p><strong>Mathematical Background</strong></p><p>An indicator function of a set S is defined as:</p><pre><code class="nohighlight hljs">I_S(x) = 0    if x ∈ S
I_S(x) = +∞   if x ∉ S</code></pre><p><strong>Properties of Indicator Functions</strong></p><ul><li>Always convex if the underlying set is convex</li><li>The proximal operator is the projection onto the set</li><li>Used to represent constraints in optimization problems</li><li>Enable conversion between constrained and unconstrained formulations</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Check if a function is an indicator function
f = IndicatorBox([-1.0, -1.0], [1.0, 1.0])
isSet(f)  # Returns true

# Use in constraint handling
if isSet(f)
    # This represents a constraint
    # Proximal operator is projection onto the set
    projected_x = proximalOracle(f, x)
else
    # This is a regular objective function
    function_value = f(x)
end</code></pre><p><strong>Built-in Indicator Functions</strong></p><ul><li><strong>Box constraints</strong>: <code>IndicatorBox</code></li><li><strong>Ball constraints</strong>: <code>IndicatorBallL2</code></li><li><strong>Cone constraints</strong>: <code>IndicatorSOC</code>, <code>IndicatorRotatedSOC</code></li><li><strong>Matrix constraints</strong>: <code>IndicatorPSD</code></li><li><strong>Linear constraints</strong>: <code>IndicatorLinearSubspace</code>, <code>IndicatorHyperplane</code></li><li><strong>Orthant constraints</strong>: <code>IndicatorNonnegativeOrthant</code></li><li><strong>Custom constraints</strong>: <code>IndicatorSumOfNVariables</code></li></ul><p><strong>Relationship to Proximal Operators</strong></p><p>For indicator functions, the proximal operator is the projection:</p><pre><code class="nohighlight hljs">prox_{γI_S}(x) = Proj_S(x) = argmin_{y∈S} ||y - x||²</code></pre><p><strong>Algorithm Applications</strong></p><ul><li><strong>Constrained optimization</strong>: Represent feasible regions</li><li><strong>Projection methods</strong>: Direct projection onto constraint sets</li><li><strong>Penalty methods</strong>: Soft constraint handling</li><li><strong>ADMM</strong>: Splitting methods for constrained problems</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L313-L374">source</a></section><section><div><pre><code class="language-julia hljs">isSet(f::WrapperScalingTranslationFunction)</code></pre><p>Check if the wrapped function is an indicator function of a set. Delegates to the original function&#39;s set property.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L108-L113">source</a></section></article><h2 id="First-Order-Oracles"><a class="docs-heading-anchor" href="#First-Order-Oracles">First Order Oracles</a><a id="First-Order-Oracles-1"></a><a class="docs-heading-anchor-permalink" href="#First-Order-Oracles" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.proximalOracle" href="#PDMO.proximalOracle"><code>PDMO.proximalOracle</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">proximalOracle(f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false) -&gt; NumericVariable</code></pre><p>Computes the proximal operator for function f, returning a new array.</p><p>This is the allocating version of the proximal operator that creates and returns a new array containing the result. For performance-critical applications, consider using the in-place version <code>proximalOracle!</code> instead.</p><p><strong>Arguments</strong></p><ul><li><code>f::AbstractFunction</code>: The function for which to compute the proximal operator</li><li><code>x::NumericVariable</code>: The input point</li><li><code>gamma::Float64=1.0</code>: The proximal parameter γ &gt; 0</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Returns</strong></p><ul><li><code>NumericVariable</code>: The result prox_{γf}(x), same type and size as input <code>x</code></li></ul><p><strong>Mathematical Definition</strong></p><p>Computes: prox<em>{γf}(x) = argmin</em>z { f(z) + (1/(2γ))||z - x||² }</p><p><strong>Implementation Notes</strong></p><p>The default implementation:</p><ol><li>Allocates a new array <code>y = similar(x)</code></li><li>Calls <code>proximalOracle!(y, f, x, gamma, enableParallel)</code></li><li>Returns the result</li></ol><p>Concrete function types may override this method for more efficient implementations, but most can rely on the default implementation.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Basic usage - returns new array
f = ElementwiseL1Norm(0.5)
x = [2.0, -3.0, 0.3]
result = proximalOracle(f, x, 1.0)  # Returns [1.5, -2.5, 0.0]

# Chaining operations
f1 = IndicatorBox([-1.0, -1.0], [1.0, 1.0])
f2 = ElementwiseL1Norm(0.1)
x = [2.0, -2.0]
result = proximalOracle(f2, proximalOracle(f1, x))  # Compose operations

# Different input types
f_matrix = IndicatorPSD(3)
X = rand(3, 3)
Y = proximalOracle(f_matrix, X)  # Returns projected matrix

# Scalar inputs (for appropriate functions)
f_scalar = ElementwiseL1Norm(1.0)
x_scalar = 2.0
result_scalar = proximalOracle(f_scalar, x_scalar)  # Returns 1.0</code></pre><p><strong>Performance Considerations</strong></p><ul><li><strong>Memory allocation</strong>: Creates new array on each call</li><li><strong>Prefer in-place version</strong>: Use <code>proximalOracle!</code> for better performance</li><li><strong>Temporary arrays</strong>: Consider pre-allocating arrays for repeated calls</li><li><strong>Memory pressure</strong>: May cause garbage collection in tight loops</li></ul><p><strong>Algorithm Applications</strong></p><ul><li><strong>Proximal gradient methods</strong>: Forward-backward splitting</li><li><strong>ADMM</strong>: Alternating direction method of multipliers</li><li><strong>Douglas-Rachford</strong>: Splitting methods</li><li><strong>Primal-dual methods</strong>: Condat-Vu, Chambolle-Pock algorithms</li></ul><p><strong>Common Use Cases</strong></p><ul><li><strong>Constraint projection</strong>: Project onto feasible sets</li><li><strong>Regularization</strong>: Apply regularization operators</li><li><strong>Denoising</strong>: Soft thresholding for sparse signals</li><li><strong>Matrix completion</strong>: Project onto low-rank or PSD constraints</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L513-L586">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle(f::IndicatorPSD, 
              x::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, 
              gamma::Float64 = 1.0, 
              enableParallel::Bool=false)</code></pre><p>Non-mutating version of the proximal operator. Returns same type as input (sparse or dense).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorPSD.jl#L175-L183">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle(f::IndicatorSumOfNVariables, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false) -&gt; NumericVariable</code></pre><p>Computes and returns the proximal operator (projection) of the indicator function, i.e. the projection of <code>x</code> onto the set</p><pre><code class="nohighlight hljs">x₁ + x₂ + … + xₙ = rhs.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorSumOfNVariables.jl#L135-L142">source</a></section><section><div><p>Non-mutating version of the proximal operator</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/MatrixNuclearNorm.jl#L78-L80">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle(f::WrapperScalingTranslationFunction, x::NumericVariable, gamma::Float64, enableParallel::Bool=false)</code></pre><p>Compute the proximal operator of f(x) = g(coe·x + translation).</p><p>Returns: prox<em>{γf}(x) = (prox</em>{γ·coe², g}(translation + coe·x) - translation) / coe</p><p><strong>Arguments</strong></p><ul><li><code>f::WrapperScalingTranslationFunction</code>: The wrapped function</li><li><code>x::NumericVariable</code>: Input point for the proximal operator</li><li><code>gamma::Float64</code>: Proximal parameter</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Returns</strong></p><ul><li><code>NumericVariable</code>: Result of the proximal operator</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Proximal operator of scaled L2 ball: f(x) = I_{||2x||₂ ≤ 1}(x)
g = IndicatorBallL2(1.0)
f = WrapperScalingTranslationFunction(g, 2.0, 0.0)
result = proximalOracle(f, [1.0, 1.0], 1.0)  # Projects onto scaled ball</code></pre><p><strong>Throws</strong></p><ul><li><code>ErrorException</code>: If original function doesn&#39;t have proximal operator</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L253-L279">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.proximalOracle!" href="#PDMO.proximalOracle!"><code>PDMO.proximalOracle!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">proximalOracle!(y::NumericVariable, f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)</code></pre><p>In-place computation of the proximal operator for function f.</p><p>The proximal operator is a fundamental concept in convex optimization and is defined as:</p><pre><code class="nohighlight hljs">prox_{γf}(x) = argmin_z { f(z) + (1/(2γ))||z - x||² }</code></pre><p>This function computes the result in-place, storing it in the pre-allocated output <code>y</code>.</p><p><strong>Arguments</strong></p><ul><li><code>y::NumericVariable</code>: Pre-allocated output buffer (modified in-place)</li><li><code>f::AbstractFunction</code>: The function for which to compute the proximal operator</li><li><code>x::NumericVariable</code>: The input point</li><li><code>gamma::Float64=1.0</code>: The proximal parameter γ &gt; 0</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Mathematical Background</strong></p><p>The proximal operator generalizes the concept of projection onto a set:</p><ul><li>For indicator functions: prox<em>{γI</em>S}(x) = Proj_S(x) (projection onto set S)</li><li>For L1 norm: prox<em>{γ||·||₁}(x) = soft</em>threshold(x, γ) (soft thresholding)</li><li>For quadratic functions: Has explicit closed-form solution</li></ul><p><strong>Implementation Requirements</strong></p><p>Functions with <code>isProximal(f) = true</code> must implement this method. The implementation should:</p><ol><li>Validate input dimensions and parameter values</li><li>Compute the proximal operator efficiently</li><li>Store the result in the pre-allocated output <code>y</code></li><li>Handle edge cases and numerical stability</li><li>Optionally support parallel computation</li></ol><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Basic usage with indicator function
f = IndicatorBox([-1.0, -1.0], [1.0, 1.0])
x = [2.0, -2.0]
y = similar(x)
proximalOracle!(y, f, x, 1.0)  # Projects onto box, y = [1.0, -1.0]

# L1 norm with soft thresholding
f = ElementwiseL1Norm(0.5)
x = [2.0, -3.0, 0.3]
y = similar(x)
proximalOracle!(y, f, x, 1.0)  # y = [1.5, -2.5, 0.0]

# Matrix functions
f = IndicatorPSD(3)
X = rand(3, 3)
Y = similar(X)
proximalOracle!(Y, f, X, 1.0)  # Projects onto PSD cone</code></pre><p><strong>Performance Considerations</strong></p><ul><li>Pre-allocate output buffer to avoid memory allocation</li><li>Use in-place operations within the implementation</li><li>Consider cache efficiency for large arrays</li><li>Parallel computation can be beneficial for large-scale problems</li></ul><p><strong>Common Proximal Operators</strong></p><ul><li><strong>Indicator functions</strong>: Projection onto constraint sets</li><li><strong>L1 norm</strong>: Soft thresholding operator</li><li><strong>L2 ball</strong>: Projection onto ball (scaling if outside)</li><li><strong>Zero function</strong>: Identity operator</li><li><strong>Quadratic functions</strong>: Require solving linear systems</li></ul><p><strong>Error Handling</strong></p><ul><li>Input validation for dimensions and parameter ranges</li><li>Numerical stability considerations</li><li>Appropriate handling of edge cases (zero gamma, boundary cases)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L437-L508">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle!(y::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, 
                f::IndicatorPSD, 
                x::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, 
                gamma::Float64 = 1.0, 
                enableParallel::Bool=false)</code></pre><p>Project onto the PSD cone. Works with both dense and sparse matrices. The projection is performed by:</p><ol><li>Symmetrizing the matrix</li><li>Computing eigendecomposition</li><li>Setting negative eigenvalues to zero</li><li>Reconstructing the matrix</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorPSD.jl#L128-L141">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle!(y::NumericVariable, f::IndicatorSumOfNVariables, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)</code></pre><p>Computes the proximal operator (i.e. the projection) of the indicator function in-place, storing the result in <code>y</code>.</p><p>For the scalar case (when <code>rhs</code> is a Number), it is assumed that <code>x</code> is a vector of length <code>numberVariables</code>, and the projection onto { x : sum(x) = rhs } subtracts the uniform shift</p><pre><code class="nohighlight hljs">shift = (sum(x) - rhs) / numberVariables</code></pre><p>from each entry. For the non-scalar case, <code>x</code> is assumed to be an array whose first dimension is size(rhs, 1) * numberVariables and remaining dimensions match <code>rhs</code>. The function computes the elementwise residual</p><pre><code class="nohighlight hljs">res = (sum of blocks) - rhs</code></pre><p>and then computes</p><pre><code class="nohighlight hljs">shift = res / numberVariables</code></pre><p>which is subtracted from every block. No additional dimension checking is performed for performance reasons.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorSumOfNVariables.jl#L72-L96">source</a></section><section><div><p>Proximal operator for weighted nuclear norm. The solution is given by U * diag(max(0, σᵢ - γbᵢ)) * Vᵀ where U, σᵢ, V come from the SVD of x.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/MatrixNuclearNorm.jl#L63-L67">source</a></section><section><div><pre><code class="language-julia hljs">proximalOracle!(y::NumericVariable, f::WrapperScalingTranslationFunction, x::NumericVariable, gamma::Float64, enableParallel::Bool=false)</code></pre><p>In-place computation of the proximal operator of f(x) = g(coe·x + translation).</p><p><strong>Mathematical Background</strong></p><p>For f(x) = g(coe·x + translation), the proximal operator is: prox<em>{γf}(z) = argmin</em>x { g(coe·x + translation) + (1/(2γ))||x - z||² }</p><p>Through variable substitution u = coe·x + translation, this becomes: prox<em>{γf}(z) = (prox</em>{γ·coe², g}(translation + coe·z) - translation) / coe</p><p><strong>Algorithm</strong></p><ol><li>Transform input: buffer = coe·x + translation</li><li>Apply original proximal: prox_{γ·coe², g}(buffer)</li><li>Transform back: (result - translation) / coe</li></ol><p><strong>Arguments</strong></p><ul><li><code>y::NumericVariable</code>: Output buffer for the result (modified in-place)</li><li><code>f::WrapperScalingTranslationFunction</code>: The wrapped function</li><li><code>x::NumericVariable</code>: Input point for the proximal operator</li><li><code>gamma::Float64</code>: Proximal parameter</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Throws</strong></p><ul><li><code>ErrorException</code>: If original function doesn&#39;t have proximal operator, or dimension mismatches occur</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L201-L227">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.gradientOracle" href="#PDMO.gradientOracle"><code>PDMO.gradientOracle</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gradientOracle(f::AbstractFunction, x::NumericVariable, enableParallel::Bool=false) -&gt; NumericVariable</code></pre><p>Computes the gradient of function f at point x, returning a new array.</p><p>This is the allocating version of the gradient computation that creates and returns a new array containing the gradient. For performance-critical applications, consider using the in-place version <code>gradientOracle!</code> instead.</p><p><strong>Arguments</strong></p><ul><li><code>f::AbstractFunction</code>: The function for which to compute the gradient</li><li><code>x::NumericVariable</code>: The point at which to evaluate the gradient</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Returns</strong></p><ul><li><code>NumericVariable</code>: The gradient ∇f(x), same type and size as input <code>x</code></li></ul><p><strong>Mathematical Definition</strong></p><p>For smooth functions: ∇f(x) where each component is ∂f/∂xᵢ For vector functions: Returns the Jacobian or gradient matrix</p><p><strong>Implementation Notes</strong></p><p>The default implementation:</p><ol><li>Allocates a new array <code>grad = similar(x)</code></li><li>Calls <code>gradientOracle!(grad, f, x, enableParallel)</code></li><li>Returns the result</li></ol><p>Concrete function types may override this method for more efficient implementations, but most can rely on the default implementation.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Basic usage with quadratic function
Q = sparse([2.0 0.0; 0.0 2.0])
q = [1.0, 1.0]
f = QuadraticFunction(Q, q, 0.0)
x = [1.0, 2.0]
grad = gradientOracle(f, x)  # Returns [3.0, 5.0]

# Gradient descent step
f = QuadraticFunction(Q, q, 0.0)
x = [1.0, 2.0]
α = 0.1  # Step size
grad = gradientOracle(f, x)
x_new = x - α * grad  # Gradient descent update

# Matrix function gradients
A = rand(10, 5)
b = rand(10, 3)
f = FrobeniusNormSquare(A, b, 5, 3)
X = rand(5, 3)
grad_X = gradientOracle(f, X)  # Returns 5×3 gradient matrix

# Chaining with function evaluation
f = ComponentwiseExponentialFunction([1.0, 2.0])
x = [0.0, 1.0]
val = f(x)  # Function value
grad = gradientOracle(f, x)  # Gradient at same point</code></pre><p><strong>Performance Considerations</strong></p><ul><li><strong>Memory allocation</strong>: Creates new array on each call</li><li><strong>Prefer in-place version</strong>: Use <code>gradientOracle!</code> for better performance</li><li><strong>Temporary arrays</strong>: Consider pre-allocating arrays for repeated calls</li><li><strong>Memory pressure</strong>: May cause garbage collection in tight loops</li><li><strong>Automatic differentiation</strong>: Consider AD tools for complex functions</li></ul><p><strong>Algorithm Applications</strong></p><ul><li><strong>Steepest descent</strong>: x<em>{k+1} = x</em>k - α∇f(x_k)</li><li><strong>Momentum methods</strong>: Incorporate previous gradient information</li><li><strong>Adam optimizer</strong>: Adaptive gradient methods</li><li><strong>Line search</strong>: Determine optimal step sizes</li><li><strong>Quasi-Newton</strong>: Approximate Hessian from gradients</li></ul><p><strong>Common Use Cases</strong></p><ul><li><strong>Optimization</strong>: First-order optimization algorithms</li><li><strong>Machine learning</strong>: Backpropagation and gradient-based training</li><li><strong>Sensitivity analysis</strong>: Study function behavior</li><li><strong>Root finding</strong>: Newton&#39;s method for systems of equations</li><li><strong>Numerical integration</strong>: Gradient-based quadrature</li></ul><p><strong>Numerical Considerations</strong></p><ul><li><strong>Gradient magnitude</strong>: Large gradients may indicate poor scaling</li><li><strong>Numerical derivatives</strong>: Consider finite difference approximations</li><li><strong>Automatic differentiation</strong>: Tools like ForwardDiff.jl or ReverseDiff.jl</li><li><strong>Condition number</strong>: Well-conditioned problems have stable gradients</li></ul><p><strong>Error Handling</strong></p><ul><li>Functions must be smooth (<code>isSmooth(f) = true</code>)</li><li>Input validation for dimensions and types</li><li>Handling of numerical issues (NaN, Inf)</li><li>Meaningful error messages for debugging</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L686-L780">source</a></section><section><div><pre><code class="language-julia hljs">gradientOracle(f::WrapperScalingTranslationFunction, x::NumericVariable, enableParallel::Bool=false)</code></pre><p>Compute the gradient ∇f(x) = coe · ∇g(coe·x + translation).</p><p><strong>Arguments</strong></p><ul><li><code>f::WrapperScalingTranslationFunction</code>: The wrapped function</li><li><code>x::NumericVariable</code>: Point at which to evaluate the gradient</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Returns</strong></p><ul><li><code>NumericVariable</code>: The gradient vector/matrix</li></ul><p><strong>Throws</strong></p><ul><li><code>ErrorException</code>: If original function is not smooth</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L172-L187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.gradientOracle!" href="#PDMO.gradientOracle!"><code>PDMO.gradientOracle!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gradientOracle!(grad::NumericVariable, f::AbstractFunction, x::NumericVariable, enableParallel::Bool=false)</code></pre><p>In-place computation of the gradient of function f at point x.</p><p>This function computes the gradient (or subgradient for non-smooth functions) and stores the result in the pre-allocated output buffer <code>grad</code>.</p><p><strong>Arguments</strong></p><ul><li><code>grad::NumericVariable</code>: Pre-allocated output buffer for gradient (modified in-place)</li><li><code>f::AbstractFunction</code>: The function for which to compute the gradient</li><li><code>x::NumericVariable</code>: The point at which to evaluate the gradient</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Mathematical Background</strong></p><p>For smooth functions, the gradient is defined as:</p><pre><code class="nohighlight hljs">∇f(x) = lim_{h→0} [f(x+h) - f(x)] / h</code></pre><p>For vector/matrix functions, this becomes the vector/matrix of partial derivatives.</p><p><strong>Implementation Requirements</strong></p><p>Functions with <code>isSmooth(f) = true</code> must implement this method. The implementation should:</p><ol><li>Validate input dimensions match between <code>x</code> and <code>grad</code></li><li>Compute the gradient efficiently</li><li>Store the result in the pre-allocated output <code>grad</code></li><li>Handle numerical stability issues</li><li>Optionally support parallel computation</li></ol><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Basic usage with quadratic function
Q = [2.0 0.0; 0.0 2.0]
q = [1.0, 1.0]
f = QuadraticFunction(sparse(Q), q, 0.0)
x = [1.0, 2.0]
grad = similar(x)
gradientOracle!(grad, f, x)  # grad = Q*x + q = [3.0, 5.0]

# Affine function (constant gradient)
A = [2.0, 3.0]
f = AffineFunction(A, 0.0)
x = [1.0, 1.0]
grad = similar(x)
gradientOracle!(grad, f, x)  # grad = A = [2.0, 3.0]

# Matrix function
A = rand(10, 5)
b = rand(10, 3)
f = FrobeniusNormSquare(A, b, 5, 3)
X = rand(5, 3)
grad = similar(X)
gradientOracle!(grad, f, X)  # grad = 2*A&#39;*(A*X - b)</code></pre><p><strong>Performance Considerations</strong></p><ul><li><strong>Memory efficiency</strong>: Uses pre-allocated buffer to avoid allocations</li><li><strong>In-place operations</strong>: Implementation should minimize temporary arrays</li><li><strong>Vectorization</strong>: Take advantage of BLAS/LAPACK when possible</li><li><strong>Parallel computation</strong>: Can be beneficial for large-scale problems</li><li><strong>Cache efficiency</strong>: Consider memory access patterns</li></ul><p><strong>Common Gradient Patterns</strong></p><ul><li><strong>Linear functions</strong>: Gradient is constant (independent of x)</li><li><strong>Quadratic functions</strong>: Gradient is linear in x</li><li><strong>Least squares</strong>: Gradient involves matrix-vector products</li><li><strong>Exponential functions</strong>: Gradient involves exponential evaluations</li><li><strong>Composite functions</strong>: Use chain rule</li></ul><p><strong>Numerical Considerations</strong></p><ul><li><strong>Finite precision</strong>: Be aware of numerical errors</li><li><strong>Scaling</strong>: Consider function scaling for numerical stability</li><li><strong>Condition numbers</strong>: Well-conditioned problems have more stable gradients</li><li><strong>Overflow/underflow</strong>: Handle extreme values appropriately</li></ul><p><strong>Algorithm Applications</strong></p><ul><li><strong>Gradient descent</strong>: Steepest descent optimization</li><li><strong>Newton&#39;s method</strong>: Second-order optimization</li><li><strong>Quasi-Newton methods</strong>: BFGS, L-BFGS</li><li><strong>Conjugate gradient</strong>: Iterative linear system solvers</li><li><strong>Trust region methods</strong>: Model-based optimization</li></ul><p><strong>Error Handling</strong></p><ul><li>Validate that <code>x</code> and <code>grad</code> have compatible dimensions</li><li>Check for numerical issues (NaN, Inf)</li><li>Provide meaningful error messages for dimension mismatches</li><li>Handle edge cases gracefully</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunction.jl#L593-L681">source</a></section><section><div><pre><code class="language-julia hljs">gradientOracle!(y::NumericVariable, f::WrapperScalingTranslationFunction, x::NumericVariable, enableParallel::Bool=false)</code></pre><p>In-place computation of the gradient ∇f(x) = coe · ∇g(coe·x + translation).</p><p><strong>Mathematical Background</strong></p><p>For f(x) = g(coe·x + translation), the chain rule gives: ∇f(x) = ∇g(coe·x + translation) · coe</p><p><strong>Arguments</strong></p><ul><li><code>y::NumericVariable</code>: Output buffer for the gradient (modified in-place)</li><li><code>f::WrapperScalingTranslationFunction</code>: The wrapped function</li><li><code>x::NumericVariable</code>: Point at which to evaluate the gradient</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Throws</strong></p><ul><li><code>ErrorException</code>: If original function is not smooth, or dimension mismatches occur</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L138-L155">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.proximalOracleOfConjugate" href="#PDMO.proximalOracleOfConjugate"><code>PDMO.proximalOracleOfConjugate</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">proximalOracleOfConjugate(f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, 
                         enableParallel::Bool=false) -&gt; NumericVariable</code></pre><p>Compute the proximal operator of the convex conjugate f* using Moreau&#39;s identity.</p><p>This function returns prox<em>{γf<em>}(x) where f</em> is the convex conjugate of f. It uses the Moreau identity: prox</em>{γf*}(x) = x - γ * prox_{f/γ}(x/γ).</p><p><strong>Arguments</strong></p><ul><li><code>f::AbstractFunction</code>: The function whose conjugate proximal operator to compute</li><li><code>x::NumericVariable</code>: Input point</li><li><code>gamma::Float64=1.0</code>: Proximal parameter (must be positive)  </li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Returns</strong></p><ul><li><code>NumericVariable</code>: Result prox_{γf*}(x), same type and size as input <code>x</code></li></ul><p><strong>Mathematical Background</strong></p><p>The proximal operator of the convex conjugate is fundamental in convex optimization:</p><ul><li>For f*(y) = sup_x [⟨x,y⟩ - f(x)], the conjugate function</li><li>prox<em>{γf*}(x) = argmin</em>y [½||y-x||² + γf*(y)]</li></ul><p>The Moreau identity provides an efficient way to compute this without explicitly constructing the conjugate function, requiring only the proximal operator of f.</p><p><strong>Errors</strong></p><ul><li>Throws error if <code>f</code> does not have a proximal oracle</li><li>Throws error if <code>gamma ≤ 0</code></li></ul><p><strong>Applications</strong></p><ul><li><strong>Dual variable updates</strong> in primal-dual algorithms</li><li><strong>Constraint handling</strong> where f is an indicator function</li><li><strong>Regularization</strong> in optimization problems</li><li><strong>Image processing</strong> and signal processing applications</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># For L1 norm f(x) = ||x||₁
# The conjugate f*(y) = indicator of ||y||∞ ≤ 1
y = proximalOracleOfConjugate(l1_norm, x, gamma)  # Projects onto ℓ∞ ball

# For indicator of convex set f = δ_C
# The conjugate f*(y) = σ_C(y) (support function)
y = proximalOracleOfConjugate(indicator_C, x, gamma)  # Scaled projection</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunctionUtil.jl#L264-L312">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.proximalOracleOfConjugate!" href="#PDMO.proximalOracleOfConjugate!"><code>PDMO.proximalOracleOfConjugate!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">proximalOracleOfConjugate!(y::NumericVariable, f::AbstractFunction, x::NumericVariable, 
                          gamma::Float64=1.0, enableParallel::Bool=false)</code></pre><p>In-place computation of the proximal operator of the convex conjugate f* using Moreau&#39;s identity.</p><p>This function computes y = prox<em>{γf<em>}(x) where f</em> is the convex conjugate of f. It uses the Moreau identity: prox</em>{γf*}(x) = x - γ * prox_{f/γ}(x/γ).</p><p><strong>Arguments</strong></p><ul><li><code>y::NumericVariable</code>: Output variable (modified in-place)</li><li><code>f::AbstractFunction</code>: The function whose conjugate proximal operator to compute</li><li><code>x::NumericVariable</code>: Input point</li><li><code>gamma::Float64=1.0</code>: Proximal parameter (must be positive)</li><li><code>enableParallel::Bool=false</code>: Whether to enable parallel computation</li></ul><p><strong>Effects</strong></p><ul><li>Modifies <code>y</code> in-place to contain prox_{γf*}(x)</li><li>Requires <code>f</code> to have a proximal oracle implementation</li></ul><p><strong>Mathematical Background</strong></p><p>The Moreau identity relates the proximal operators of a function and its conjugate:</p><ul><li>prox<em>{γf*}(x) + γ * prox</em>{f/γ}(x/γ) = x</li></ul><p>This identity allows computing the proximal operator of f* when only the proximal operator of f is available, which is common in primal-dual algorithms.</p><p><strong>Errors</strong></p><ul><li>Throws error if <code>f</code> does not have a proximal oracle</li><li>Throws error if <code>x</code> and <code>y</code> have different sizes</li><li>Throws error if <code>gamma ≤ 0</code></li><li>Throws error for scalar inputs (use scalar version of proximal operators)</li></ul><p><strong>Applications</strong></p><ul><li>Primal-dual splitting algorithms (ADMM, Condat-Vu)</li><li>Computing dual proximal steps when only primal proximal is available</li><li>Implementing algorithms that require conjugate function evaluations</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># For constraint f = indicator of convex set C
# prox_{γf*}(x) computes projection onto γC
proximalOracleOfConjugate!(y, f, x, gamma)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunctionUtil.jl#L195-L239">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.estimateLipschitzConstant" href="#PDMO.estimateLipschitzConstant"><code>PDMO.estimateLipschitzConstant</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">estimateLipschitzConstant(f::AbstractFunction, x::NumericVariable; maxTrials::Int=50, 
                         minStepSize::Float64=1e-6, maxStepSize::Float64=1.0) -&gt; Float64</code></pre><p>Estimate the Lipschitz constant of the gradient ∇f at point x using multiple sampling strategies.</p><p>This function provides robust estimation of the Lipschitz constant L such that  ||∇f(x) - ∇f(y)|| ≤ L||x - y|| for points near x. It handles special cases with exact solutions and uses multiple numerical strategies for general functions.</p><p><strong>Arguments</strong></p><ul><li><code>f::AbstractFunction</code>: The function for which to estimate the Lipschitz constant</li><li><code>x::NumericVariable</code>: The point around which to estimate the constant</li><li><code>maxTrials::Int=50</code>: Maximum number of sampling trials for estimation</li><li><code>minStepSize::Float64=1e-6</code>: Minimum step size for finite differences</li><li><code>maxStepSize::Float64=1.0</code>: Maximum step size for finite differences</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Estimated Lipschitz constant (conservative upper bound)</li></ul><p><strong>Algorithm</strong></p><p>The function uses a multi-strategy approach:</p><p><strong>Exact Cases</strong></p><ul><li><strong>QuadraticFunction</strong>: Returns 2||Q|| where Q is the Hessian matrix</li><li><strong>AffineFunction/Zero</strong>: Returns 0 (constant gradient)</li></ul><p><strong>Numerical Estimation (3 strategies)</strong></p><ol><li><strong>Random Directions</strong>: Sample random unit directions with multiple scales</li><li><strong>Coordinate Directions</strong>: Test axis-aligned perturbations (for problems ≤ 1000 dimensions)  </li><li><strong>Adaptive Scaling</strong>: Use gradient magnitude to inform step size selection</li></ol><p><strong>Statistical Robustness</strong></p><ul><li>Collects estimates from all strategies</li><li>Removes outliers using percentile-based filtering</li><li>Returns conservative estimate (90th percentile or 1.2× median)</li><li>Handles edge cases with appropriate fallbacks</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># For a quadratic function f(x) = 0.5x&#39;Qx
L = estimateLipschitzConstant(f, x0)  # Returns 2*opnorm(Q)

# For a general smooth function
L = estimateLipschitzConstant(f, x0, maxTrials=100)  # More thorough sampling</code></pre><p><strong>Notes</strong></p><ul><li>The estimate is intentionally conservative to ensure algorithm stability</li><li>For high-dimensional problems, coordinate sampling is limited for efficiency</li><li>Multiple strategies provide robustness across different function types</li><li>Handles both scalar and vector inputs appropriately</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AbstractFunctionUtil.jl#L1-L53">source</a></section></article><h2 id="Functions-implemented-in-PDMO.jl"><a class="docs-heading-anchor" href="#Functions-implemented-in-PDMO.jl">Functions implemented in <code>PDMO.jl</code></a><a id="Functions-implemented-in-PDMO.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-implemented-in-PDMO.jl" title="Permalink"></a></h2><h3 id="Basic-Functions"><a class="docs-heading-anchor" href="#Basic-Functions">Basic Functions</a><a id="Basic-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.Zero" href="#PDMO.Zero"><code>PDMO.Zero</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Zero()</code></pre><p>Represents the zero function f(x) = 0 for all x.</p><p><strong>Mathematical Definition</strong></p><p>f(x) = 0 for all x ∈ ℝⁿ</p><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, constant functions are infinitely differentiable</li><li><strong>Convex</strong>: Yes, constant functions are convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Gradient</strong>: ∇f(x) = 0 (zero vector/scalar)</li><li><strong>Proximal Operator</strong>: prox_γf(x) = x (identity function)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Zero function
f = Zero()
x = [1.0, 2.0, 3.0]
val = f(x)  # Returns 0.0

# Gradient is always zero
grad = gradientOracle(f, x)  # Returns [0.0, 0.0, 0.0]

# Proximal operator is identity
prox_x = proximalOracle(f, x)  # Returns [1.0, 2.0, 3.0]</code></pre><p><strong>Applications</strong></p><ul><li>Neutral elements in optimization</li><li>Baseline functions in algorithms</li><li>Simplified formulations</li><li>Algorithm testing and verification</li><li>Initialization in iterative methods</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/Zero.jl#L1-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AffineFunction" href="#PDMO.AffineFunction"><code>PDMO.AffineFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AffineFunction(A::NumericVariable, r::Float64=0.0)</code></pre><p>Represents an affine function of the form f(x) = ⟨A, x⟩ + r, where A is a coefficient vector/matrix and r is a scalar offset.</p><p><strong>Mathematical Definition</strong></p><ul><li>For vector input x: f(x) = A&#39;x + r  </li><li>For scalar input x: f(x) = A*x + r</li></ul><p><strong>Arguments</strong></p><ul><li><code>A::NumericVariable</code>: Coefficient vector/matrix/scalar</li><li><code>r::Float64=0.0</code>: Scalar offset term</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, gradient is constant</li><li><strong>Convex</strong>: Yes, affine functions are convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Gradient</strong>: ∇f(x) = A (constant)</li><li><strong>Proximal Operator</strong>: prox_γf(x) = x - γA</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Linear function f(x) = 2x₁ + 3x₂ + 1
A = [2.0, 3.0]
r = 1.0
f = AffineFunction(A, r)
x = [1.0, 2.0]
val = f(x)  # Returns 2*1 + 3*2 + 1 = 9

# Scalar function f(x) = 5x + 2
f = AffineFunction(5.0, 2.0)
val = f(3.0)  # Returns 5*3 + 2 = 17</code></pre><p><strong>Applications</strong></p><ul><li>Linear constraints in optimization</li><li>Objective functions in linear programming</li><li>Penalty terms in regularization</li><li>Building blocks for more complex functions</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/AffineFunction.jl#L1-L43">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.QuadraticFunction" href="#PDMO.QuadraticFunction"><code>PDMO.QuadraticFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">QuadraticFunction(Q::SparseMatrixCSC{Float64, Int64}, q::Vector{Float64}, r::Float64)</code></pre><p>Represents a quadratic function of the form f(x) = (1/2)x&#39;Qx + q&#39;x + r.</p><p><strong>Mathematical Definition</strong></p><p>f(x) = (1/2)x&#39;Qx + q&#39;x + r</p><p>where:</p><ul><li>Q is a symmetric matrix (Hessian)</li><li>q is a linear term vector</li><li>r is a scalar offset</li></ul><p><strong>Arguments</strong></p><ul><li><code>Q::SparseMatrixCSC{Float64, Int64}</code>: Quadratic coefficient matrix (should be symmetric)</li><li><code>q::Vector{Float64}</code>: Linear coefficient vector</li><li><code>r::Float64</code>: Scalar offset term</li></ul><p><strong>Constructors</strong></p><ul><li><code>QuadraticFunction(Q, q, r)</code>: Full specification</li><li><code>QuadraticFunction(n::Int64)</code>: Zero quadratic function of dimension n</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, quadratic functions are infinitely differentiable</li><li><strong>Convex</strong>: Yes, if Q is positive semidefinite</li><li><strong>Proximal</strong>: No, proximal operator not implemented (requires solving linear system)</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Gradient</strong>: ∇f(x) = Qx + Q&#39;x + q = (Q + Q&#39;)x + q</li><li><strong>Hessian</strong>: ∇²f(x) = Q + Q&#39;</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Quadratic function f(x) = x₁² + x₂² + x₁ + 2x₂ + 3
Q = sparse([1.0 0.0; 0.0 1.0])  # Identity matrix
q = [1.0, 2.0]
r = 3.0
f = QuadraticFunction(Q, q, r)
x = [1.0, 1.0]
val = f(x)  # Returns 1 + 1 + 1 + 2 + 3 = 8

# Zero quadratic function
f = QuadraticFunction(2)  # f(x) = 0 for x ∈ ℝ²</code></pre><p><strong>Applications</strong></p><ul><li>Quadratic programming</li><li>Least squares problems</li><li>Trust region methods</li><li>Newton&#39;s method approximations</li><li>Model predictive control</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/QuadraticFunction.jl#L1-L52">source</a></section></article><h3 id="Norm-Functions"><a class="docs-heading-anchor" href="#Norm-Functions">Norm Functions</a><a id="Norm-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Norm-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.ElementwiseL1Norm" href="#PDMO.ElementwiseL1Norm"><code>PDMO.ElementwiseL1Norm</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ElementwiseL1Norm(coefficient::Float64=1.0)</code></pre><p>Represents the element-wise L1 norm function f(x) = coefficient * ||x||₁.</p><p><strong>Mathematical Definition</strong></p><p>f(x) = coefficient * ∑ᵢ |xᵢ|</p><p>where ||x||₁ is the L1 norm (sum of absolute values).</p><p><strong>Arguments</strong></p><ul><li><code>coefficient::Float64=1.0</code>: Positive scaling coefficient</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, not differentiable at zero</li><li><strong>Convex</strong>: Yes, L1 norm is convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator (soft thresholding)</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Subdifferential</strong>: ∂f(x) = coefficient * sign(x) (element-wise)</li><li><strong>Proximal Operator</strong>: Soft thresholding operator<ul><li>prox_γf(x)ᵢ = sign(xᵢ) * max(0, |xᵢ| - γ*coefficient)</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Standard L1 norm
f = ElementwiseL1Norm()
x = [1.0, -2.0, 3.0]
val = f(x)  # Returns |1| + |-2| + |3| = 6

# Scaled L1 norm with coefficient 0.5
f = ElementwiseL1Norm(0.5)
x = [4.0, -6.0]
val = f(x)  # Returns 0.5 * (4 + 6) = 5

# Proximal operator (soft thresholding)
f = ElementwiseL1Norm(1.0)
x = [2.0, -3.0, 0.5]
prox_x = proximalOracle(f, x, 1.0)  # γ = 1.0
# Returns [1.0, -2.0, 0.0] (soft thresholding with threshold 1.0)</code></pre><p><strong>Applications</strong></p><ul><li>Sparse regression (LASSO)</li><li>Compressed sensing</li><li>Feature selection</li><li>Regularization in machine learning</li><li>Signal denoising</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/ElementwiseL1Norm.jl#L1-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.FrobeniusNormSquare" href="#PDMO.FrobeniusNormSquare"><code>PDMO.FrobeniusNormSquare</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FrobeniusNormSquare(A::Matrix{Float64}, b::Union{Matrix{Float64}, Vector{Float64}}, numberRows::Int64, numberColumns::Int64, coe::Float64=0.5)</code></pre><p>Represents the squared Frobenius norm of the residual AX - b.</p><p><strong>Mathematical Definition</strong></p><pre><code class="nohighlight hljs">f(X) = coe * ||AX - b||²_F</code></pre><p>where:</p><ul><li>A is an L × m coefficient matrix</li><li>X is an m × n variable matrix (or vector if n=1)</li><li>b is an L × n target matrix (or L-dimensional vector if n=1)</li><li>||·||<em>F denotes the Frobenius norm: ||M||²</em>F = ∑ᵢⱼ M²ᵢⱼ = trace(M&#39;M)</li></ul><p><strong>Arguments</strong></p><ul><li><code>A::Matrix{Float64}</code>: Coefficient matrix (L × m)</li><li><code>b::Union{Matrix{Float64}, Vector{Float64}}</code>: Target matrix (L × n) or vector (L)</li><li><code>numberRows::Int64</code>: Number of rows in X (m)</li><li><code>numberColumns::Int64</code>: Number of columns in X (n), set to 1 if b is a vector</li><li><code>coe::Float64=0.5</code>: Positive scaling coefficient</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, quadratic functions are infinitely differentiable</li><li><strong>Convex</strong>: Yes, squared Frobenius norm is convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator via linear system solution</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Gradient</strong>: ∇f(X) = 2 * coe * A&#39;(AX - b)</li><li><strong>Proximal Operator</strong>: Solution to (I + 2γ<em>coe</em>A&#39;A)Y = X + 2γ<em>coe</em>A&#39;b</li></ul><p><strong>Internal Structure</strong></p><p>The struct pre-computes and caches:</p><ul><li><code>ATransA = A&#39;A</code>: Gram matrix for efficient repeated computations</li><li>Factorization of the proximal system matrix for fast linear solves</li><li>Buffer arrays to minimize memory allocations</li></ul><p><strong>Implementation Details</strong></p><ul><li><strong>Automatic problem detection</strong>: Handles both vector (n=1) and matrix (n&gt;1) cases</li><li><strong>Efficient factorization</strong>: Uses Cholesky when possible, falls back to LU</li><li><strong>Factorization caching</strong>: Reuses factorization when γ parameter doesn&#39;t change</li><li><strong>Memory management</strong>: Pre-allocated buffers for all intermediate computations</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Least squares problem: minimize ||Ax - b||²
A = rand(10, 5)  # 10 constraints, 5 variables
b = rand(10)     # Target vector
f = FrobeniusNormSquare(A, b, 5, 1, 0.5)  # Note: coe=0.5 gives (1/2)||Ax-b||²
x = rand(5)
val = f(x)  # Function value
grad = gradientOracle(f, x)  # Gradient: A&#39;(Ax - b)

# Matrix least squares: minimize ||AX - B||²_F
A = rand(10, 5)    # Linear operator
B = rand(10, 3)    # Target matrix
f = FrobeniusNormSquare(A, B, 5, 3, 1.0)
X = rand(5, 3)
val = f(X)  # Function value
grad = gradientOracle(f, X)  # Gradient matrix

# Proximal operator (useful in optimization algorithms)
f = FrobeniusNormSquare(A, b, 5, 1, 1.0)
x_current = rand(5)
γ = 0.1  # Proximal parameter
x_prox = proximalOracle(f, x_current, γ)  # Proximal step</code></pre><p><strong>Algorithm Applications</strong></p><ul><li><strong>Least squares regression</strong>: Direct formulation of ||Ax - b||²</li><li><strong>Ridge regression</strong>: Add L2 regularization</li><li><strong>Matrix completion</strong>: Frobenius norm data fitting term</li><li><strong>Image denoising</strong>: Data fidelity term in variational methods</li><li><strong>System identification</strong>: Parameter estimation problems</li><li><strong>Proximal gradient methods</strong>: Smooth term in composite optimization</li><li><strong>ADMM</strong>: Quadratic penalty terms</li></ul><p><strong>Optimization Context</strong></p><p>This function commonly appears in:</p><pre><code class="language-julia hljs"># Composite optimization problem
minimize f(x) + g(x)
# where f(x) = (1/2)||Ax - b||² (smooth)
# and g(x) is some regularizer (possibly non-smooth)</code></pre><p>Algorithms like proximal gradient descent alternate between:</p><ol><li>Gradient step on f: x̃ = x - α∇f(x)</li><li>Proximal step on g: x⁺ = prox_g(x̃)</li></ol><p><strong>Performance Characteristics</strong></p><ul><li><strong>Function evaluation</strong>: O(mn + Ln) operations</li><li><strong>Gradient computation</strong>: O(mn + Ln) operations  </li><li><strong>Proximal operator</strong>: O(m³) for factorization + O(m²n) for solve</li><li><strong>Memory usage</strong>: O(m²) for factorization + O(mn + Ln) for buffers</li></ul><p><strong>Numerical Considerations</strong></p><ul><li><strong>Condition number</strong>: Well-conditioned when A&#39;A is well-conditioned</li><li><strong>Factorization choice</strong>: Cholesky (faster) vs LU (more robust)</li><li><strong>Scaling</strong>: Consider rescaling A and b for numerical stability</li><li><strong>Caching</strong>: Factorization is cached and reused when γ doesn&#39;t change</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/FrobeniusNormSquare.jl#L1-L104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.MatrixNuclearNorm" href="#PDMO.MatrixNuclearNorm"><code>PDMO.MatrixNuclearNorm</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixNuclearNorm(b, rows, cols)</code></pre><p>Weighted nuclear norm of a matrix with component-wise weights, defined as:     ||W||_{b,*} = ∑ᵢ bᵢσᵢ(W)</p><p>where σᵢ(W) are the singular values of W, b ∈ ℝᵐ₊ is a vector of positive weights  (m = min(rows,cols)), and the sum goes up to the number of singular values.</p><p><strong>Arguments</strong></p><ul><li><code>b::Vector{Float64}</code>: Vector of positive weights, one per singular value</li><li><code>rows::Int64</code>: Number of rows in the matrix</li><li><code>cols::Int64</code>: Number of columns in the matrix</li></ul><p><strong>Properties</strong></p><ul><li>Proximal: Yes</li><li>Proximal Operator: Component-wise soft thresholding of singular values</li><li>Convex: No. Only when all entries of b are equal</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># For a 3×2 matrix, we need 2 weights (min(3,2) = 2)
b = [1.0, 2.0]  # Different weights for different singular values
f = MatrixNuclearNorm(b, 3, 2)
X = [1.0 2.0; 3.0 4.0; 5.0 6.0]
val = f(X)  # Computes weighted nuclear norm</code></pre><p>Note: The number of weights in b must equal min(rows, cols), as this is the  maximum possible number of non-zero singular values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/MatrixNuclearNorm.jl#L1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.WeightedMatrixL1Norm" href="#PDMO.WeightedMatrixL1Norm"><code>PDMO.WeightedMatrixL1Norm</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedMatrixL1Norm(A, inNonnegativeOrthant=false)</code></pre><p>Weighted L1 norm of a matrix, defined as:     ||A ⊙ x||₁ = ∑ᵢⱼ A<em>{i,j}|x</em>{i,j}|</p><p>When inNonnegativeOrthant is true, the function becomes:     ||A ⊙ x||₁ + indicator(x ≥ 0)</p><p>where A is a sparse matrix of non-negative weights and ⊙ denotes element-wise multiplication.</p><p><strong>Arguments</strong></p><ul><li><code>A::SparseMatrixCSC{Float64, Int64}</code>: Matrix of non-negative weights</li><li><code>inNonnegativeOrthant::Bool=false</code>: If true, restricts the domain to the non-negative orthant</li></ul><p><strong>Properties</strong></p><ul><li>Convex: Yes</li><li>Proximal: Yes</li><li>Proximal Operator: <ul><li>If inNonnegativeOrthant=false: Element-wise soft thresholding with weights A</li><li>If inNonnegativeOrthant=true: Project onto non-negative orthant after soft thresholding with weights A</li></ul></li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = sparse([1.0 2.0; 3.0 4.0])
f = WeightedMatrixL1Norm(A)            # Standard weighted L1 norm
g = WeightedMatrixL1Norm(A, true)      # Weighted L1 norm restricted to non-negative orthant
x = [1.0 -1.0; 2.0 -2.0]
val = f(x)  # Computes weighted L1 norm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WeightedMatrixL1Norm.jl#L1-L31">source</a></section></article><h3 id="Indicator-Functions"><a class="docs-heading-anchor" href="#Indicator-Functions">Indicator Functions</a><a id="Indicator-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Indicator-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorBallL2" href="#PDMO.IndicatorBallL2"><code>PDMO.IndicatorBallL2</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorBallL2(r::Float64)</code></pre><p>Indicator function of the L2 ball with radius r.</p><p><strong>Mathematical Definition</strong></p><p>The indicator function of the set B₂(r) = {x : ||x||₂ ≤ r}:</p><p>f(x) = 0    if ||x||₂ ≤ r f(x) = +∞   otherwise</p><p><strong>Arguments</strong></p><ul><li><code>r::Float64</code>: Radius of the L2 ball (must be positive)</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, not differentiable on the boundary</li><li><strong>Convex</strong>: Yes, indicator functions of convex sets are convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator (projection onto ball)</li><li><strong>Set Indicator</strong>: Yes, this is an indicator function</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Proximal Operator</strong>: Projection onto the L2 ball<ul><li>If ||x||₂ ≤ r: prox_f(x) = x</li><li>If ||x||₂ &gt; r: prox_f(x) = (r/||x||₂) * x</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Unit L2 ball (radius 1)
f = IndicatorBallL2(1.0)
x = [0.5, 0.5]
val = f(x)  # Returns 0.0 since ||x||₂ = √0.5 &lt; 1

# Point outside the ball
x = [2.0, 2.0]
val = f(x)  # Returns +∞ since ||x||₂ = 2√2 &gt; 1

# Proximal operator (projection onto ball)
f = IndicatorBallL2(1.0)
x = [3.0, 4.0]  # ||x||₂ = 5
prox_x = proximalOracle(f, x)  # Returns [0.6, 0.8] (normalized to unit length)</code></pre><p><strong>Applications</strong></p><ul><li>Constraint sets in optimization</li><li>Regularization in machine learning</li><li>Trust region methods</li><li>Robust optimization</li><li>Signal processing (bounded energy constraints)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorBallL2.jl#L1-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorBox" href="#PDMO.IndicatorBox"><code>PDMO.IndicatorBox</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorBox(lb::NumericVariable, ub::NumericVariable)</code></pre><p>Indicator function of a box constraint set [lb, ub].</p><p><strong>Mathematical Definition</strong></p><p>The indicator function of the box constraint set: {x : lb ≤ x ≤ ub} (element-wise inequalities)</p><p>f(x) = 0    if lb ≤ x ≤ ub (element-wise) f(x) = +∞   otherwise</p><p><strong>Arguments</strong></p><ul><li><code>lb::NumericVariable</code>: Lower bound vector/scalar</li><li><code>ub::NumericVariable</code>: Upper bound vector/scalar</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, not differentiable on the boundary</li><li><strong>Convex</strong>: Yes, indicator functions of convex sets are convex</li><li><strong>Proximal</strong>: Yes, has explicit proximal operator (projection onto box)</li><li><strong>Set Indicator</strong>: Yes, this is an indicator function</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Proximal Operator</strong>: Element-wise projection onto the box<ul><li>prox_f(x) = clamp(x, lb, ub) = max(lb, min(x, ub))</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Unit box constraint [-1, 1]ⁿ
lb = [-1.0, -1.0]
ub = [1.0, 1.0]
f = IndicatorBox(lb, ub)
x = [0.5, -0.5]
val = f(x)  # Returns 0.0 since x is within bounds

# Point outside the box
x = [2.0, -2.0]
val = f(x)  # Returns +∞ since x violates bounds

# Proximal operator (projection onto box)
f = IndicatorBox([-1.0, -1.0], [1.0, 1.0])
x = [2.0, -2.0]
prox_x = proximalOracle(f, x)  # Returns [1.0, -1.0] (clamped to bounds)</code></pre><p><strong>Applications</strong></p><ul><li>Variable bounds in optimization</li><li>Constraint sets in quadratic programming</li><li>Image processing (pixel value bounds)</li><li>Control systems (actuator limits)</li><li>Portfolio optimization (position limits)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorBox.jl#L1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorHyperplane" href="#PDMO.IndicatorHyperplane"><code>PDMO.IndicatorHyperplane</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorHyperplane(slope::Vector{Float64}, intercept::Float64)</code></pre><p>Represents the indicator function of a hyperplane defined by ⟨slope, x⟩ = intercept.</p><p><strong>Mathematical Definition</strong></p><p>The indicator function of the hyperplane H = {x : ⟨a,x⟩ = b}:</p><p class="math-container">\[I_H(x) = \begin{cases}
0 &amp; \text{if } \langle a,x \rangle = b \\
+\infty &amp; \text{otherwise}
\end{cases}\]</p><p>where a is the normal vector (slope) and b is the intercept.</p><p><strong>Arguments</strong></p><ul><li><code>slope::Vector{Float64}</code>: Normal vector of the hyperplane (must be non-zero)</li><li><code>intercept::Float64</code>: Right-hand side of the hyperplane equation</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, not differentiable (indicator function)</li><li><strong>Convex</strong>: Yes, indicator functions of convex sets are convex</li><li><strong>Proximal</strong>: Yes, proximal operator is projection onto the hyperplane</li><li><strong>Set Indicator</strong>: Yes, represents the constraint set ⟨a,x⟩ = b</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Hyperplane equation</strong>: ⟨slope, x⟩ = intercept</li><li><strong>Normal vector</strong>: slope vector points orthogonal to the hyperplane</li><li><strong>Proximal operator (projection)</strong>: P_H(x) = x - (⟨a,x⟩ - b) · a/||a||²</li></ul><p><strong>Geometric Interpretation</strong></p><ul><li><strong>Hyperplane</strong>: (n-1)-dimensional affine subspace in ℝⁿ</li><li><strong>Normal vector</strong>: slope defines the orientation of the hyperplane</li><li><strong>Distance from origin</strong>: |intercept|/||slope|| when slope is normalized</li><li><strong>Projection</strong>: Orthogonal projection onto the hyperplane</li></ul><p><strong>Internal Structure</strong></p><p>The constructor pre-computes:</p><ul><li><code>scaledSlope = slope / ||slope||²</code>: Normalized direction for efficient projection</li><li>Validation: slope vector cannot be zero or empty</li></ul><p><strong>Implementation Details</strong></p><ul><li><strong>Numerical stability</strong>: Pre-computes normalized slope to avoid repeated divisions</li><li><strong>Efficient projection</strong>: Uses pre-computed scaledSlope in proximal operator</li><li><strong>Tolerance handling</strong>: Uses FeasTolerance for numerical feasibility checks</li><li><strong>Memory efficiency</strong>: Minimal storage with pre-computed values</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Hyperplane x₁ + 2x₂ = 3
slope = [1.0, 2.0]
intercept = 3.0
f = IndicatorHyperplane(slope, intercept)

# Check if point is on hyperplane
x_on = [1.0, 1.0]  # 1*1 + 2*1 = 3 ✓
val = f(x_on)  # Returns 0.0

x_off = [0.0, 0.0]  # 1*0 + 2*0 = 0 ≠ 3 ✗
val = f(x_off)  # Returns +∞

# Project point onto hyperplane
x = [2.0, 3.0]  # Not on hyperplane: 1*2 + 2*3 = 8 ≠ 3
x_proj = proximalOracle(f, x)  # Projects x onto hyperplane

# Verify projection is on hyperplane
@assert abs(dot(slope, x_proj) - intercept) &lt; 1e-10

# 2D example: line x - y = 1
f_line = IndicatorHyperplane([1.0, -1.0], 1.0)
x = [0.0, 0.0]
x_proj = proximalOracle(f_line, x)  # Projects to line x - y = 1

# 3D example: plane x + y + z = 1
f_plane = IndicatorHyperplane([1.0, 1.0, 1.0], 1.0)
x = [2.0, 2.0, 2.0]
x_proj = proximalOracle(f_plane, x)  # Projects to plane</code></pre><p><strong>Algorithm Applications</strong></p><ul><li><strong>Equality constraints</strong>: Represent linear equality constraints in optimization</li><li><strong>Projection methods</strong>: Project iterates onto constraint hyperplanes</li><li><strong>ADMM</strong>: Enforce linear equality constraints</li><li><strong>Feasibility problems</strong>: Find points satisfying linear equations</li><li><strong>Method of alternating projections</strong>: Between multiple hyperplanes</li><li><strong>Linear programming</strong>: Represent equality constraints</li><li><strong>Support vector machines</strong>: Separating hyperplanes</li></ul><p><strong>Optimization Context</strong></p><p>Commonly appears in constrained optimization:</p><pre><code class="language-julia hljs">minimize f(x)
subject to ⟨a,x⟩ = b  # Represented by IndicatorHyperplane</code></pre><p>Or in penalty/augmented Lagrangian methods:</p><pre><code class="language-julia hljs">minimize f(x) + (μ/2)||⟨a,x⟩ - b||² + IndicatorHyperplane(a,b)(x)</code></pre><p><strong>Projection Formula</strong></p><p>The projection of point x onto hyperplane H = {z : ⟨a,z⟩ = b} is:</p><pre><code class="nohighlight hljs">P_H(x) = x - ((⟨a,x⟩ - b) / ||a||²) · a</code></pre><p>This formula:</p><ol><li>Computes the signed distance from x to the hyperplane: (⟨a,x⟩ - b) / ||a||</li><li>Moves x by this distance in the negative normal direction: a / ||a||²</li></ol><p><strong>Special Cases</strong></p><ul><li><strong>Origin-centered hyperplane</strong>: intercept = 0 gives ⟨a,x⟩ = 0</li><li><strong>Coordinate hyperplane</strong>: slope = eᵢ gives xᵢ = intercept</li><li><strong>45-degree line (2D)</strong>: slope = [1,1] gives x₁ + x₂ = intercept</li></ul><p><strong>Numerical Considerations</strong></p><ul><li><strong>Non-zero slope</strong>: Constructor validates ||slope|| &gt; ZeroTolerance</li><li><strong>Scaling invariance</strong>: (α·slope, α·intercept) represents the same hyperplane</li><li><strong>Numerical stability</strong>: Pre-computed scaledSlope avoids numerical issues</li><li><strong>Tolerance</strong>: Uses FeasTolerance for feasibility checking</li></ul><p><strong>Relationship to Other Constraints</strong></p><ul><li><strong>Linear subspace</strong>: IndicatorLinearSubspace for Ax = b (multiple equations)</li><li><strong>Half-space</strong>: Related to ⟨a,x⟩ ≤ b constraints</li><li><strong>Box constraints</strong>: Coordinate hyperplanes form box constraint boundaries</li><li><strong>Polytopes</strong>: Intersection of multiple hyperplanes and half-spaces</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorHyperplane.jl#L1-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorLinearSubspace" href="#PDMO.IndicatorLinearSubspace"><code>PDMO.IndicatorLinearSubspace</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorLinearSubspace(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64})</code></pre><p>Represents the indicator function of a linear subspace defined by the equation Ax = b.</p><p>The indicator function is defined as:</p><p class="math-container">\[I_{Ax=b}(x) = \begin{cases}
0 &amp; \text{if } Ax = b \\
\infty &amp; \text{otherwise}
\end{cases}\]</p><p><strong>Arguments</strong></p><ul><li><code>A::SparseMatrixCSC{Float64, Int64}</code>: The constraint matrix</li><li><code>b::Vector{Float64}</code>: The right-hand side vector</li></ul><p><strong>Fields</strong></p><ul><li><code>A</code>: The constraint matrix</li><li><code>b</code>: The right-hand side vector</li><li><code>U</code>: Left singular vectors from SVD decomposition</li><li><code>S</code>: Singular values from SVD decomposition</li><li><code>V</code>: Right singular vectors from SVD decomposition</li><li><code>rank</code>: Numerical rank of matrix A</li><li><code>isFullRank</code>: Boolean indicating if A has full row rank</li><li><code>projectionMatrix</code>: Pre-computed matrix for projection:<ul><li>If full rank: stores (AA&#39;)^{-1}</li><li>If rank deficient: stores A^+ (pseudoinverse)</li></ul></li></ul><p><strong>Notes</strong></p><ul><li>SVD decomposition and projection matrices are computed once during initialization for efficiency</li><li>The proximal operator (projection) is computed differently based on whether A has full row rank:<ul><li>For full rank: y = x - A&#39;(AA&#39;)^{-1}(Ax - b)</li><li>For rank deficient: y = x - A^+(Ax - b), where A^+ is the pseudoinverse</li></ul></li><li>Numerical rank is determined using a tolerance based on machine epsilon</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = sparse([1.0 2.0; 3.0 4.0])
b = [1.0, 2.0]
f = IndicatorLinearSubspace(A, b)
x = [0.0, 0.0]
proj = proximalOracle(f, x)  # Projects x onto the subspace Ax = b</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorLinearSubspace.jl#L1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorNonnegativeOrthant" href="#PDMO.IndicatorNonnegativeOrthant"><code>PDMO.IndicatorNonnegativeOrthant</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorNonnegativeOrthant()</code></pre><p>Indicator function of the nonnegative orthant ℝⁿ₊.</p><p><strong>Mathematical Definition</strong></p><p>The indicator function of the nonnegative orthant:</p><pre><code class="nohighlight hljs">I_{ℝⁿ₊}(x) = 0     if xᵢ ≥ 0 for all i ∈ {1,...,n}
I_{ℝⁿ₊}(x) = +∞    otherwise</code></pre><p>This represents the constraint that all components of x must be non-negative.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">IndicatorNonnegativeOrthant()</code></pre><p>No parameters needed - the constraint applies to any dimensional input.</p><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, not differentiable at the boundary (xᵢ = 0)</li><li><strong>Convex</strong>: Yes, the nonnegative orthant is a convex cone</li><li><strong>Proximal</strong>: Yes, proximal operator is element-wise projection</li><li><strong>Set Indicator</strong>: Yes, represents the constraint set ℝⁿ₊</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Constraint set</strong>: ℝⁿ₊ = {x ∈ ℝⁿ : xᵢ ≥ 0 ∀i}</li><li><strong>Geometric interpretation</strong>: First orthant in ℝⁿ (all coordinates non-negative)</li><li><strong>Proximal operator</strong>: Element-wise projection: [prox_f(x)]ᵢ = max(xᵢ, 0)</li><li><strong>Conjugate function</strong>: Sum of negative parts: f*(y) = ∑ᵢ max(-yᵢ, 0)</li></ul><p><strong>Geometric Interpretation</strong></p><ul><li><strong>Orthant</strong>: One of 2ⁿ orthants in ℝⁿ (the &quot;positive&quot; one)</li><li><strong>Boundary</strong>: Coordinate hyperplanes where xᵢ = 0</li><li><strong>Interior</strong>: Points where xᵢ &gt; 0 for all i</li><li><strong>Convex cone</strong>: Closed under positive linear combinations</li></ul><p><strong>Implementation Details</strong></p><ul><li><strong>Dimension-agnostic</strong>: Works with any input dimension</li><li><strong>Tolerance-aware</strong>: Uses FeasTolerance for numerical feasibility checks</li><li><strong>Parallel support</strong>: Implements parallel computation for large vectors</li><li><strong>Memory efficient</strong>: In-place operations, no temporary allocations</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create constraint function
f = IndicatorNonnegativeOrthant()

# Check feasibility
x_feasible = [1.0, 2.0, 0.0]  # All components ≥ 0
val = f(x_feasible)  # Returns 0.0

x_infeasible = [1.0, -1.0, 2.0]  # Contains negative component
val = f(x_infeasible)  # Returns +∞

# Project onto nonnegative orthant
x = [2.0, -3.0, 0.5, -1.0]
x_proj = proximalOracle(f, x)  # Returns [2.0, 0.0, 0.5, 0.0]

# In-place projection (more efficient)
x = [2.0, -3.0, 0.5, -1.0]
y = similar(x)
proximalOracle!(y, f, x)  # y = [2.0, 0.0, 0.5, 0.0]

# Works with any dimension
x_1d = [-5.0]
proj_1d = proximalOracle(f, x_1d)  # Returns [0.0]

x_high_dim = randn(1000)  # Random 1000-dimensional vector
proj_high = proximalOracle(f, x_high_dim)  # Projects all components

# Parallel computation for large vectors
x_large = randn(10000)
proj_parallel = proximalOracle(f, x_large, 1.0, true)  # enableParallel=true</code></pre><p><strong>Algorithm Applications</strong></p><ul><li><strong>Non-negativity constraints</strong>: Enforce xᵢ ≥ 0 in optimization</li><li><strong>Projected gradient descent</strong>: Project gradient steps onto feasible region</li><li><strong>ADMM</strong>: Enforce non-negativity in alternating minimization</li><li><strong>Support vector machines</strong>: Non-negative dual variables</li><li><strong>Non-negative matrix factorization</strong>: Constrain factor matrices</li><li><strong>Portfolio optimization</strong>: Non-negative asset weights</li><li><strong>Image processing</strong>: Non-negative pixel intensities</li><li><strong>Compressed sensing</strong>: Sparse non-negative signal recovery</li></ul><p><strong>Optimization Context</strong></p><p>Commonly appears in constrained optimization:</p><pre><code class="language-julia hljs">minimize f(x)
subject to x ≥ 0  # Component-wise non-negativity</code></pre><p>This constraint is often handled via:</p><ol><li><strong>Projected methods</strong>: Project iterates onto ℝⁿ₊</li><li><strong>Penalty methods</strong>: Add barrier/penalty terms</li><li><strong>Primal-dual methods</strong>: Use dual variables for constraints</li></ol><p><strong>Projection Properties</strong></p><p>The projection P_{ℝⁿ₊}(x) has several important properties:</p><ul><li><strong>Idempotent</strong>: P(P(x)) = P(x)</li><li><strong>Non-expansive</strong>: ||P(x) - P(y)|| ≤ ||x - y||</li><li><strong>Separable</strong>: [P(x)]ᵢ depends only on xᵢ</li><li><strong>Monotone</strong>: xᵢ ≤ yᵢ ⟹ [P(x)]ᵢ ≤ [P(y)]ᵢ</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Function evaluation</strong>: O(n) complexity</li><li><strong>Proximal operator</strong>: O(n) complexity</li><li><strong>Memory usage</strong>: O(1) additional storage</li><li><strong>Parallel scaling</strong>: Excellent (independent components)</li></ul><p><strong>Special Cases and Extensions</strong></p><ul><li><strong>Box constraints</strong>: Combine with upper bounds for IndicatorBox</li><li><strong>Simplex constraints</strong>: Add ∑xᵢ = 1 constraint</li><li><strong>Cone constraints</strong>: Extension to more general convex cones</li><li><strong>Integer constraints</strong>: Combine with integrality requirements</li></ul><p><strong>Numerical Considerations</strong></p><ul><li><strong>Tolerance handling</strong>: Uses FeasTolerance for boundary points</li><li><strong>Floating-point precision</strong>: Robust to small numerical errors</li><li><strong>Parallel threshold</strong>: Uses parallelization for vectors &gt; 1000 elements</li><li><strong>Zero preservation</strong>: Exactly preserves zero values (no numerical drift)</li></ul><p><strong>Relationship to Other Constraints</strong></p><ul><li><strong>IndicatorBox</strong>: Generalization with both lower and upper bounds</li><li><strong>IndicatorBallL2</strong>: Different geometry (curved vs. polyhedral boundary)</li><li><strong>IndicatorSOC</strong>: Generalization to second-order cone constraints</li><li><strong>Linear inequalities</strong>: Subset of general linear inequality constraints</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorNonnegativeOrthant.jl#L1-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorPSD" href="#PDMO.IndicatorPSD"><code>PDMO.IndicatorPSD</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorPSD(dim::Int64)</code></pre><p>Represents the indicator function of the positive semidefinite cone.</p><p>The indicator function of the positive semidefinite cone is defined as:</p><p class="math-container">\[I_{\mathcal{S}_+^n}(X) = \begin{cases}
0 &amp; \text{if } X \in \mathcal{S}_+^n \\
\infty &amp; \text{otherwise}
\end{cases}\]</p><p>where <span>$\mathcal{S}_+^n = \{X \in \mathbb{R}^{n \times n} : X = X^T, X \succeq 0\}$</span> is the cone of  symmetric positive semidefinite matrices.</p><p><strong>Mathematical Properties</strong></p><ul><li><strong>Convex</strong>: The PSD cone is convex, making this a convex indicator function</li><li><strong>Closed</strong>: The PSD cone is closed, ensuring well-defined projections</li><li><strong>Self-dual</strong>: The PSD cone is self-dual under the trace inner product</li><li><strong>Proximal</strong>: The proximal operator corresponds to projection onto the PSD cone</li></ul><p><strong>Arguments</strong></p><ul><li><code>dim::Int64</code>: The dimension n of the n×n matrix space (must be ≥ 1)</li></ul><p><strong>Fields</strong></p><ul><li><code>dim::Int64</code>: Matrix dimension (n×n)</li><li><code>X::Matrix{Float64}</code>: Internal buffer for dense matrix computations</li></ul><p><strong>Function Properties</strong></p><ul><li><code>isProximal(IndicatorPSD)</code>: <code>true</code> - admits efficient proximal operator</li><li><code>isConvex(IndicatorPSD)</code>: <code>true</code> - indicator of convex cone</li><li><code>isSet(IndicatorPSD)</code>: <code>true</code> - indicator function of a set</li></ul><p><strong>Proximal Operator</strong></p><p>The proximal operator (projection onto PSD cone) is computed via:</p><ol><li><strong>Symmetrization</strong>: Ensure input matrix is symmetric: <span>$\bar{X} = (X + X^T)/2$</span></li><li><strong>Eigendecomposition</strong>: Compute <span>$\bar{X} = Q\Lambda Q^T$</span></li><li><strong>Projection</strong>: Set negative eigenvalues to zero: <span>$\Lambda_+ = \max(\Lambda, 0)$</span></li><li><strong>Reconstruction</strong>: Return <span>$\text{proj}(X) = Q\Lambda_+ Q^T$</span></li></ol><p><strong>Implementation Details</strong></p><ul><li>Supports both dense (<code>Matrix{Float64}</code>) and sparse (<code>SparseMatrixCSC{Float64}</code>) matrices</li><li>Uses symmetric eigendecomposition for numerical stability</li><li>Enforces perfect symmetry in output to avoid numerical artifacts</li><li>Tolerances: <code>ZeroTolerance</code> for symmetry, <code>FeasTolerance</code> for positive semidefiniteness</li></ul><p><strong>Applications</strong></p><ul><li><strong>Semidefinite Programming</strong>: Constraint X ⪰ 0 in optimization</li><li><strong>Covariance Estimation</strong>: Ensuring positive semidefinite covariance matrices</li><li><strong>Gram Matrix Constraints</strong>: In kernel methods and matrix completion</li><li><strong>Relaxations</strong>: Convex relaxation of rank constraints</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create 3×3 PSD indicator
f = IndicatorPSD(3)

# Test with positive semidefinite matrix
X_psd = [2.0 1.0 0.0; 1.0 2.0 0.0; 0.0 0.0 1.0]
@assert f(X_psd) == 0.0  # In PSD cone

# Test with indefinite matrix
X_indef = [1.0 2.0; 2.0 1.0]  # Has negative eigenvalue
@assert f(X_indef) == Inf  # Not in PSD cone

# Project indefinite matrix onto PSD cone
X_proj = proximalOracle(f, X_indef)
@assert f(X_proj) == 0.0  # Projection is PSD</code></pre><p><strong>Performance Notes</strong></p><ul><li>Eigendecomposition is O(n³) - dominant computational cost</li><li>Dense matrices preferred for efficiency; sparse matrices converted internally</li><li>Pre-allocated buffers reduce memory allocation overhead</li><li>Symmetric views used when possible to exploit structure</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorPSD.jl#L1-L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorRotatedSOC" href="#PDMO.IndicatorRotatedSOC"><code>PDMO.IndicatorRotatedSOC</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorRotatedSOC(dim::Int64)</code></pre><p>Represents the indicator function of the rotated second-order cone (also known as the quadratic cone).</p><p>The indicator function of the rotated second-order cone is defined as:</p><p class="math-container">\[I_{\mathcal{Q}^n}(x) = \begin{cases}
0 &amp; \text{if } x \in \mathcal{Q}^n \\
\infty &amp; \text{otherwise}
\end{cases}\]</p><p>where the rotated second-order cone is defined as:</p><p class="math-container">\[\mathcal{Q}^n = \{(u,v,w) \in \mathbb{R} \times \mathbb{R} \times \mathbb{R}^{n-2} : \|w\|_2^2 \leq 2uv, u \geq 0, v \geq 0\}\]</p><p>for <span>$n \geq 3$</span>, where <span>$u = x_1$</span>, <span>$v = x_2$</span>, and <span>$w = x_{3:n}$</span>.</p><p><strong>Mathematical Properties</strong></p><ul><li><strong>Convex</strong>: The rotated SOC is a convex cone, making this a convex indicator function</li><li><strong>Self-dual</strong>: The rotated SOC is self-dual under the standard inner product</li><li><strong>Closed</strong>: The rotated SOC is closed, ensuring well-defined projections</li><li><strong>Pointed</strong>: Has a non-empty interior and is pointed (contains no lines)</li></ul><p><strong>Arguments</strong></p><ul><li><code>dim::Int64</code>: The dimension n of the ambient space (must be ≥ 3)</li></ul><p><strong>Fields</strong></p><ul><li><code>dim::Int64</code>: Dimension of the ambient space</li><li><code>sqrt2::Float64</code>: Pre-computed √2 constant for efficiency</li></ul><p><strong>Function Properties</strong></p><ul><li><code>isProximal(IndicatorRotatedSOC)</code>: <code>true</code> - admits efficient proximal operator</li><li><code>isConvex(IndicatorRotatedSOC)</code>: <code>true</code> - indicator of convex cone</li><li><code>isSet(IndicatorRotatedSOC)</code>: <code>true</code> - indicator function of a set</li></ul><p><strong>Proximal Operator</strong></p><p>The proximal operator (projection onto rotated SOC) is computed via transformation to standard SOC:</p><ol><li><p><strong>Transform to standard SOC</strong>: For point <span>$(u,v,w)$</span>, compute:</p><ul><li><span>$s = u + v$</span> (sum)</li><li><span>$t = u - v$</span> (difference)  </li><li><span>$z = \sqrt{2}w$</span> (scaled vector)</li></ul></li><li><p><strong>Project onto standard SOC</strong>: Project <span>$(s,t,z)$</span> onto <span>$\{(s,t,z) : \|(t,z)\|_2 \leq s\}$</span></p></li><li><p><strong>Transform back</strong>: From projected <span>$(s&#39;,t&#39;,z&#39;)$</span>, compute:</p><ul><li><span>$u&#39; = \max(0, (s&#39; + t&#39;)/2)$</span></li><li><span>$v&#39; = \max(0, (s&#39; - t&#39;)/2)$</span></li><li><span>$w&#39; = z&#39;/\sqrt{2}$</span></li></ul></li></ol><p><strong>Implementation Details</strong></p><ul><li>Uses linear transformation to reduce to standard SOC projection</li><li>Enforces non-negativity constraints explicitly after transformation</li><li>Pre-computes √2 for numerical efficiency</li><li>Handles edge cases and maintains numerical stability</li></ul><p><strong>Applications</strong></p><ul><li><strong>Quadratic Programming</strong>: Quadratic constraints <span>$x^T Q x \leq t$</span> with <span>$Q \succeq 0$</span></li><li><strong>Geometric Programming</strong>: Posynomial constraints in convex form</li><li><strong>Robust Optimization</strong>: Ellipsoidal uncertainty sets</li><li><strong>Signal Processing</strong>: Power constraints in communication systems</li><li><strong>Finance</strong>: Mean-variance portfolio optimization with transaction costs</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create 4D rotated SOC
f = IndicatorRotatedSOC(4)

# Test point inside the cone
x_in = [2.0, 1.0, 1.0, 1.0]  # ||[1.0, 1.0]||₂² = 2 ≤ 2·2·1 = 4
@assert f(x_in) == 0.0  # Inside cone

# Test point outside the cone
x_out = [1.0, 1.0, 2.0, 2.0]  # ||[2.0, 2.0]||₂² = 8 &gt; 2·1·1 = 2
@assert f(x_out) == Inf  # Outside cone

# Project onto cone
x_proj = proximalOracle(f, x_out)
@assert f(x_proj) == 0.0  # Projection is in cone

# Boundary case: on the boundary
x_bound = [1.0, 2.0, 2.0, 0.0]  # ||[2.0, 0.0]||₂² = 4 = 2·1·2
@assert f(x_bound) == 0.0  # On boundary</code></pre><p><strong>Geometric Interpretation</strong></p><p>The rotated second-order cone represents the set of points where the squared norm of the  vector part is bounded by twice the product of two non-negative scalars. Key properties:</p><ul><li><strong>Quadratic constraint</strong>: The defining constraint is quadratic in the variables</li><li><strong>Hyperbolic</strong>: The boundary forms a hyperbolic surface in higher dimensions</li><li><strong>Geometric mean</strong>: Related to the geometric mean constraint <span>$\sqrt{uv} \geq \|w\|_2/\sqrt{2}$</span></li></ul><p><strong>Relationship to Standard SOC</strong></p><p>The rotated SOC is related to the standard SOC via the linear transformation:</p><p class="math-container">\[\begin{pmatrix} u \\ v \\ w \end{pmatrix} \mapsto \begin{pmatrix} u+v \\ u-v \\ \sqrt{2}w \end{pmatrix}\]</p><p>This transformation preserves the cone structure and enables efficient projection algorithms.</p><p><strong>Performance Notes</strong></p><ul><li>Projection requires O(n) operations via SOC transformation</li><li>Memory allocation minimal - uses pre-allocated buffers</li><li>Numerically stable for well-conditioned problems</li><li>Handles degenerate cases (zero components) gracefully</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorRotatedSOC.jl#L1-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorSOC" href="#PDMO.IndicatorSOC"><code>PDMO.IndicatorSOC</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndicatorSOC(dim::Int64, radiusIndex::Int64)</code></pre><p>Represents the indicator function of the second-order cone (also known as the Lorentz cone).</p><p>The indicator function of the second-order cone is defined as:</p><p class="math-container">\[I_{\mathcal{L}^n}(x) = \begin{cases}
0 &amp; \text{if } x \in \mathcal{L}^n \\
\infty &amp; \text{otherwise}
\end{cases}\]</p><p>where the second-order cone is defined as:</p><ul><li>If <code>radiusIndex = n</code>: <span>$\mathcal{L}^n = \{x \in \mathbb{R}^n : \|x_{1:n-1}\|_2 \leq x_n\}$</span></li><li>If <code>radiusIndex = 1</code>: <span>$\mathcal{L}^n = \{x \in \mathbb{R}^n : \|x_{2:n}\|_2 \leq x_1\}$</span></li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Convex</strong>: The SOC is a convex cone, making this a convex indicator function</li><li><strong>Self-dual</strong>: The SOC is self-dual under the standard inner product</li><li><strong>Closed</strong>: The SOC is closed, ensuring well-defined projections</li><li><strong>Pointed</strong>: The SOC has a non-empty interior and is pointed (contains no lines)</li></ul><p><strong>Arguments</strong></p><ul><li><code>dim::Int64</code>: The dimension n of the ambient space (must be ≥ 2)</li><li><code>radiusIndex::Int64</code>: Position of the radius variable (must be 1 or <code>dim</code>)</li></ul><p><strong>Fields</strong></p><ul><li><code>dim::Int64</code>: Dimension of the ambient space</li><li><code>radiusIndex::Int64</code>: Index of the radius variable (1 or <code>dim</code>)</li></ul><p><strong>Function Properties</strong></p><ul><li><code>isProximal(IndicatorSOC)</code>: <code>true</code> - admits efficient proximal operator</li><li><code>isConvex(IndicatorSOC)</code>: <code>true</code> - indicator of convex cone</li><li><code>isSet(IndicatorSOC)</code>: <code>true</code> - indicator function of a set</li></ul><p><strong>Proximal Operator</strong></p><p>The proximal operator (projection onto SOC) is computed analytically:</p><p>For a point <span>$x$</span> with radius component <span>$r$</span> and vector component <span>$v$</span>:</p><ol><li>If <span>$\|v\|_2 \leq r$</span>: <span>$x$</span> is already in the cone, so <span>$\text{proj}(x) = x$</span></li><li>If <span>$\|v\|_2 \leq -r$</span>: <span>$x$</span> is in the polar cone, so <span>$\text{proj}(x) = 0$</span></li><li>Otherwise: <span>$\text{proj}(x) = \frac{\|v\|_2 + r}{2\|v\|_2}\begin{pmatrix} v \\ \|v\|_2 \end{pmatrix}$</span></li></ol><p><strong>Implementation Details</strong></p><ul><li>Projection formula handles both <code>radiusIndex = 1</code> and <code>radiusIndex = dim</code> cases</li><li>Uses tolerance <code>FeasTolerance</code> for membership testing</li><li>Efficient O(n) implementation avoiding matrix operations</li><li>Handles edge cases (zero norm, negative radius) robustly</li></ul><p><strong>Applications</strong></p><ul><li><strong>Second-Order Cone Programming (SOCP)</strong>: Constraint <span>$\|Ax + b\|_2 \leq c^T x + d$</span></li><li><strong>Robust Optimization</strong>: Uncertainty sets and robust constraints</li><li><strong>Signal Processing</strong>: Beamforming and antenna array design</li><li><strong>Machine Learning</strong>: Support vector machines with nonlinear kernels</li><li><strong>Control Theory</strong>: Linear matrix inequalities and stability analysis</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create 3D SOC with radius at last position
f = IndicatorSOC(3, 3)

# Test point inside the cone
x_in = [0.5, 0.3, 1.0]  # ||[0.5, 0.3]||₂ = 0.583 &lt; 1.0
@assert f(x_in) == 0.0  # Inside cone

# Test point outside the cone  
x_out = [2.0, 1.0, 1.0]  # ||[2.0, 1.0]||₂ = 2.236 &gt; 1.0
@assert f(x_out) == Inf  # Outside cone

# Project onto cone
x_proj = proximalOracle(f, x_out)
@assert f(x_proj) == 0.0  # Projection is in cone

# Alternative: radius at first position
f_alt = IndicatorSOC(3, 1)
x_alt = [1.0, 0.5, 0.3]  # ||[0.5, 0.3]||₂ = 0.583 &lt; 1.0
@assert f_alt(x_alt) == 0.0  # Inside cone</code></pre><p><strong>Geometric Interpretation</strong></p><p>The second-order cone is the set of points where the Euclidean norm of the vector part  is bounded by the scalar radius part. It has several equivalent representations:</p><ul><li><strong>Ice cream cone</strong>: The familiar &quot;ice cream cone&quot; shape in 3D</li><li><strong>Epigraph</strong>: Epigraph of the Euclidean norm function</li><li><strong>Intersection</strong>: Can be represented as intersection of half-spaces</li></ul><p><strong>Performance Notes</strong></p><ul><li>Projection requires O(n) operations (single norm computation)</li><li>Memory allocation minimal - uses in-place operations when possible</li><li>Numerically stable for well-conditioned problems</li><li>Handles degenerate cases (zero radius, zero vector) gracefully</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorSOC.jl#L1-L96">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.IndicatorSumOfNVariables" href="#PDMO.IndicatorSumOfNVariables"><code>PDMO.IndicatorSumOfNVariables</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(f::IndicatorSumOfNVariables)(x::NumericVariable, enableParallel::Bool=false) -&gt; Float64</code></pre><p>Evaluates the indicator function for the constraint</p><pre><code class="nohighlight hljs">x₁ + x₂ + … + xₙ = rhs</code></pre><p>If <code>rhs</code> is a scalar, then <code>x</code> is expected to be a vector of length <code>numberVariables</code> and the constraint is sum(x) ≈ rhs (within <code>FeasTolerance</code>). If <code>rhs</code> is not a scalar, then <code>x</code> is assumed to be an array whose first dimension is size(rhs, 1) * numberVariables and the remaining dimensions match <code>rhs</code>. In that case, the function reshapes each block (along the first dimension) to the shape of <code>rhs</code>, sums them elementwise, and compares the result with <code>rhs</code>. Returns 0.0 if the constraint is satisfied (within tolerance) and <code>Inf</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/IndicatorSumOfNVariables.jl#L26-L40">source</a></section></article><h3 id="User-Defined-Functions"><a class="docs-heading-anchor" href="#User-Defined-Functions">User-Defined Functions</a><a id="User-Defined-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#User-Defined-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.ComponentwiseExponentialFunction" href="#PDMO.ComponentwiseExponentialFunction"><code>PDMO.ComponentwiseExponentialFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ComponentwiseExponentialFunction(coefficients::Vector{Float64})</code></pre><p>Represents a component-wise exponential function f(x) = ∑ᵢ aᵢ exp(xᵢ).</p><p><strong>Mathematical Definition</strong></p><p>f(x) = ∑ᵢ aᵢ exp(xᵢ)</p><p>where a = coefficients is a vector of non-negative weights.</p><p><strong>Arguments</strong></p><ul><li><code>coefficients::Vector{Float64}</code>: Vector of non-negative coefficients</li></ul><p><strong>Constructors</strong></p><ul><li><code>ComponentwiseExponentialFunction(coefficients)</code>: With specified coefficients</li><li><code>ComponentwiseExponentialFunction(n::Int64)</code>: With unit coefficients (ones vector)</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, exponential functions are infinitely differentiable</li><li><strong>Convex</strong>: Yes, exponential functions are convex</li><li><strong>Proximal</strong>: No, proximal operator requires Lambert W function (not implemented)</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Gradient</strong>: ∇f(x) = [a₁ exp(x₁), a₂ exp(x₂), ..., aₙ exp(xₙ)]</li><li><strong>Hessian</strong>: ∇²f(x) = diag(a₁ exp(x₁), a₂ exp(x₂), ..., aₙ exp(xₙ))</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Standard exponential function f(x) = exp(x₁) + exp(x₂)
f = ComponentwiseExponentialFunction([1.0, 1.0])
x = [0.0, 1.0]
val = f(x)  # Returns exp(0) + exp(1) = 1 + e ≈ 3.718

# Weighted exponential function f(x) = 2exp(x₁) + 3exp(x₂)
f = ComponentwiseExponentialFunction([2.0, 3.0])
x = [0.0, 0.0]
val = f(x)  # Returns 2*1 + 3*1 = 5

# Gradient computation
grad = gradientOracle(f, x)  # Returns [2.0, 3.0]</code></pre><p><strong>Applications</strong></p><ul><li>Exponential utility functions</li><li>Log-sum-exp approximations</li><li>Entropic regularization</li><li>Barrier methods in optimization</li><li>Statistical modeling (exponential families)</li></ul><p><strong>Note</strong></p><p>The proximal operator requires the Lambert W function and is not currently implemented.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/ComponentwiseExponentialFunction.jl#L1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.UserDefinedProximalFunction" href="#PDMO.UserDefinedProximalFunction"><code>PDMO.UserDefinedProximalFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UserDefinedProximalFunction(func::Function, proximalFunc::Function, convex::Bool=true)</code></pre><p>Wrapper for user-defined functions that have a proximal operator.</p><p>This allows users to define custom functions by providing both the function evaluation and its proximal operator, enabling integration with proximal algorithms.</p><p><strong>Arguments</strong></p><ul><li><code>func::Function</code>: Function evaluation f(x) → Float64</li><li><code>proximalFunc::Function</code>: Proximal operator (x, γ) → result</li><li><code>convex::Bool=true</code>: Whether the function is convex</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: No, proximal functions are typically non-smooth</li><li><strong>Convex</strong>: User-specified (default true)</li><li><strong>Proximal</strong>: Yes, by definition</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Function evaluation</strong>: f(x) provided by user</li><li><strong>Proximal operator</strong>: prox_γf(x) provided by user</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># L1 Norm Function
# f(x) = λ||x||₁ (L1 norm with coefficient λ)
λ = 0.5
func = x -&gt; λ * sum(abs.(x))
proximalFunc = (x, gamma) -&gt; sign.(x) .* max.(abs.(x) .- gamma * λ, 0.0)  # Soft thresholding
f = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true

# Indicator Function of Box Constraints
# f(x) = I_{[a,b]}(x) (indicator function of box [a,b])
a, b = -1.0, 1.0
func = x -&gt; all(a .&lt;= x .&lt;= b) ? 0.0 : Inf
proximalFunc = (x, gamma) -&gt; clamp.(x, a, b)  # Projection onto box
f = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true

# Custom Regularization Function
# f(x) = α * g(x) where g has known proximal operator
α = 0.1
func = x -&gt; α * myCustomFunction(x)
proximalFunc = (x, gamma) -&gt; myCustomProximal(x, gamma * α)
f = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true</code></pre><p><strong>Integration with Bipartization</strong></p><pre><code class="language-julia hljs"># In your optimization problems
block_x = BlockVariable(xID)
block_x.f = UserDefinedProximalFunction(myFunc, myProximal, true)
addBlockVariable!(nlp, block_x)</code></pre><p><strong>Requirements</strong></p><ul><li><code>func(x)</code> must return a Float64 value</li><li><code>proximalFunc(x, gamma)</code> must return a result of the same type as x</li><li>Both functions must be consistent with the mathematical definition</li><li>The proximal operator must satisfy: prox<em>γf(x) = argmin</em>z { f(z) + (1/(2γ))||z - x||² }</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/UserDefinedProximalFunction.jl#L1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.UserDefinedSmoothFunction" href="#PDMO.UserDefinedSmoothFunction"><code>PDMO.UserDefinedSmoothFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UserDefinedSmoothFunction(func::Function, gradientFunc::Function, convex::Bool=true)</code></pre><p>Wrapper for user-defined smooth functions that have a gradient.</p><p>This allows users to define custom smooth functions by providing both the function  evaluation and its gradient, enabling integration with gradient-based algorithms.</p><p><strong>Arguments</strong></p><ul><li><code>func::Function</code>: Function evaluation f(x) → Float64</li><li><code>gradientFunc::Function</code>: Gradient function x → ∇f(x)</li><li><code>convex::Bool=true</code>: Whether the function is convex</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Yes, by definition</li><li><strong>Convex</strong>: User-specified (default true)</li><li><strong>Proximal</strong>: No, user-defined functions typically don&#39;t have proximal oracles</li></ul><p><strong>Mathematical Properties</strong></p><ul><li><strong>Function evaluation</strong>: f(x) provided by user</li><li><strong>Gradient</strong>: ∇f(x) provided by user</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Simple Quadratic Function
# f(x) = x₁² + 2x₂² + x₁x₂
func = x -&gt; x[1]^2 + 2*x[2]^2 + x[1]*x[2]
gradientFunc = x -&gt; [2*x[1] + x[2], 4*x[2] + x[1]]
f = UserDefinedSmoothFunction(func, gradientFunc, true)  # convex=true

# Non-convex Function
# f(x) = sin(x₁) + cos(x₂)
func = x -&gt; sin(x[1]) + cos(x[2])
gradientFunc = x -&gt; [cos(x[1]), -sin(x[2])]
f = UserDefinedSmoothFunction(func, gradientFunc, false)  # convex=false

# Rosenbrock Function (classic optimization test function)
# f(x) = (1-x₁)² + 100(x₂-x₁²)²
func = x -&gt; (1-x[1])^2 + 100*(x[2]-x[1]^2)^2
gradientFunc = x -&gt; [-2*(1-x[1]) - 400*x[1]*(x[2]-x[1]^2), 200*(x[2]-x[1]^2)]
f = UserDefinedSmoothFunction(func, gradientFunc, false)  # non-convex</code></pre><p><strong>Integration with Bipartization</strong></p><pre><code class="language-julia hljs"># In your optimization problems
block_x = BlockVariable(xID)
block_x.f = UserDefinedSmoothFunction(myFunc, myGradient, true)
addBlockVariable!(nlp, block_x)</code></pre><p><strong>Requirements</strong></p><ul><li><code>func(x)</code> must return a Float64 value</li><li><code>gradientFunc(x)</code> must return a gradient of the same type as x</li><li>Both functions must be consistent with the mathematical definition</li><li>The gradient must satisfy: ∇f(x) = lim_{h→0} [f(x+h) - f(x)]/h</li><li>For correctness, consider using automatic differentiation tools to compute gradients</li></ul><p><strong>Applications</strong></p><ul><li>Custom objective functions</li><li>Domain-specific regularization terms</li><li>Physics-informed optimization</li><li>Machine learning loss functions</li><li>Engineering design optimization</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/UserDefinedSmoothFunction.jl#L1-L65">source</a></section></article><h3 id="Wrapper-Functions"><a class="docs-heading-anchor" href="#Wrapper-Functions">Wrapper Functions</a><a id="Wrapper-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Wrapper-Functions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.WrapperScalarInputFunction" href="#PDMO.WrapperScalarInputFunction"><code>PDMO.WrapperScalarInputFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WrapperScalarInputFunction(originalFunction::AbstractFunction)</code></pre><p>Wrapper that adapts scalar-input functions to work with vector interfaces.</p><p>This wrapper takes a function that operates on scalars and adapts it to work with 1-dimensional vectors, enabling integration with vector-based optimization algorithms.</p><p><strong>Arguments</strong></p><ul><li><code>originalFunction::AbstractFunction</code>: Function that operates on scalar inputs</li></ul><p><strong>Properties</strong></p><ul><li><strong>Smooth</strong>: Inherits from original function</li><li><strong>Convex</strong>: Inherits from original function  </li><li><strong>Proximal</strong>: Inherits from original function</li><li><strong>Set</strong>: Inherits from original function</li></ul><p><strong>Mathematical Properties</strong></p><p>All properties are inherited from the original function, with input/output adaptation:</p><ul><li>Function evaluation: f([x]) = originalFunction(x)</li><li>Gradient: ∇f([x]) = [∇originalFunction(x)]</li><li>Proximal operator: prox<em>f([x]) = [prox</em>originalFunction(x)]</li></ul><p><strong>Input/Output Format</strong></p><ul><li><strong>Input</strong>: Vector of length 1, e.g., [x]</li><li><strong>Output</strong>: Scalar for function evaluation, vector of length 1 for gradient/proximal</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Adapt a scalar indicator function to vector interface
# Original function: f(x) = I_{[0,1]}(x) for scalar x
original_f = IndicatorBox(0.0, 1.0)  # Scalar version
vector_f = WrapperScalarInputFunction(original_f)

# Usage with vector input
x = [0.5]  # Vector of length 1
val = vector_f(x)  # Returns 0.0
grad = gradientOracle(vector_f, x)  # Returns [0.0] (if smooth)
prox = proximalOracle(vector_f, x)  # Returns [0.5] (projection)

# Invalid usage
x = [0.5, 1.0]  # Vector of length 2
# val = vector_f(x)  # ERROR: input must be vector of length 1</code></pre><p><strong>Use Cases</strong></p><ul><li>Adapting scalar functions for vector-based algorithms</li><li>Interfacing with optimization solvers that expect vector inputs</li><li>Building composite functions with mixed scalar/vector components</li><li>Legacy code integration</li></ul><p><strong>Limitations</strong></p><ul><li>Only works with vector inputs of length 1</li><li>Adds slight computational overhead due to wrapping</li><li>Error checking is performed at runtime</li><li>Original function must properly handle scalar inputs</li></ul><p><strong>Implementation Notes</strong></p><p>The wrapper performs the following adaptations:</p><ol><li>Extracts scalar from length-1 vector input</li><li>Calls original function with scalar</li><li>Wraps scalar result back into vector format (for gradients/proximal)</li><li>Delegates all trait checks to the original function</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalarInputFunction.jl#L1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.WrapperScalingTranslationFunction" href="#PDMO.WrapperScalingTranslationFunction"><code>PDMO.WrapperScalingTranslationFunction</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WrapperScalingTranslationFunction</code></pre><p>A wrapper that represents a transformed function of the form f(x) = g(coe·x + translation), where g is the original function, coe is a positive scaling coefficient, and translation is an additive shift.</p><p>This transformation is useful for:</p><ul><li>Scaling and shifting existing functions</li><li>Implementing variable transformations in optimization</li><li>Creating scaled versions of constraint functions</li><li>Handling affine transformations of variables</li></ul><p><strong>Fields</strong></p><ul><li><code>originalFunction::AbstractFunction</code>: The original function g</li><li><code>coe::Float64</code>: The scaling coefficient (must be positive)</li><li><code>translation::NumericVariable</code>: The translation vector/scalar</li><li><code>buffer::NumericVariable</code>: Internal buffer for computations</li></ul><p><strong>Mathematical Properties</strong></p><p>For f(x) = g(coe·x + translation):</p><ul><li>Function evaluation: f(x) = g(coe·x + translation)</li><li>Gradient (if g is smooth): ∇f(x) = coe · ∇g(coe·x + translation)</li><li>Proximal operator: prox<em>{γf}(z) = (prox</em>{γ·coe², g}(translation + coe·z) - translation) / coe</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create a scaled and shifted L2 ball: ||2x + [1,1]||₂ ≤ 1
g = IndicatorBallL2(1.0)
f = WrapperScalingTranslationFunction(g, 2.0, [1.0, 1.0])

# Create a shifted quadratic: (1/2)(x - 2)²
g = QuadraticFunction(1.0, 0.0, 0.0)  # (1/2)x²
f = WrapperScalingTranslationFunction(g, 1.0, -2.0)  # (1/2)(x + (-2))² = (1/2)(x - 2)²</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/9d6891e3437e5f93b3cf4e6c8d96528235ed5716/src/Components/Functions/WrapperScalingTranslationFunction.jl#L1-L36">source</a></section></article><pre><code class="nohighlight hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../formulations/">« Formulations</a><a class="docs-footer-nextpage" href="../mappings/">Mappings »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 22 July 2025 20:18">Tuesday 22 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
