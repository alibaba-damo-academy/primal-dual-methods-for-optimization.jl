<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>ADMM · PDMO.jl Documentation</title><meta name="title" content="ADMM · PDMO.jl Documentation"/><meta property="og:title" content="ADMM · PDMO.jl Documentation"/><meta property="twitter:title" content="ADMM · PDMO.jl Documentation"/><meta name="description" content="Documentation for PDMO.jl Documentation."/><meta property="og:description" content="Documentation for PDMO.jl Documentation."/><meta property="twitter:description" content="Documentation for PDMO.jl Documentation."/><meta property="og:url" content="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/admm/"/><meta property="twitter:url" content="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/admm/"/><link rel="canonical" href="https://alibaba-damo-academy.github.io/PDMO.jl/S4_api/admm/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PDMO.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../S1_getting_started/">Getting Started</a></li><li><span class="tocitem">Algorithms</span><ul><li><a class="tocitem" href="../../S2_algorithms/AdaPDM/">AdaPDM</a></li><li><a class="tocitem" href="../../S2_algorithms/ADMM/">ADMM</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../S3_examples/DistributedOPF/">Distributed OPF</a></li><li><a class="tocitem" href="../../S3_examples/DualLasso/">Dual Lasso</a></li><li><a class="tocitem" href="../../S3_examples/DualSVM/">Dual SVM</a></li><li><a class="tocitem" href="../../S3_examples/FusedLasso/">Fused Lasso</a></li><li><a class="tocitem" href="../../S3_examples/LeastL1Norm/">Least L1 Norm</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../main/">Main</a></li><li><a class="tocitem" href="../formulations/">Formulations</a></li><li><a class="tocitem" href="../functions/">Functions</a></li><li><a class="tocitem" href="../mappings/">Mappings</a></li><li class="is-active"><a class="tocitem" href>ADMM</a><ul class="internal"><li><a class="tocitem" href="#Parameters-and-Iteration-Information"><span>Parameters and Iteration Information</span></a></li><li><a class="tocitem" href="#Subproblem-Solvers"><span>Subproblem Solvers</span></a></li><li><a class="tocitem" href="#Adapters"><span>Adapters</span></a></li><li><a class="tocitem" href="#Accelerators"><span>Accelerators</span></a></li></ul></li><li><a class="tocitem" href="../pdm/">AdaPDM</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>ADMM</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>ADMM</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/alibaba-damo-academy/PDMO.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/main/docs/src/S4_api/admm.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ADMM"><a class="docs-heading-anchor" href="#ADMM">ADMM</a><a id="ADMM-1"></a><a class="docs-heading-anchor-permalink" href="#ADMM" title="Permalink"></a></h1><p>This page documents the Alternating Direction Method of Multipliers (ADMM) algorithm components in PDMO.jl.</p><h2 id="Parameters-and-Iteration-Information"><a class="docs-heading-anchor" href="#Parameters-and-Iteration-Information">Parameters and Iteration Information</a><a id="Parameters-and-Iteration-Information-1"></a><a class="docs-heading-anchor-permalink" href="#Parameters-and-Iteration-Information" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.ADMMParam" href="#PDMO.ADMMParam"><code>PDMO.ADMMParam</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ADMMParam</code></pre><p>Parameters for the Alternating Direction Method of Multipliers (ADMM) algorithm.</p><p>ADMM solves constrained optimization problems of the form:</p><pre><code class="nohighlight hljs">minimize    f(x) + g(y)
subject to  Ax + By = c</code></pre><p>The algorithm iteratively updates primal variables x, y and dual variable λ (Lagrange multiplier) using the augmented Lagrangian with penalty parameter ρ.</p><p><strong>Fields</strong></p><p><strong>Convergence Parameters</strong></p><ul><li><p><code>initialRho::Float64</code>: Initial penalty parameter ρ &gt; 0. Controls the weight of the constraint  violation penalty in the augmented Lagrangian. Larger values enforce constraints more strictly  but may slow convergence. Default: 10.0</p></li><li><p><code>maxIter::Int64</code>: Maximum number of ADMM iterations before termination. Default: 100000</p></li><li><p><code>presTolL2::Float64</code>: Primal residual tolerance in L2 norm. Terminates when  ||Ax + By - c||₂ ≤ presTolL2. Default: 1e-4</p></li><li><p><code>dresTolL2::Float64</code>: Dual residual tolerance in L2 norm. Terminates when  ||ρAᵀ(λᵏ⁺¹ - λᵏ)||₂ ≤ dresTolL2. Default: 1e-4</p></li><li><p><code>presTolLInf::Float64</code>: Primal residual tolerance in L∞ norm. Alternative termination  criterion: ||Ax + By - c||∞ ≤ presTolLInf. Default: 1e-6</p></li><li><p><code>dresTolLInf::Float64</code>: Dual residual tolerance in L∞ norm. Alternative termination  criterion for dual residual. Default: 1e-6</p></li></ul><p><strong>Algorithm Components</strong></p><ul><li><p><code>solver::AbstractADMMSubproblemSolver</code>: Strategy for solving the x and y subproblems:</p><ul><li><code>DoublyLinearizedSolver()</code>: Linearizes both subproblems for faster iterations</li><li><code>OriginalADMMSubproblemSolver()</code>: Solves subproblems exactly (slower but more accurate)</li><li><code>AdaptiveLinearizedSolver()</code>: Adaptive linearization with dynamic step size control</li></ul><p>Default: DoublyLinearizedSolver()</p></li><li><p><code>adapter::AbstractADMMAdapter</code>: Strategy for dynamically adjusting ρ during iterations:</p><ul><li><code>NullAdapter()</code>: Fixed ρ throughout iterations</li><li><code>ResidualBalancingAdapter()</code>: Adjusts ρ to balance primal and dual residual magnitudes</li></ul><p>Default: NullAdapter()</p></li><li><p><code>accelerator::AbstractADMMAccelerator</code>: Acceleration scheme for faster convergence:</p><ul><li><code>NullAccelerator()</code>: Standard ADMM without acceleration</li><li><code>AndersonAccelerator()</code>: Anderson acceleration using previous iterates</li><li><code>AutoHalpernAccelerator()</code>: Automatic Halpern acceleration</li></ul><p>Default: NullAccelerator()</p></li></ul><p><strong>Practical Settings</strong></p><ul><li><p><code>logInterval::Int64</code>: Print progress information every logInterval iterations.  Set to 0 to disable logging. Default: 1000</p></li><li><p><code>timeLimit::Float64</code>: Maximum wall-clock time in seconds. Algorithm terminates if  time limit is exceeded. Default: 3600.0 (1 hour)</p></li><li><p><code>applyScaling::Bool</code>: Whether to apply problem scaling for better numerical conditioning. Default: false</p></li><li><p><code>enablePathologyCheck::Bool</code>: Whether to enable checks for pathological behavior  (e.g., divergence, numerical instability). Default: false</p></li></ul><p><strong>Constructor</strong></p><p>The constructor <code>ADMMParam(; kwargs...)</code> creates a parameter set with default values that can be customized via keyword arguments. All parameters are optional and have sensible defaults.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Default parameters
params = ADMMParam()

# Custom parameters via keyword arguments
params = ADMMParam(
    initialRho = 1.0,
    maxIter = 50000,
    presTolL2 = 1e-6,
    solver = OriginalADMMSubproblemSolver(),
    applyScaling = true
)

# Modify after construction
params = ADMMParam()
params.initialRho = 1.0
params.adapter = RBAdapter()</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMParameter.jl#L1-L90">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.ADMMIterationInfo" href="#PDMO.ADMMIterationInfo"><code>PDMO.ADMMIterationInfo</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ADMMIterationInfo</code></pre><p>Data structure to track the progress and results of ADMM iterations.</p><p><strong>Fields</strong></p><ul><li><code>presL2::Vector{Float64}</code>: Primal residual L2 norms</li><li><code>dresL2::Vector{Float64}</code>: Dual residual L2 norms</li><li><code>presLInf::Vector{Float64}</code>: Primal residual infinity norms</li><li><code>dresLInf::Vector{Float64}</code>: Dual residual infinity norms</li><li><code>obj::Vector{Float64}</code>: Objective values</li><li><code>alObj::Vector{Float64}</code>: Augmented Lagrangian objective values</li><li><code>rhoHistory::Vector{Tuple{Float64, Int64}}</code>: History of penalty parameter updates</li><li><code>primalSol::Dict{String, NumericVariable}</code>: Current primal solution</li><li><code>dualSol::Dict{String, NumericVariable}</code>: Current dual solution</li><li><code>dualSolPrev::Dict{String, NumericVariable}</code>: Previous dual solution</li><li><code>primalBuffer::Dict{String, NumericVariable}</code>: Buffer for primal computations</li><li><code>dualBuffer::Dict{String, NumericVariable}</code>: Buffer for dual computations</li><li><code>stopIter::Int64</code>: Iteration at which the algorithm stopped</li><li><code>totalTime::Float64</code>: Total computation time</li><li><code>terminationStatus::ADMMTerminationStatus</code>: Termination status</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMIterationInfo.jl#L28-L49">source</a></section></article><h2 id="Subproblem-Solvers"><a class="docs-heading-anchor" href="#Subproblem-Solvers">Subproblem Solvers</a><a id="Subproblem-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Subproblem-Solvers" title="Permalink"></a></h2><p>Implementations for ADMM subproblem solvers.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AbstractADMMSubproblemSolver" href="#PDMO.AbstractADMMSubproblemSolver"><code>PDMO.AbstractADMMSubproblemSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractADMMSubproblemSolver</code></pre><p>Abstract base type for ADMM subproblem solvers.</p><p>ADMM subproblem solvers are responsible for solving the primal variable update steps in the ADMM algorithm. Each solver implements a specific strategy for handling the optimization subproblems that arise in the ADMM decomposition.</p><p><strong>Mathematical Background</strong></p><p>The ADMM algorithm alternates between updating primal variables (x, z) and dual variables (λ). The primal subproblems take the general form:</p><p class="math-container">\[\min_x f(x) + g(x) + ⟨λ, Ax + Bz - c⟩ + \frac{ρ}{2}\|Ax + Bz - c\|_2^2\]</p><p>which can be rewritten as:</p><p class="math-container">\[\min_x f(x) + g(x) + ⟨λ, Ax⟩ + \frac{ρ}{2}(x^T A^T A x + 2⟨A^T(Bz - c), x⟩ + \text{const})\]</p><p><strong>Required Interface Methods</strong></p><p>All concrete subproblem solvers must implement:</p><ol><li><code>initialize!(solver, admmGraph, info)</code>: Initialize solver with problem structure</li><li><code>solve!(solver, nodeID, accelerator, admmGraph, info, isLeft, enableParallel)</code>: Solve subproblem for a specific node</li><li><code>updateDualResidualsInBuffer!(solver, info, admmGraph, accelerator)</code>: Compute dual residuals</li><li><code>update!(solver, info, admmGraph, rhoUpdated)</code>: Update solver state when parameters change</li></ol><p><strong>Available Solver Types</strong></p><ul><li><code>OriginalADMMSubproblemSolver</code>: Exact solution using specialized solvers (JuMP, proximal, linear)</li><li><code>DoublyLinearizedSolver</code>: Linearized approach with separate stepsizes for left/right nodes</li><li><code>AdaptiveLinearizedSolver</code>: Adaptive linearized method with dynamic stepsize selection</li></ul><p><strong>Solver Selection Guidelines</strong></p><ul><li><strong>OriginalADMMSubproblemSolver</strong>: Use when exact solutions are needed and subproblems have known structure</li><li><strong>DoublyLinearizedSolver</strong>: Use for large-scale problems where exact solutions are expensive</li><li><strong>AdaptiveLinearizedSolver</strong>: Use when optimal convergence rates are critical and problem geometry varies</li></ul><p><strong>Performance Considerations</strong></p><ul><li><strong>Exact solvers</strong>: Higher per-iteration cost but fewer iterations</li><li><strong>Linearized solvers</strong>: Lower per-iteration cost but more iterations</li><li><strong>Adaptive solvers</strong>: Moderate per-iteration cost with optimal convergence rates</li></ul><p><strong>Example Usage</strong></p><pre><code class="language-julia hljs"># Initialize solver
solver = DoublyLinearizedSolver(dualStepsize=1.0)
initialize!(solver, admmGraph, info)

# Solve subproblems for all nodes
for nodeID in keys(admmGraph.nodes)
    isLeft = nodeID in admmGraph.left
    solve!(solver, nodeID, accelerator, admmGraph, info, isLeft)
end

# Update dual residuals
updateDualResidualsInBuffer!(solver, info, admmGraph, accelerator)</code></pre><p><strong>Implementation Notes</strong></p><ul><li>Solvers should handle both left and right nodes in bipartite graphs</li><li>Buffer management is crucial for performance</li><li>Parallel computation should be supported when beneficial</li><li>State updates are needed when penalty parameters change</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/AbstractADMMSubproblemSolver.jl#L2-L76">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.OriginalADMMSubproblemSolver" href="#PDMO.OriginalADMMSubproblemSolver"><code>PDMO.OriginalADMMSubproblemSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OriginalADMMSubproblemSolver &lt;: AbstractADMMSubproblemSolver</code></pre><p>Original ADMM subproblem solver that computes exact solutions using specialized solvers.</p><p>This solver implements the standard ADMM approach where each primal subproblem is solved exactly (or to high precision) using problem-specific specialized solvers. It automatically detects subproblem structure and selects the most appropriate solver from a hierarchy of specialized methods.</p><p><strong>Mathematical Background</strong></p><p>The ADMM subproblem for node <code>i</code> takes the form:</p><p class="math-container">\[\min_{x_i} f_i(x_i) + g_i(x_i) + ⟨λ, A_i x_i⟩ + \frac{ρ}{2}\|A_i x_i + \sum_{j≠i} A_j x_j - c\|_2^2\]</p><p>This can be rewritten as:</p><p class="math-container">\[\min_{x_i} f_i(x_i) + g_i(x_i) + ⟨λ, A_i x_i⟩ + \frac{ρ}{2}(x_i^T A_i^T A_i x_i + 2⟨A_i^T(\sum_{j≠i} A_j x_j - c), x_i⟩ + \text{const})\]</p><p><strong>Specialized Solver Hierarchy</strong></p><p>The solver attempts to use specialized solvers in order of preference:</p><ol><li><strong>LinearSolver</strong>: For linear/quadratic subproblems (fastest)</li><li><strong>ProximalMappingSolver</strong>: For problems with known proximal operators</li><li><strong>JuMPSolver</strong>: For general nonlinear problems (most flexible)</li></ol><p><strong>Key Features</strong></p><ul><li><strong>Automatic Detection</strong>: Automatically selects the best solver for each subproblem</li><li><strong>Exact Solutions</strong>: Computes exact or high-precision solutions</li><li><strong>Precomputed Adjoints</strong>: Uses <code>EdgeData</code> for efficient repeated computations</li><li><strong>Parallel Support</strong>: Supports parallel solution of independent subproblems</li><li><strong>Anderson Acceleration</strong>: Special handling for Anderson acceleration schemes</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Convergence</strong>: Typically requires fewer iterations due to exact solutions</li><li><strong>Per-iteration Cost</strong>: Higher than linearized methods but more reliable</li><li><strong>Memory Usage</strong>: Moderate (precomputed adjoint mappings)</li><li><strong>Scalability</strong>: Good for problems with structured subproblems</li></ul><p><strong>Implementation Notes</strong></p><ul><li>Each node gets its own specialized solver instance</li><li>Edge data is precomputed once during initialization</li><li>Augmented Lagrangian coefficients are recomputed each iteration</li><li>Special handling for Anderson acceleration requires modified linear terms</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/OriginalADMMSubproblemSolver.jl#L16-L67">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.SpecializedOriginalADMMSubproblemSolver" href="#PDMO.SpecializedOriginalADMMSubproblemSolver"><code>PDMO.SpecializedOriginalADMMSubproblemSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SpecializedOriginalADMMSubproblemSolver</code></pre><p>Abstract type for specialized original ADMM subproblem solvers.</p><p>This type represents the interface for specialized solvers that implement the original ADMM subproblem formulation. Each specialized solver must provide the following methods:</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/OriginalADMMSubproblemSolver.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.LinearSolver" href="#PDMO.LinearSolver"><code>PDMO.LinearSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LinearSolver &lt;: SpecializedOriginalADMMSubproblemSolver</code></pre><p>Specialized solver for ADMM subproblems that reduce to linear systems.</p><p>This solver handles ADMM subproblems with quadratic or linear objective functions and unconstrained domains, which can be solved exactly by solving linear systems rather than using iterative optimization methods. The solver uses sparse matrix factorizations for efficient solution of the resulting linear systems.</p><p><strong>Mathematical Formulation</strong></p><p>The solver handles subproblems of the form:     min f(x) + ⟨λ, Ax + By^k - b⟩ + (ρ/2)||Ax + By^k - b||²</p><p>where f(x) is quadratic, linear, or zero:</p><ul><li><strong>Quadratic</strong>: f(x) = (1/2)x^T Q x + q^T x + r</li><li><strong>Linear</strong>: f(x) = q^T x + r  </li><li><strong>Zero</strong>: f(x) = 0</li></ul><p><strong>Optimality Conditions</strong></p><p>The first-order optimality conditions yield linear systems:</p><ul><li><strong>With Q</strong>: (ρA^T A + Q)x = -A^T λ - ρA^T(By^k - b) - q</li><li><strong>Without Q</strong>: A^T A x = -A^T λ/ρ - A^T(By^k - b) - q/ρ</li></ul><p><strong>Supported Problem Types</strong></p><ul><li><strong>Objective Functions</strong>: Zero, AffineFunction, QuadraticFunction</li><li><strong>Domain Constraints</strong>: Unconstrained or unbounded box constraints</li><li><strong>Variable Types</strong>: Vector variables only (no matrix variables)</li><li><strong>Constraint Mappings</strong>: Matrix, Identity, and Extraction mappings</li></ul><p><strong>Factorization Strategy</strong></p><p>The solver uses adaptive factorization based on matrix properties:</p><ul><li><strong>Cholesky</strong>: For positive definite system matrices (preferred)</li><li><strong>LDLT</strong>: For positive semidefinite system matrices (fallback)</li><li><strong>Precomputed A^T A</strong>: Efficient handling of constraint structure</li></ul><p><strong>Fields</strong></p><ul><li><code>Q::Union{SparseMatrixCSC{Float64, Int64}, Nothing}</code>: Quadratic term matrix</li><li><code>q::Union{Vector{Float64}, Nothing}</code>: Linear term vector</li><li><code>AAdjointSelf::SparseMatrixCSC{Float64, Int64}</code>: Precomputed A^T A matrix</li><li><code>currentRho::Float64</code>: Current penalty parameter ρ</li><li><code>systemMatrix::SparseMatrixCSC{Float64, Int64}</code>: System matrix for factorization</li><li><code>factorization::Union{SparseArrays.CHOLMOD.Factor{Float64}, Nothing}</code>: Matrix factorization</li><li><code>isPositiveDefinite::Bool</code>: Whether system matrix is positive definite</li><li><code>rhsBuffer::Vector{Float64}</code>: Working space for right-hand side vectors</li></ul><p><strong>Constructor Parameters</strong></p><ul><li><code>nodeID::String</code>: Node identifier in the ADMM graph</li><li><code>admmGraph::ADMMBipartiteGraph</code>: The bipartite graph structure</li><li><code>edgeData::Dict{String, EdgeData}</code>: Precomputed edge information</li><li><code>initialRho::Float64</code>: Initial penalty parameter</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Complexity</strong>: O(n³) for factorization, O(n²) for solves</li><li><strong>Memory Usage</strong>: O(n²) for factorization storage</li><li><strong>Convergence</strong>: Exact solution in single step</li><li><strong>Scalability</strong>: Excellent for problems with sparse structure</li></ul><p><strong>Implementation Notes</strong></p><ul><li>Uses CHOLMOD for high-performance sparse factorizations</li><li>Reuses factorizations when Q is present and ρ is fixed</li><li>Handles both positive definite and semidefinite cases</li><li>Efficient A^T A precomputation for constraint structure</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/SpecializedOriginalADMMSubproblemSolvers/LinearSolver.jl#L1-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.JuMPSolver" href="#PDMO.JuMPSolver"><code>PDMO.JuMPSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">JuMPSolver &lt;: SpecializedOriginalADMMSubproblemSolver</code></pre><p>JuMP-based solver for exact ADMM subproblems requiring optimization modeling.</p><p>This solver handles complex ADMM subproblems that cannot be solved in closed form, including those with nonlinear objectives, complex constraints, or specialized structure. It constructs and solves optimization models using the JuMP mathematical programming interface with the Ipopt nonlinear optimizer.</p><p><strong>Mathematical Formulation</strong></p><p>The solver handles subproblems of the form:     min f(x) + g(x) + ⟨λ, Ax + By^k - b⟩ + (ρ/2)||Ax + By^k - b||²</p><p>where:</p><ul><li>f(x): Smooth objective function (linear, quadratic, or nonlinear)</li><li>g(x): Constraint function (handled via JuMP constraints)</li><li>Ax + By^k - b: ADMM coupling constraints</li><li>λ: Dual variables, ρ: Penalty parameter</li></ul><p><strong>Supported Problem Types</strong></p><ul><li><strong>Linear Objectives</strong>: f(x) = c^T x</li><li><strong>Quadratic Objectives</strong>: f(x) = (1/2)x^T Q x + c^T x</li><li><strong>Nonlinear Objectives</strong>: Currently supports ComponentwiseExponentialFunction</li><li><strong>Constraint Mappings</strong>: Matrix, Identity, and Extraction mappings</li><li><strong>Domain Constraints</strong>: Handled through JuMP constraint system</li></ul><p><strong>Solver Configuration</strong></p><ul><li><strong>Primary Optimizer</strong>: Ipopt (Interior Point Optimizer)</li><li><strong>Linear Solver</strong>: MA27 (if HSL library available) for enhanced performance</li><li><strong>Tolerance Settings</strong>: High precision for ADMM convergence requirements</li><li><strong>Output</strong>: Silent mode for integration with ADMM framework</li></ul><p><strong>Fields</strong></p><ul><li><code>model::JuMP.AbstractModel</code>: JuMP optimization model</li><li><code>var::Dict{String, Vector{JuMP.VariableRef}}</code>: JuMP variables by node</li><li><code>aux::Dict{String, Vector{JuMP.VariableRef}}</code>: Auxiliary variables if needed</li><li><code>objExpressions::Vector{Union{JuMP.AffExpr, JuMP.QuadExpr}}</code>: Linear/quadratic objective parts</li><li><code>blockHasNonlinearSmoothFunction::Bool</code>: Flag for nonlinear objective detection</li><li><code>currentRho::Float64</code>: Current penalty parameter value</li></ul><p><strong>Constructor Parameters</strong></p><ul><li><code>nodeID::String</code>: Node identifier in the ADMM graph</li><li><code>admmGraph::ADMMBipartiteGraph</code>: The bipartite graph structure</li><li><code>edgeData::Dict{String, EdgeData}</code>: Precomputed edge information</li><li><code>rho::Float64</code>: Initial penalty parameter</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Complexity</strong>: Depends on problem structure and Ipopt performance</li><li><strong>Memory Usage</strong>: JuMP model storage plus solver workspace</li><li><strong>Convergence</strong>: Depends on Ipopt convergence for each subproblem</li><li><strong>Scalability</strong>: Limited by nonlinear solver performance</li></ul><p><strong>Implementation Notes</strong></p><ul><li>Uses HSL MA27 linear solver when available for better performance</li><li>Handles both linear/quadratic and nonlinear objectives seamlessly</li><li>Integrates with ADMM framework through standardized interface</li><li>Provides automatic model construction from problem structure</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/SpecializedOriginalADMMSubproblemSolvers/JuMPSolver.jl#L1-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.ProximalMappingSolver" href="#PDMO.ProximalMappingSolver"><code>PDMO.ProximalMappingSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ProximalMappingSolver &lt;: SpecializedOriginalADMMSubproblemSolver</code></pre><p>Specialized solver for ADMM subproblems that reduce to proximal mappings.</p><p>This solver handles a specific class of ADMM subproblems where the objective function is purely regularization (f(x) = 0) and the constraint mapping is a scaled identity. Such subproblems can be efficiently solved using proximal operators rather than general optimization methods.</p><p><strong>Mathematical Formulation</strong></p><p>The solver handles subproblems of the form:     min g(y) + ⟨λ, Ax^k + By - b⟩ + (ρ/2)||Ax^k + By - b||²</p><p>where:</p><ul><li>g(y): Proximal-friendly regularization function</li><li>B = mI: Scaled identity mapping (m ≠ 0)</li><li>f(y) = 0: Zero smooth objective function</li></ul><p><strong>Transformation to Proximal Mapping</strong></p><p>By substituting B = mI, the subproblem becomes:     min g(y) + ⟨m·λ, y⟩ + (ρm²/2)||y - (b - Ax^k)/m||²</p><p>This is equivalent to the proximal mapping:     min g(y) + (1/2γ)||y - z||²</p><p>where:</p><ul><li>z = (b - Ax^k - λ/ρ)/m: Proximal center point</li><li>γ = 1/(ρm²): Proximal parameter</li></ul><p><strong>Supported Mapping Types</strong></p><ul><li><strong>LinearMappingIdentity</strong>: B = cI with scaling coefficient c</li><li><strong>LinearMappingExtraction</strong>: B = cE with extraction operator E</li><li><strong>LinearMappingMatrix</strong>: B = cI where matrix A is c·I (validated)</li></ul><p><strong>Consistency Validation</strong></p><p>The constructor verifies that all constraint mappings for the node are consistent scaled identity operations:</p><ul><li>All mappings must scale the same vector by the same coefficient</li><li>The scaling coefficient must be non-zero</li><li>Inconsistent scaling throws an error</li></ul><p><strong>Fields</strong></p><ul><li><code>scalingCoefficient::Float64</code>: The scaling coefficient m in B = mI</li><li><code>proximalPoint::NumericVariable</code>: Working space for proximal center z</li><li><code>currentRho::Float64</code>: Current penalty parameter ρ</li><li><code>gamma::Float64</code>: Proximal parameter γ = 1/(ρm²)</li></ul><p><strong>Constructor Parameters</strong></p><ul><li><code>nodeID::String</code>: Node identifier in the ADMM graph</li><li><code>admmGraph::ADMMBipartiteGraph</code>: The bipartite graph structure</li><li><code>edgeData::Dict{String, EdgeData}</code>: Precomputed edge information</li><li><code>rho::Float64</code>: Initial penalty parameter</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Complexity</strong>: O(n) where n is the variable dimension</li><li><strong>Memory Usage</strong>: O(n) for proximal point storage</li><li><strong>Convergence</strong>: Depends on proximal operator efficiency</li><li><strong>Scalability</strong>: Excellent for large-scale problems with appropriate structure</li></ul><p><strong>Implementation Notes</strong></p><ul><li>Requires f(x) = 0 (Zero objective function)</li><li>Validates mapping consistency during construction</li><li>Uses efficient proximal operator implementations</li><li>Automatic scaling coefficient detection and validation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/SpecializedOriginalADMMSubproblemSolvers/ProximalMappingSolver.jl#L1-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.DoublyLinearizedSolver" href="#PDMO.DoublyLinearizedSolver"><code>PDMO.DoublyLinearizedSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DoublyLinearizedSolver &lt;: AbstractADMMSubproblemSolver</code></pre><p>Doubly Linearized Subproblem Solver for ADMM optimization.</p><p>This solver uses a doubly linearized approach where both the objective function gradients and the augmented Lagrangian terms are linearized. It maintains separate proximal stepsizes for left and right nodes (α and β) and uses operator norm estimates for stability.</p><p><strong>Fields</strong></p><ul><li><code>dualStepsize::Float64</code>: Step size for dual variable updates</li><li><code>proximalStepsizeAlpha::Float64</code>: Proximal step size for left nodes  </li><li><code>proximalStepsizeBeta::Float64</code>: Proximal step size for right nodes</li><li><code>primalBuffer::Dict{String, NumericVariable}</code>: Buffer for primal computations</li><li><code>dualBuffer::Dict{String, NumericVariable}</code>: Buffer for dual computations</li><li><code>maxLeftLipschitzConstant::Float64</code>: Maximum Lipschitz constant for left node objectives</li><li><code>maxRightLipschitzConstant::Float64</code>: Maximum Lipschitz constant for right node objectives</li><li><code>maxLeftMatrixAdjointSelfOperatorNorm::Float64</code>: Maximum operator norm for left mappings</li><li><code>maxRightMatrixAdjointSelfOperatorNorm::Float64</code>: Maximum operator norm for right mappings</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/DoublyLinearizedSolver.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AdaptiveLinearizedSolver" href="#PDMO.AdaptiveLinearizedSolver"><code>PDMO.AdaptiveLinearizedSolver</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdaptiveLinearizedSolver &lt;: AbstractADMMSubproblemSolver</code></pre><p>Adaptive Linearized ADMM Solver with dynamic step size optimization.</p><p>This solver implements a sophisticated adaptive variant of linearized ADMM where the proximal step size γ is dynamically optimized based on real-time analysis of the optimization landscape. The algorithm maintains separate gradient histories for left and right nodes and uses geometric convergence analysis to balance convergence speed with numerical stability.</p><p><strong>Mathematical Foundation</strong></p><p>The adaptive linearized ADMM solves subproblems of the form:</p><ul><li>Left nodes: x^{k+1} = prox_{γg}(x^k - γ(∇f(x^k) + A^T u^{k+1}))</li><li>Right nodes: y^{k+1} = prox_{γh}(y^k - γ(∇g(y^k) + B^T u^{k+1}))</li></ul><p>where γ is adaptively updated by analyzing:</p><ul><li>Local Lipschitz constants L<em>f, L</em>g of objective gradients</li><li>Strong convexity parameters ℓ<em>f, ℓ</em>g when available</li><li>Operator norms ||A^T A||, ||B^T B|| of constraint mappings</li><li>Convergence rate bounds and stability margins</li></ul><p><strong>Adaptive Strategy</strong></p><p>The algorithm optimizes γ by solving constrained optimization problems:</p><ol><li><strong>Primal-Dual Balance</strong>: Ensure balanced convergence rates between primal and dual variables</li><li><strong>Geometric Adaptation</strong>: Incorporate problem-specific curvature information</li><li><strong>Stability Constraints</strong>: Maintain numerical stability through step size bounds</li><li><strong>Convergence Acceleration</strong>: Maximize convergence speed subject to stability</li></ol><p><strong>Algorithm Phases</strong></p><ol><li><strong>Gradient History Tracking</strong>: Maintain ∇f(x^k), ∇f(x^{k-1}), ∇g(y^k), ∇g(y^{k-1})</li><li><strong>Constraint Violation Analysis</strong>: Compute Δu = φ(A(x^k - x^{k-1}) + B(y^k - y^{k-1})) + Ax^k + By^k - c</li><li><strong>Geometry Estimation</strong>: Estimate local Lipschitz constants and operator norms</li><li><strong>Optimization</strong>: Solve cubic equations for optimal γ candidates</li><li><strong>Selection</strong>: Choose minimum valid γ among all candidates</li></ol><p><strong>Fields</strong></p><ul><li><code>proximalStepsizeGamma::Float64</code>: Current adaptive proximal step size γ</li><li><code>primalBuffer::Dict{String, NumericVariable}</code>: Working space for primal computations</li><li><code>dualBuffer::Dict{String, NumericVariable}</code>: Working space for dual computations</li><li><code>φ::Float64</code>: Golden ratio parameter (≈1.618) for constraint violation weighting</li><li><code>r::Float64</code>: Primal-dual ratio parameter controlling balance between updates</li><li><code>primalprev::Dict{String, NumericVariable}</code>: Previous iteration primal variables</li><li><code>xgradPrev::Dict{String, NumericVariable}</code>: Previous gradients for left nodes</li><li><code>xgradCur::Dict{String, NumericVariable}</code>: Current gradients for left nodes</li><li><code>ygradPrev::Dict{String, NumericVariable}</code>: Previous gradients for right nodes</li><li><code>ygradCur::Dict{String, NumericVariable}</code>: Current gradients for right nodes</li><li><code>ifSimple::Bool</code>: Whether to use simplified adaptive scheme</li></ul><p><strong>Constructor Parameters</strong></p><ul><li><code>gamma::Float64=1.0</code>: Initial proximal step size</li><li><code>r::Float64=1.0</code>: Primal-dual balancing parameter</li><li><code>ifSimple::Bool=false</code>: Use simplified adaptive scheme if true</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Complexity</strong>: O(n) per iteration plus gradient evaluations</li><li><strong>Memory Usage</strong>: O(n) for gradient storage per node</li><li><strong>Convergence Rate</strong>: Adaptive between O(1/k) and linear depending on problem structure</li><li><strong>Numerical Stability</strong>: Automatic step size bounds prevent divergence</li></ul><p><strong>Implementation Notes</strong></p><ul><li>Uses golden ratio φ = (1 + √5)/2 for optimal constraint violation weighting</li><li>Simplified scheme sets φ = 2 for reduced computational overhead</li><li>Gradient buffering enables efficient multi-step analysis</li><li>Parallel computation across nodes for scalability</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMSubproblemSolvers/AdaptiveLinearizedSolver.jl#L117-L182">source</a></section></article><h2 id="Adapters"><a class="docs-heading-anchor" href="#Adapters">Adapters</a><a id="Adapters-1"></a><a class="docs-heading-anchor-permalink" href="#Adapters" title="Permalink"></a></h2><p>Penalty parameter adaptation strategies for ADMM algorithms.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AbstractADMMAdapter" href="#PDMO.AbstractADMMAdapter"><code>PDMO.AbstractADMMAdapter</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractADMMAdapter</code></pre><p>Abstract base type for ADMM penalty parameter adaptation schemes.</p><p>ADMM adapters are responsible for dynamically adjusting the penalty parameter (ρ) during ADMM iterations to improve convergence rates and numerical stability. The penalty parameter plays a crucial role in balancing the enforcement of constraints versus the optimization of the objective function.</p><p><strong>Mathematical Background</strong></p><p>The ADMM algorithm solves optimization problems of the form:</p><p class="math-container">\[\min_{x,z} f(x) + g(z) \text{ subject to } Ax + Bz = c\]</p><p>The augmented Lagrangian is:</p><p class="math-container">\[L_ρ(x,z,λ) = f(x) + g(z) + λ^T(Ax + Bz - c) + \frac{ρ}{2}\|Ax + Bz - c\|_2^2\]</p><p>where ρ is the penalty parameter that affects:</p><ul><li><strong>Convergence speed</strong>: Higher ρ can accelerate convergence but may cause numerical issues</li><li><strong>Numerical stability</strong>: Lower ρ provides better conditioning but slower convergence</li><li><strong>Primal-dual balance</strong>: Optimal ρ balances primal and dual residual reductions</li></ul><p><strong>Adapter Interface</strong></p><p>All concrete adapter types must implement:</p><ul><li><code>initialize!(adapter, info, admmGraph)</code>: Initialize adapter with problem data</li><li><code>updatePenalty(adapter, info, admmGraph, iter)</code>: Update penalty parameter and return whether changed</li></ul><p><strong>Common Adaptation Strategies</strong></p><ol><li><strong>Residual Balancing</strong>: Adjust ρ based on primal vs dual residual magnitudes</li><li><strong>Spectral Methods</strong>: Use spectral radius estimates to guide parameter selection</li><li><strong>Adaptive Schedules</strong>: Predetermined schedules based on iteration counts</li><li><strong>Gradient-based</strong>: Use gradient information to optimize parameter selection</li></ol><p><strong>Performance Considerations</strong></p><ul><li><strong>Frequency</strong>: Adapters should not update ρ too frequently (every 5-10 iterations)</li><li><strong>Magnitude</strong>: Changes should be moderate (factors of 2-10) to avoid instability</li><li><strong>Bounds</strong>: ρ should be constrained within reasonable numerical bounds</li><li><strong>Convergence</strong>: Adaptation should eventually stabilize as algorithm converges</li></ul><p><strong>Available Implementations</strong></p><ul><li><code>NullAdapter</code>: No adaptation (constant ρ)</li><li><code>RBAdapter</code>: Residual balancing adaptation</li><li><code>SRAAdapter</code>: Spectral radius adaptive adaptation</li></ul><p><strong>See Also</strong></p><ul><li><code>NullAdapter</code>: No-operation adapter for baseline comparison</li><li><code>RBAdapter</code>: Residual balancing adapter</li><li><code>SRAAdapter</code>: Spectral radius adaptive adapter</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAdapters/AbstractADMMAdapter.jl#L1-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.NullAdapter" href="#PDMO.NullAdapter"><code>PDMO.NullAdapter</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NullAdapter &lt;: AbstractADMMAdapter</code></pre><p>No-operation adapter that maintains a constant penalty parameter throughout ADMM iterations.</p><p>This adapter provides a baseline behavior where the penalty parameter ρ remains unchanged from its initial value. It&#39;s useful for:</p><ul><li>Baseline performance comparisons</li><li>Problems where optimal ρ is known a priori</li><li>Debugging and testing ADMM implementations</li><li>Scenarios where adaptation might be harmful</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAdapters/AbstractADMMAdapter.jl#L97-L109">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.RBAdapter" href="#PDMO.RBAdapter"><code>PDMO.RBAdapter</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RBAdapter &lt;: AbstractADMMAdapter</code></pre><p>Residual Balance (RB) adapter for ADMM penalty parameter adaptation.</p><p>This adapter implements the residual balancing strategy which adjusts the penalty parameter ρ based on the relative magnitudes of primal and dual residuals to maintain balanced convergence rates.</p><p><strong>Mathematical Background</strong></p><p>The ADMM algorithm generates two types of residuals at each iteration:</p><ol><li><strong>Primal residual</strong>: <code>r^k = Ax^k + Bz^k - c</code></li><li><strong>Dual residual</strong>: <code>s^k = ρA^T B(z^k - z^{k-1})</code></li></ol><p>The optimal penalty parameter should balance the reduction of both residuals. The RB adapter uses the following strategy:</p><ul><li>If <code>‖r^k‖₂ &gt; μ ‖s^k‖₂</code>: Increase ρ to emphasize constraint satisfaction</li><li>If <code>‖s^k‖₂ &gt; μ ‖r^k‖₂</code>: Decrease ρ to emphasize optimality conditions</li><li>Otherwise: Keep ρ unchanged</li></ul><p><strong>Algorithm Details</strong></p><p>At each iteration, the adapter:</p><ol><li>Computes the current primal residual norm: <code>‖r^k‖₂</code></li><li>Computes the current dual residual norm: <code>‖s^k‖₂</code></li><li>Compares their ratio against the threshold <code>testRatio</code></li><li>Updates ρ by multiplying or dividing by <code>adapterRatio</code></li><li>Clamps ρ to be within <code>[ADMM_MIN_RHO, ADMM_MAX_RHO]</code></li></ol><p><strong>Parameters</strong></p><ul><li><code>testRatio::Float64</code>: Threshold for residual ratio comparison (default: 10.0)</li><li><code>adapterRatio::Float64</code>: Factor for ρ adjustment (default: 2.0)</li></ul><p><strong>Parameter Guidelines</strong></p><ul><li><strong>testRatio</strong>: Typical values 5-20. Higher values make adaptation less aggressive</li><li><strong>adapterRatio</strong>: Typical values 1.5-5. Higher values make larger adjustments</li></ul><p><strong>Convergence Properties</strong></p><ul><li><strong>Balanced Convergence</strong>: Maintains similar convergence rates for primal and dual residuals</li><li><strong>Robust Performance</strong>: Works well across different problem types and scales</li><li><strong>Proven Theory</strong>: Backed by convergence analysis</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Cost</strong>: O(1) per iteration (just ratio comparison)</li><li><strong>Memory Usage</strong>: O(1) - no additional storage required</li><li><strong>Stability</strong>: Generally stable due to moderate adjustment factors</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAdapters/RBAdapter.jl#L1-L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.SRAAdapter" href="#PDMO.SRAAdapter"><code>PDMO.SRAAdapter</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SRAAdapter &lt;: AbstractADMMAdapter</code></pre><p>Spectral Radius Adaptive (SRA) adapter for ADMM penalty parameter adaptation.</p><p>This adapter implements a sophisticated penalty parameter adaptation strategy based on spectral radius estimation. It analyzes the convergence behavior of dual variables and constraint violations to automatically adjust ρ for optimal convergence rates.</p><p><strong>Mathematical Background</strong></p><p>The SRA adapter is based on the observation that the optimal penalty parameter ρ is related to the spectral radius of the iteration matrix. The adapter approximates this by analyzing the relationship between consecutive iterates:</p><p>For dual variables: <code>Δy^k = y^{k+1} - y^k</code> For constraint violations: <code>ΔBz^k = B(z^{k+1} - z^k)</code></p><p>The ratio <code>‖Δy^k‖₂ / ‖ΔBz^k‖₂</code> provides an estimate of the appropriate penalty parameter.</p><p><strong>Algorithm Strategy</strong></p><p>The adapter computes:</p><ol><li><strong>Dual difference norm</strong>: <code>‖y^{k+1} - y^k‖₂</code></li><li><strong>Constraint mapping difference norm</strong>: <code>‖B(z^{k+1} - z^k)‖₂</code>  </li><li><strong>Spectral ratio</strong>: <code>ρ_new = ‖Δy^k‖₂ / ‖ΔBz^k‖₂</code></li></ol><p>Special cases are handled when norms are near zero:</p><ul><li>If <code>‖Δy^k‖₂ ≈ 0</code> and <code>‖ΔBz^k‖₂ &gt; 0</code>: Decrease ρ (dual optimality achieved)</li><li>If <code>‖Δy^k‖₂ &gt; 0</code> and <code>‖ΔBz^k‖₂ ≈ 0</code>: Increase ρ (constraint satisfaction achieved)</li><li>If both ≈ 0: Keep ρ unchanged (convergence achieved)</li></ul><p><strong>Parameters</strong></p><ul><li><code>T::Int64</code>: Update frequency (default: 5) - Parameter updated every T iterations</li><li><code>increasingFactor::Float64</code>: Factor for increasing ρ in special cases (default: 2.0)</li><li><code>decreasingFactor::Float64</code>: Factor for decreasing ρ in special cases (default: 2.0)</li></ul><p><strong>Advantages over RB Adapter</strong></p><ol><li><strong>Theoretical Foundation</strong>: Based on spectral analysis of ADMM convergence</li><li><strong>Adaptive Estimation</strong>: Directly estimates optimal ρ rather than simple ratio balancing</li><li><strong>Sophisticated Handling</strong>: Special case analysis for near-convergence scenarios</li><li><strong>Reduced Oscillations</strong>: Updates less frequently to avoid parameter oscillations</li></ol><p><strong>Performance Characteristics</strong></p><ul><li><strong>Computational Cost</strong>: O(problem_size) per update (computing norms)</li><li><strong>Memory Usage</strong>: O(problem_size) for storing difference vectors</li><li><strong>Update Frequency</strong>: Every T iterations (default: 5)</li><li><strong>Stability</strong>: Generally more stable than frequent RB updates</li></ul><p><strong>Parameter Selection Guidelines</strong></p><ul><li><p><strong>T (Update Frequency)</strong>: </p><ul><li>Smaller values (3-5): More responsive adaptation</li><li>Larger values (8-15): More stable, less frequent updates</li><li>Very large values (&gt;20): May miss important adaptations</li></ul></li><li><p><strong>increasingFactor/decreasingFactor</strong>:</p><ul><li>Smaller values (1.5-2.0): Conservative adjustments</li><li>Larger values (2.5-5.0): More aggressive adjustments</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAdapters/SRAAdapter.jl#L1-L65">source</a></section></article><h2 id="Accelerators"><a class="docs-heading-anchor" href="#Accelerators">Accelerators</a><a id="Accelerators-1"></a><a class="docs-heading-anchor-permalink" href="#Accelerators" title="Permalink"></a></h2><p>Acceleration techniques for improving ADMM convergence.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AbstractADMMAccelerator" href="#PDMO.AbstractADMMAccelerator"><code>PDMO.AbstractADMMAccelerator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractADMMAccelerator</code></pre><p>Abstract base type for ADMM acceleration schemes that can improve convergence rates.</p><p>ADMM accelerators implement various techniques to speed up the convergence of the Alternating Direction Method of Multipliers (ADMM) algorithm. Common approaches include:</p><ul><li>Anderson acceleration (extrapolation methods)</li><li>Halpern-type acceleration schemes</li><li>Null acceleration (no acceleration applied)</li></ul><p><strong>Required Interface Methods</strong></p><p>All concrete subtypes must implement:</p><ul><li><code>initialize!(accelerator, info, admmGraph)</code>: Initialize the accelerator with ADMM problem data</li><li><code>accelerateBetweenPrimalUpdates!(accelerator, info, admmGraph)</code>: Apply acceleration between primal updates</li><li><code>accelerateAfterDualUpdates!(accelerator, info)</code>: Apply acceleration after dual updates</li></ul><p><strong>Mathematical Background</strong></p><p>ADMM acceleration works by combining information from multiple previous iterations to compute improved estimates of the solution. The general form can be written as:</p><p class="math-container">\[x_{k+1} = f(x_k, x_{k-1}, ..., x_{k-m})\]</p><p>where <code>f</code> is the acceleration function and <code>m</code> is the memory depth.</p><p><strong>Performance Considerations</strong></p><ul><li>Different accelerators have varying memory requirements</li><li>Some accelerators may increase per-iteration cost while reducing iteration count</li><li>The effectiveness depends on problem structure and conditioning</li></ul><p><strong>See Also</strong></p><ul><li><code>AndersonAccelerator</code>: Implementation of Anderson acceleration</li><li><code>AutoHalpernAccelerator</code>: Implementation of Halpern-type acceleration</li><li><code>NullAccelerator</code>: No-operation accelerator for baseline comparison</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAccelerators/AbstractADMMAccelerator.jl#L1-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.NullAccelerator" href="#PDMO.NullAccelerator"><code>PDMO.NullAccelerator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NullAccelerator &lt;: AbstractADMMAccelerator</code></pre><p>A no-operation accelerator that maintains default ADMM behavior without any acceleration.</p><p>This accelerator serves as a baseline for comparison with other acceleration schemes. All methods are implemented as no-ops, so the ADMM algorithm proceeds with its standard update rules.</p><p><strong>Use Cases</strong></p><ul><li>Baseline performance comparison</li><li>Debugging and testing ADMM implementations</li><li>Scenarios where acceleration is not desired or beneficial</li></ul><p><strong>Performance</strong></p><ul><li>Zero computational overhead</li><li>No memory usage beyond the accelerator object itself</li><li>Identical convergence behavior to unaccelerated ADMM</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAccelerators/AbstractADMMAccelerator.jl#L45-L65">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AndersonAccelerator" href="#PDMO.AndersonAccelerator"><code>PDMO.AndersonAccelerator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AndersonAccelerator &lt;: AbstractADMMAccelerator</code></pre><p>Implementation of Anderson acceleration for ADMM algorithms with safeguard strategy.</p><p>This extends the original Anderson accelerator with a safeguard mechanism that detects when the acceleration might be causing instability and reverts to previous stable iterates.</p><p><strong>Safeguard Strategy</strong></p><p>The safeguard monitors the residual norm progression and triggers when:</p><p class="math-container">\[\|w_k\| &gt; δ \|w_{k-1}\|\]</p><p>where:</p><ul><li><code>w_k</code> is the current iterate difference (residual)</li><li><code>δ</code> is the safeguard threshold parameter</li><li>When triggered, the algorithm clears history and reverts to previous iterates</li></ul><p><strong>Algorithm Details</strong></p><ol><li><strong>History Management</strong>: Maintains circular buffers for previous iterates</li><li><strong>Residual Monitoring</strong>: Tracks residual norm progression for stability</li><li><strong>Safeguard Triggering</strong>: Detects potential instability and takes corrective action</li><li><strong>State Reversion</strong>: Reverts to previous stable state when safeguard triggers</li><li><strong>Efficient Updates</strong>: Uses QR decomposition with Givens rotations for least squares</li></ol><p><strong>Parameters</strong></p><ul><li><code>historyDepth::Int64</code>: Number of previous iterates to store and use</li><li><code>beta::Float64</code>: Mixing parameter controlling acceleration strength</li><li><code>delta::Float64</code>: Safeguard threshold parameter (typical values: 0.1-1.0)</li></ul><p><strong>Performance Characteristics</strong></p><ul><li><strong>Memory</strong>: O(historyDepth × problem_size)</li><li><strong>Per-iteration Cost</strong>: O(historyDepth × problem_size)</li><li><strong>Stability</strong>: Enhanced stability through safeguard mechanism</li><li><strong>Convergence</strong>: Robust convergence even for ill-conditioned problems</li></ul><p><strong>Example Usage</strong></p><pre><code class="language-julia hljs">accelerator = AndersonAccelerator(historyDepth=5, beta=1.0, delta=0.1)
initialize!(accelerator, info, admmGraph)

# During ADMM iterations:
accelerateBetweenPrimalUpdates!(accelerator, info, admmGraph)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAccelerators/AndersonAccelerator.jl#L39-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PDMO.AutoHalpernAccelerator" href="#PDMO.AutoHalpernAccelerator"><code>PDMO.AutoHalpernAccelerator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AutoHalpernAccelerator &lt;: AbstractADMMAccelerator</code></pre><p>Automatic Halpern acceleration for ADMM with adaptive activation and period detection.</p><p>This accelerator implements a sophisticated variant of Halpern iteration that automatically detects when to activate acceleration and adaptively manages the acceleration period.</p><p><strong>Mathematical Background</strong></p><p>The Halpern iteration scheme takes the form:</p><p class="math-container">\[x_{k+1} = \frac{k}{k+1} T(x_k) + \frac{1}{k+1} x_0\]</p><p>where:</p><ul><li><code>T(x_k)</code> is the ADMM operator applied to the current iterate</li><li><code>x_0</code> is the anchor point (center point) for acceleration</li><li><code>k</code> is the iteration counter within the acceleration period</li></ul><p><strong>Adaptive Features</strong></p><ol><li><strong>Automatic Activation</strong>: Detects when acceleration should be enabled</li><li><strong>Period Management</strong>: Automatically restarts acceleration cycles</li><li><strong>Retrace Detection</strong>: Monitors iterate behavior to detect oscillations</li><li><strong>Dual-Phase Operation</strong>: Switches between exploration and acceleration phases</li></ol><p><strong>Algorithm States</strong></p><ul><li><strong>Inactive</strong>: Standard ADMM updates, monitoring for activation conditions</li><li><strong>Semi-Surpassed</strong>: Preparing for acceleration, validating conditions</li><li><strong>Active</strong>: Full Halpern acceleration with automatic period management</li></ul><p><strong>Key Parameters</strong></p><ul><li><code>maxPeriod::Int</code>: Maximum iterations in an acceleration cycle</li><li><code>iterDepth::Int</code>: Current position within acceleration cycle</li><li><code>isActive::Bool</code>: Whether acceleration is currently applied</li><li><code>isSemiSurpassed::Bool</code>: Whether preparing for activation</li></ul><p><strong>Retrace Mechanism</strong></p><p>The accelerator tracks whether iterates are consistently increasing or decreasing. When full retrace is detected (all components have changed direction), it triggers acceleration restart with a new center point.</p><p><strong>Performance Characteristics</strong></p><ul><li><strong>Memory</strong>: O(problem_size) for tracking iterate history</li><li><strong>Computation</strong>: O(problem_size) per iteration</li><li><strong>Convergence</strong>: Adaptive behavior can improve robustness over fixed schemes</li></ul><p><strong>Example Usage</strong></p><pre><code class="language-julia hljs">accelerator = AutoHalpernAccelerator(maxPeriod=100)
initialize!(accelerator, info, admmGraph)

# Acceleration is applied automatically during ADMM iterations
accelerateAfterDualUpdates!(accelerator, info)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alibaba-damo-academy/PDMO.jl/blob/cdf28e9b38bb816eff0b0013adabbed25cc9fd9f/src/Algorithms/ADMM/ADMMAccelerators/AutoHalpernAccelerator.jl#L184-L242">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mappings/">« Mappings</a><a class="docs-footer-nextpage" href="../pdm/">AdaPDM »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 22 July 2025 05:56">Tuesday 22 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
