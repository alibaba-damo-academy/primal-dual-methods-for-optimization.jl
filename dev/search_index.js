var documenterSearchIndex = {"docs":
[{"location":"S1_getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Before official release, we recommend the following practice to download and use PDMO.jl. ","category":"page"},{"location":"S1_getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Download the package and cd into the project folder.","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"cd PDMO","category":"page"},{"location":"S1_getting_started/#HSL-Setup-(Optional)","page":"Getting Started","title":"HSL Setup (Optional)","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"PDMO.jl relies on Ipopt.jl to solve certain ADMM subproblems. Linear solvers can significantly affect the performance of Ipopt. For enhanced performance, you can optionally use linear solvers from HSL. If HSL is available, linear solver MA27 will be used for Ipopt by default.","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Default setup (HSL not required): No additional setup needed. PDMO.jl will use Ipopt's default linear solver.","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Enhanced performance with HSL: ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Obtain HSL library from https://www.hsl.rl.ac.uk/\nSet up your HSL_jll directory with the following structure:\nHSL_jll/\n└── override/\n    └── lib/\n        └── {SYSTEM_ARCHITECTURE}/\n            └── libhsl.{so|dylib}\nExample architectures: x86_64-linux-gnu-libgfortran5, aarch64-apple-darwin-libgfortran5\nEdit warmup.jl and update the HSL path:\n# Uncomment and modify this line:\nHSL_PATH = \"/path/to/your/HSL_jll\"","category":"page"},{"location":"S1_getting_started/#Project-Setup","page":"Getting Started","title":"Project Setup","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Run the setup script:","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"julia warmup.jl","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"This one-time step will:","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Set up all required dependencies\nConfigure HSL if available\nReport HSL detection status","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"After successful setup, PDMO.jl is ready for use:","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"using PDMO","category":"page"},{"location":"S1_getting_started/#Quick-Start","page":"Getting Started","title":"Quick Start","text":"","category":"section"},{"location":"S1_getting_started/#Dual-Square-Root-LASSO","page":"Getting Started","title":"Dual Square Root LASSO","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"We use the Dual Square Root LASSO as a beginning example: ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"beginaligned\n    min_x zquad  langle b xrangle \n    mathrmst quad  Ax - z = 0 \n     x_2 leq 1 z_infty leq lambda\nendaligned","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"where (A b lambda) are given problem data of proper dimensions. ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"To begin with, load PDMO.jl and other necessary packages.","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"using PDMO\nusing LinearAlgebra\nusing SparseArrays\nusing Random ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Next generate or load your own problem data. We use synthetic data here. ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"numberRows = 10 \nnumberColumns = 20 \nA = sparse(randn(numberRows, numberColumns))\nb = randn(numberColumns)\nlambda = 1.0","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Then we can generate a MultiblockProblem for the Dual Square Root LASSO problem.","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"mbp = MultiblockProblem()\n\n# add x block\nblock_x = BlockVariable() \nblock_x.f = AffineFunction(b, 0.0)    # f_1(x) = <b, x>\nblock_x.g = IndicatorBallL2(1.0)      # g_1(x) = indicator of L2 ball \nblock_x.val = zeros(numberColumns)    # initial value\nxID = addBlockVariable!(mbp, block_x) # add x block to mbp; an ID is assigned\n\n# add z block \nblock_z = BlockVariable()                              \nblock_z.g = IndicatorBox(-lambda * ones(numberRows), # f_2(z) = Zero() by default\n    ones(numberRows) * lambda)                       # g_2(x) = indicator of box\nblock_z.val = zeros(numberRows)                      # initial value\nzID = addBlockVariable!(mbp, block_z)                # add z block to mbp; an ID is assigned\n\n# add constraint: Ax-z=0\nconstr = BlockConstraint() \naddBlockMappingToConstraint!(constr, xID, LinearMappingMatrix(A))      # specify the mapping of x\naddBlockMappingToConstraint!(constr, zID, LinearMappingIdentity(-1.0)) # specify the mapping of z \nconstr.rhs = zeros(numberRows)                                         # specify RHS\naddBlockConstraint!(mbp, constr)                                       # add constraint to mbp","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Next we can run different variants of ADMM: ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run ADMM \nparam = ADMMParam() \nparam.solver = OriginalADMMSubproblemSolver()\nparam.adapter = RBAdapter(testRatio=10.0, adapterRatio=2.0)\nparam.accelerator = AndersonAccelerator()\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run Doubly Linearized ADMM\nparam = ADMMParam() \nparam.solver = DoublyLinearizedSolver() \nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run Adaptive Linearized ADMM\nparam = ADMMParam() \nparam.solver = AdaptiveLinearizedSolver()\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"or different adaptive primal-dual methods: ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run AdaPDM \nparamAdaPDM = AdaPDMParam(mbp)\nresult = runAdaPDM(mbp, paramAdaPDM)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run AdaPDMPlus \nparamAdaPDMPlus = AdaPDMPlusParam(mbp)\nresult = runAdaPDM(mbp, paramAdaPDMPlus)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run Malitsky-Pock \nparamMalitskyPock = MalitskyPockParam(mbp)\nresult = runAdaPDM(mbp, paramMalitskyPock)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"# run Condat-Vu \nparamCondatVu = CondatVuParam(mbp)\nresult = runAdaPDM(mbp, paramCondatVu)","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"Upon termination of the selected algorithm, one can look for primal solution and iteration information through result.solution and result.iterationInfo, respectively. ","category":"page"},{"location":"S1_getting_started/#User-Defined-Smooth-and-Proximable-Functions","page":"Getting Started","title":"User Defined Smooth and Proximable Functions","text":"","category":"section"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"In addition to a set of built-in functions whose gradient or proximal oracles have been implemented, PDMO.jl supports user defined smooth and proximable functions. Consider the function ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"    F(x) = x_1 + x_2 + x_3^4 x = x_1 x_2 x_3^top in mathbbR^3","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"which can be expressed as the sum of a smooth f and a proximable g: ","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"    f(x) = x_1 + x_3^4 quad g(x) = x_2","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"In PDMO.jl, this block can be constructed as follows:","category":"page"},{"location":"S1_getting_started/","page":"Getting Started","title":"Getting Started","text":"block = BlockVariable()\nblock.f = UserDefinedSmoothFunction(\n    x -> x[1] + x[3]^4,                  # f\n    x -> [1.0, 0.0, 4*x[3]^3])           # ∇f\nblock.g = UserDefinedProximalFunction(\n    x -> abs(x[2]),                      # g\n    (x, gamma) -> [                      # prox_{gamma g} \n        x[1], \n        sign(x[2]) * max(abs(x[2]) - gamma, 0.0),\n        x[3]])\nblock.val = zeros(3)                     # initial value","category":"page"},{"location":"S3_examples/DistributedOPF/#Distributed-Optimal-Power-Flow-(DC-OPF)","page":"Distributed OPF","title":"Distributed Optimal Power Flow (DC OPF)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"This example demonstrates how to solve DC Optimal Power Flow problems in a distributed manner using ADMM with bipartization algorithms.","category":"page"},{"location":"S3_examples/DistributedOPF/#Problem-Background","page":"Distributed OPF","title":"Problem Background","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/#DC-Optimal-Power-Flow","page":"Distributed OPF","title":"DC Optimal Power Flow","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"DC Optimal Power Flow (DC OPF) is a fundamental optimization problem in electric power systems. The goal is to find the most cost-effective way to dispatch power generators while satisfying all network constraints under the DC power flow approximation.","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Consider a power network G(N E) where N is the set of buses and E is the set of transmission lines. Let G_i be the set of generators at bus i, and G = cup_iin NG_i be all generators. The DC OPF problem is formulated as:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"beginaligned\nmin quad  sum_g in G c_g(P_g) \ntextst quad  sum_gin G_iP_g - P^D_i = sum_j (ij)in E f_ij quad forall i in N \n f_ij = B_ij(theta_i - theta_j) quad forall (ij) in E \n underlineP_g leq P_g leq overlineP_g quad forall g in G \n underlinef_ij leq f_ij leq overlinef_ij quad forall (ij) in E\nendaligned","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Decision Variables:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Real power generated by generator g: P_g\nVoltage angle at bus i: theta_i\nPower flow on transmission line (ij): f_ij","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Constraints:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Power balance: Net power injection equals outflow at each bus\nPower flow equations: Flow proportional to angle difference (DC approximation)\nGeneration limits: Generator capacity bounds\nTransmission limits: Thermal flow limits on lines","category":"page"},{"location":"S3_examples/DistributedOPF/#Distributed-Reformulation","page":"Distributed OPF","title":"Distributed Reformulation","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"The power network is partitioned into P zones Z_1 ldots Z_P, each controlled by a separate agent. For each zone p in P:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Zone buses: Z_p contains buses controlled by agent p\nBoundary buses: textNB_p contains neighboring buses from other zones\nExtended zone: overlineZ_p = Z_p cup textNB_p includes all buses agent p needs to consider\nTie lines: Lines connecting buses in different zones, denoted by textTL","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"In real-world power systems, there's often a need for distributed solutions:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Power grids span multiple regions controlled by different operators\nCritical system information cannot be shared between regions\nLarge-scale problems require distributed computation for efficiency","category":"page"},{"location":"S3_examples/DistributedOPF/#Distributed-DC-OPF-Formulation","page":"Distributed OPF","title":"Distributed DC OPF Formulation","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Each agent p maintains local copies of variables for its extended zone:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"x_p = left(P_g^p_gin G_i iin Z_p theta^p_i_i in overlineZ_p f^p_ij_(ij)in Ecap(Z_p times overlineZ_p) right)","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Local Constraint Set: X_p = x_p  textlocal power balance generation limits flow equations and limits","category":"page"},{"location":"S3_examples/DistributedOPF/#Distributed-DC-OPF-Formulation-2","page":"Distributed OPF","title":"Distributed DC OPF Formulation","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"The original centralized problem is reformulated as:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"beginaligned\nmin_x_p_pin P quad  sum_p=1^P left(sum_gin G_i iin Z_p c_g(P_g^p)right) \ntextst quad  theta^z(i)_i = theta^z(j)_i quad forall (ij)in textTL \n theta^z(i)_j = theta^z(j)_j quad forall (ij)in textTL \n f^z(i)_ij= f^z(j)_ij quad forall (ij)in textTL \n x_p in X_p quad forall p in P\nendaligned","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Where z(i) denotes the zone containing bus i.","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Key Features:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Separable objective: Generation costs are distributed across zones. They are usually quaradtic or linear with respect to real power generations, and hence are differentiable. \nConsensus constraints: Ensure consistency for tie line variables. These constraints will be relaxed through the augmented Lagrangian function. \nLocal constraints: Each zone maintains its own feasible region. We assume the proximal oracle of X_p, i.e., projection onto X_p can be computed. This requres minimizing a quadratic function over linear constraints defined by X_p. ","category":"page"},{"location":"S3_examples/DistributedOPF/#Zone-Partitioning","page":"Distributed OPF","title":"Zone Partitioning","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"The distributed formulation above is suitable for ADMM only if the zone connectivity graph is bipartite. When zones are connected in non-bipartite patterns (containing odd cycles), subproblems corresponding to connected zones are coupled together and hence ADMM are not applicable. ","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Core Idea: PDMO.jl will automatically transform the non-bipartite zone graph into a bipartite form through the following three-step process:","category":"page"},{"location":"S3_examples/DistributedOPF/#1.-Detect","page":"Distributed OPF","title":"1. Detect","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Bipartite structure detection: Automatically detect if the zone connectivity graph is bipartite or contains odd cycles\nGraph assessment: Determine if transformation is needed (e.g., if there are only two zones, the graph is already bipartite)","category":"page"},{"location":"S3_examples/DistributedOPF/#2.-Transform-using-User-Selected-Algorithm","page":"Distributed OPF","title":"2. Transform using User-Selected Algorithm","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Algorithm selection: Apply the user-specified bipartization algorithm to transform the graph into a bipartite structure\nEdge subdivision: Break odd cycles by strategically subdividing problematic edges","category":"page"},{"location":"S3_examples/DistributedOPF/#3.-Prepare-Necessary-Data-Structures","page":"Distributed OPF","title":"3. Prepare Necessary Data Structures","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Introduce auxiliary variables: Create new consensus variables at subdivision points to maintain problem equivalence\nAdd new consensus constraints: Establish additional equality constraints linking original and auxiliary variables\nUpdate coupling structure: Ensure all inter-zone couplings follow the bipartite pattern (each constraint couples exactly one \"left\" variable with one \"right\" variable)","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"After this automatic transformation procedure, the OPF problem achieves the desired structure:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Bipartite connectivity: All zones can be assigned to \"left\" or \"right\" partitions\nSimple couplings: Each consensus constraint involves exactly two zones from different partitions\nADMM-ready: Direct application of ADMM with theoretical convergence guarantees","category":"page"},{"location":"S3_examples/DistributedOPF/#Experimental-Setup","page":"Distributed OPF","title":"Experimental Setup","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/#Test-Cases","page":"Distributed OPF","title":"Test Cases","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"We evaluate the above distributed approach on standard IEEE test systems. These data files are available in Matpower. ","category":"page"},{"location":"S3_examples/DistributedOPF/#Partitioning-Strategy","page":"Distributed OPF","title":"Partitioning Strategy","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"We use METIS.jl to divide each network into 3-10 zones, where each zone represents a separate control area.  As the partitions given by METIS.jl are in general not bipartitie, we further compare two bipartization algorithms: ","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP Bipartization: Use mixed-integer programming to minimize cetrain metric related to the worst case iteration complexity. \nBFS Bipartization: Heuristic using breadth-first search to break odd cycles. ","category":"page"},{"location":"S3_examples/DistributedOPF/#ADMM-Parameters","page":"Distributed OPF","title":"ADMM Parameters","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"param = ADMMParam()\nparam.solver = AdaptiveLinearizedSolver(gamma=1.0, r=1e5)\nparam.maxIter = 500000\nparam.timeLimit = 7200      # time limit in seconds\nparam.presTolL2 = Inf \nparam.dresTolL2 = Inf \nparam.presTolLInf = 1e-3    # report optimal if max primal/dual\nparam.dresTolLInf = 1e-3    # residuals are less than 1e-3","category":"page"},{"location":"S3_examples/DistributedOPF/#Results","page":"Distributed OPF","title":"Results","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"The experimental results demonstrate the effectiveness of the bipartization approach across different IEEE test systems. Below we present detailed performance comparisons between MILP and BFS bipartization algorithms for each test case.","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-30-(30-bus-system)","page":"Distributed OPF","title":"Case 30 (30-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Performance Summary:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 696 4.75 6.00e-04 696 2.77 6.00e-04\n4 1462 6.05 2.39e-02 4421 17.81 1.88e-02\n5 565 2.24 1.00e-02 804 3.11 1.60e-03\n6 680 2.97 1.61e-02 680 2.65 1.61e-02\n7 720 3.32 7.10e-03 612 2.65 8.20e-03\n8 733 3.34 4.50e-03 792 3.61 1.70e-03\n9 2598 11.96 1.13e-02 1560 6.60 1.36e-02\n10 2586 13.28 1.36e-02 1614 6.97 1.00e-04\nAverage 1255 5.99 1.09e-02 1397 5.77 7.60e-03","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Overall Statistics:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP: Average 1255.0 iterations, 5.99s, 8/8 convergence\nBFS: Average 1397.4 iterations, 5.77s, 8/8 convergence","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-57-(57-bus-system)","page":"Distributed OPF","title":"Case 57 (57-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Performance Summary:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 4717 21.83 0.00e+00 4722 19.29 0.00e+00\n4 3369 14.07 2.20e-01 5530 21.78 6.00e-02\n5 7685 31.38 0.00e+00 9168 38.21 1.80e-01\n6 12651 51.16 4.00e-02 9907 40.31 1.80e-01\n7 9119 36.91 7.30e-01 11176 45.76 2.70e-01\n8 7012 30.59 2.00e-01 11661 54.55 1.00e-01\n9 12450 72.65 2.80e-01 20922 141.91 3.70e-01\n10 30712 197.54 9.00e-02 43959 282.54 2.00e-02\nAverage 10964 57.02 1.95e-01 14631 80.54 1.48e-01","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Overall Statistics:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP: Average 10964.4 iterations, 57.02s, 8/8 convergence\nBFS: Average 14630.6 iterations, 80.54s, 8/8 convergence","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-89pegase-(89-bus-system)","page":"Distributed OPF","title":"Case 89pegase (89-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Performance Summary:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 100740 818.72 0.00e+00 58182 493.29 0.00e+00\n4 70619 573.48 0.00e+00 70619 556.25 0.00e+00\n5 152987 1169.31 1.00e-02 127198 797.80 1.00e-02\n6 347983 2166.73 1.00e-02 212998 1302.99 1.00e-02\n7 172778 1107.11 0.00e+00 118400 719.54 0.00e+00\n8 399074 2945.31 0.00e+00 500000* 3776.84 0.00e+00\n9 424753 3306.33 0.00e+00 416630 2763.47 0.00e+00\n10 500000* 3508.14 0.00e+00 500000* 3893.15 0.00e+00\nAverage 271117 1949.39 2.50e-03 250503 1787.92 2.50e-03","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"*Hit iteration limit","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Overall Statistics:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP: Average 271116.8 iterations, 1949.39s, 7/8 convergence\nBFS: Average 250503.4 iterations, 1787.92s, 6/8 convergence","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-118-(118-bus-system)","page":"Distributed OPF","title":"Case 118 (118-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 500000* 3328.25 6.00e-02 500000* 3134.82 6.00e-02\n4 196674 1192.41 0.00e+00 219267 1357.12 2.00e-02\n5 157265 913.78 1.00e-02 421100 2538.47 5.00e-02\n6 469894 2861.42 1.00e-02 310285 1878.80 1.00e-02\n7 500000* 3134.15 7.00e-02 500000* 3916.00 1.00e-01\n8 327146 2056.68 0.00e+00 273241 2532.66 1.00e-02\n9 500000* 3287.07 4.00e-02 500000* 3298.54 2.60e-01\n10 247418 1609.12 0.00e+00 500000* 4097.89 3.00e-02\nAverage 362300 2297.86 2.38e-02 402987 2844.29 6.75e-02","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"*Hit iteration limit","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Overall Statistics:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP: Average 362299.6 iterations, 2297.86s, 5/8 convergence\nBFS: Average 402986.6 iterations, 2844.29s, 4/8 convergence","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-300-(300-bus-system)","page":"Distributed OPF","title":"Case 300 (300-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 500000* 6595.10 2.00e-02 500000* 5745.13 3.00e-02\n4 44125 451.56 4.00e-02 46336 470.23 1.00e-02\n5 63456 613.71 1.00e-02 62758 605.65 1.00e-02\n6 141267 1262.08 1.00e-02 256760 2301.45 1.00e-02\n7 88318 755.26 1.00e-02 86520 736.64 2.00e-02\n8 320560 2671.08 1.00e-02 441353 4476.19 2.00e-02\n9 78130 674.80 1.00e-02 75884 636.14 3.00e-02\n10 333189 2867.36 0.00e+00 317086 2589.16 1.00e-02\nAverage 196131 1986.37 1.38e-02 223337 2195.08 1.75e-02","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"*Hit iteration limit","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Overall Statistics:","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"MILP: Average 196130.6 iterations, 1986.37s, 7/8 convergence \nBFS: Average 223337.1 iterations, 2195.08s, 7/8 convergence","category":"page"},{"location":"S3_examples/DistributedOPF/#Case-1888rte-(1888-bus-system)","page":"Distributed OPF","title":"Case 1888rte (1888-bus system)","text":"","category":"section"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Partitions MILP Iters MILP Time (s) MILP Abs Diff BFS Iters BFS Time (s) BFS Abs Diff\n3 101368 5511.77 0.00e+00 101368 5610.75 0.00e+00\n4 148390 6423.14 0.00e+00 135895 5857.55 0.00e+00\n5 196675 6945.33 0.00e+00 193456 7200.01* 1.00e-02\n6 228659 6742.48 0.00e+00 228659 6839.82 0.00e+00\n7 243140 7200.00* 0.00e+00 237310 7070.78 0.00e+00\n8 247383 7029.14 0.00e+00 212039 5732.35 0.00e+00\n9 231385 6112.67 0.00e+00 226505 5970.93 0.00e+00\n10 164693 4072.68 0.00e+00 216148 7200.01* 0.00e+00\nAverage 195212 6254.65 0.00e+00 193923 6435.27 1.25e-03","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"*Hit time limit","category":"page"},{"location":"S3_examples/DistributedOPF/","page":"Distributed OPF","title":"Distributed OPF","text":"Note: The experiments presented here are primarily intended to demonstrate the correctness of bipartization algorithms and illustrate how different reformulations affect ADMM convergence behavior. The distributed approach is not currently designed to compete with centralized solvers in terms of computational efficiency. We will focus on performance optimization and scalability improvements in future development.","category":"page"},{"location":"S4_api/admm/#ADMM","page":"ADMM","title":"ADMM","text":"","category":"section"},{"location":"S4_api/admm/","page":"ADMM","title":"ADMM","text":"This page documents the Alternating Direction Method of Multipliers (ADMM) algorithm components in PDMO.jl.","category":"page"},{"location":"S4_api/admm/#Parameters-and-Iteration-Information","page":"ADMM","title":"Parameters and Iteration Information","text":"","category":"section"},{"location":"S4_api/admm/#PDMO.ADMMParam","page":"ADMM","title":"PDMO.ADMMParam","text":"ADMMParam\n\nParameters for the Alternating Direction Method of Multipliers (ADMM) algorithm.\n\nADMM solves constrained optimization problems of the form:\n\nminimize    f(x) + g(y)\nsubject to  Ax + By = c\n\nThe algorithm iteratively updates primal variables x, y and dual variable λ (Lagrange multiplier) using the augmented Lagrangian with penalty parameter ρ.\n\nFields\n\nConvergence Parameters\n\ninitialRho::Float64: Initial penalty parameter ρ > 0. Controls the weight of the constraint  violation penalty in the augmented Lagrangian. Larger values enforce constraints more strictly  but may slow convergence. Default: 10.0\nmaxIter::Int64: Maximum number of ADMM iterations before termination. Default: 100000\npresTolL2::Float64: Primal residual tolerance in L2 norm. Terminates when  ||Ax + By - c||₂ ≤ presTolL2. Default: 1e-4\ndresTolL2::Float64: Dual residual tolerance in L2 norm. Terminates when  ||ρAᵀ(λᵏ⁺¹ - λᵏ)||₂ ≤ dresTolL2. Default: 1e-4\npresTolLInf::Float64: Primal residual tolerance in L∞ norm. Alternative termination  criterion: ||Ax + By - c||∞ ≤ presTolLInf. Default: 1e-6\ndresTolLInf::Float64: Dual residual tolerance in L∞ norm. Alternative termination  criterion for dual residual. Default: 1e-6\n\nAlgorithm Components\n\nsolver::AbstractADMMSubproblemSolver: Strategy for solving the x and y subproblems:\nDoublyLinearizedSolver(): Linearizes both subproblems for faster iterations\nOriginalADMMSubproblemSolver(): Solves subproblems exactly (slower but more accurate)\nAdaptiveLinearizedSolver(): Adaptive linearization with dynamic step size control\nDefault: DoublyLinearizedSolver()\nadapter::AbstractADMMAdapter: Strategy for dynamically adjusting ρ during iterations:\nNullAdapter(): Fixed ρ throughout iterations\nResidualBalancingAdapter(): Adjusts ρ to balance primal and dual residual magnitudes\nDefault: NullAdapter()\naccelerator::AbstractADMMAccelerator: Acceleration scheme for faster convergence:\nNullAccelerator(): Standard ADMM without acceleration\nAndersonAccelerator(): Anderson acceleration using previous iterates\nAutoHalpernAccelerator(): Automatic Halpern acceleration\nDefault: NullAccelerator()\n\nPractical Settings\n\nlogInterval::Int64: Print progress information every logInterval iterations.  Set to 0 to disable logging. Default: 1000\ntimeLimit::Float64: Maximum wall-clock time in seconds. Algorithm terminates if  time limit is exceeded. Default: 3600.0 (1 hour)\napplyScaling::Bool: Whether to apply problem scaling for better numerical conditioning. Default: false\nenablePathologyCheck::Bool: Whether to enable checks for pathological behavior  (e.g., divergence, numerical instability). Default: false\n\nConstructor\n\nThe constructor ADMMParam(; kwargs...) creates a parameter set with default values that can be customized via keyword arguments. All parameters are optional and have sensible defaults.\n\nExamples\n\n# Default parameters\nparams = ADMMParam()\n\n# Custom parameters via keyword arguments\nparams = ADMMParam(\n    initialRho = 1.0,\n    maxIter = 50000,\n    presTolL2 = 1e-6,\n    solver = OriginalADMMSubproblemSolver(),\n    applyScaling = true\n)\n\n# Modify after construction\nparams = ADMMParam()\nparams.initialRho = 1.0\nparams.adapter = RBAdapter()\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.ADMMIterationInfo","page":"ADMM","title":"PDMO.ADMMIterationInfo","text":"ADMMIterationInfo\n\nData structure to track the progress and results of ADMM iterations.\n\nFields\n\npresL2::Vector{Float64}: Primal residual L2 norms\ndresL2::Vector{Float64}: Dual residual L2 norms\npresLInf::Vector{Float64}: Primal residual infinity norms\ndresLInf::Vector{Float64}: Dual residual infinity norms\nobj::Vector{Float64}: Objective values\nalObj::Vector{Float64}: Augmented Lagrangian objective values\nrhoHistory::Vector{Tuple{Float64, Int64}}: History of penalty parameter updates\nprimalSol::Dict{String, NumericVariable}: Current primal solution\ndualSol::Dict{String, NumericVariable}: Current dual solution\ndualSolPrev::Dict{String, NumericVariable}: Previous dual solution\nprimalBuffer::Dict{String, NumericVariable}: Buffer for primal computations\ndualBuffer::Dict{String, NumericVariable}: Buffer for dual computations\nstopIter::Int64: Iteration at which the algorithm stopped\ntotalTime::Float64: Total computation time\nterminationStatus::ADMMTerminationStatus: Termination status\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#Subproblem-Solvers","page":"ADMM","title":"Subproblem Solvers","text":"","category":"section"},{"location":"S4_api/admm/","page":"ADMM","title":"ADMM","text":"Implementations for ADMM subproblem solvers.","category":"page"},{"location":"S4_api/admm/#PDMO.AbstractADMMSubproblemSolver","page":"ADMM","title":"PDMO.AbstractADMMSubproblemSolver","text":"AbstractADMMSubproblemSolver\n\nAbstract base type for ADMM subproblem solvers.\n\nADMM subproblem solvers are responsible for solving the primal variable update steps in the ADMM algorithm. Each solver implements a specific strategy for handling the optimization subproblems that arise in the ADMM decomposition.\n\nMathematical Background\n\nThe ADMM algorithm alternates between updating primal variables (x, z) and dual variables (λ). The primal subproblems take the general form:\n\nmin_x f(x) + g(x) + λ Ax + Bz - c + fracρ2Ax + Bz - c_2^2\n\nwhich can be rewritten as:\n\nmin_x f(x) + g(x) + λ Ax + fracρ2(x^T A^T A x + 2A^T(Bz - c) x + textconst)\n\nRequired Interface Methods\n\nAll concrete subproblem solvers must implement:\n\ninitialize!(solver, admmGraph, info): Initialize solver with problem structure\nsolve!(solver, nodeID, accelerator, admmGraph, info, isLeft, enableParallel): Solve subproblem for a specific node\nupdateDualResidualsInBuffer!(solver, info, admmGraph, accelerator): Compute dual residuals\nupdate!(solver, info, admmGraph, rhoUpdated): Update solver state when parameters change\n\nAvailable Solver Types\n\nOriginalADMMSubproblemSolver: Exact solution using specialized solvers (JuMP, proximal, linear)\nDoublyLinearizedSolver: Linearized approach with separate stepsizes for left/right nodes\nAdaptiveLinearizedSolver: Adaptive linearized method with dynamic stepsize selection\n\nSolver Selection Guidelines\n\nOriginalADMMSubproblemSolver: Use when exact solutions are needed and subproblems have known structure\nDoublyLinearizedSolver: Use for large-scale problems where exact solutions are expensive\nAdaptiveLinearizedSolver: Use when optimal convergence rates are critical and problem geometry varies\n\nPerformance Considerations\n\nExact solvers: Higher per-iteration cost but fewer iterations\nLinearized solvers: Lower per-iteration cost but more iterations\nAdaptive solvers: Moderate per-iteration cost with optimal convergence rates\n\nExample Usage\n\n# Initialize solver\nsolver = DoublyLinearizedSolver(dualStepsize=1.0)\ninitialize!(solver, admmGraph, info)\n\n# Solve subproblems for all nodes\nfor nodeID in keys(admmGraph.nodes)\n    isLeft = nodeID in admmGraph.left\n    solve!(solver, nodeID, accelerator, admmGraph, info, isLeft)\nend\n\n# Update dual residuals\nupdateDualResidualsInBuffer!(solver, info, admmGraph, accelerator)\n\nImplementation Notes\n\nSolvers should handle both left and right nodes in bipartite graphs\nBuffer management is crucial for performance\nParallel computation should be supported when beneficial\nState updates are needed when penalty parameters change\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.OriginalADMMSubproblemSolver","page":"ADMM","title":"PDMO.OriginalADMMSubproblemSolver","text":"OriginalADMMSubproblemSolver <: AbstractADMMSubproblemSolver\n\nOriginal ADMM subproblem solver that computes exact solutions using specialized solvers.\n\nThis solver implements the standard ADMM approach where each primal subproblem is solved exactly (or to high precision) using problem-specific specialized solvers. It automatically detects subproblem structure and selects the most appropriate solver from a hierarchy of specialized methods.\n\nMathematical Background\n\nThe ADMM subproblem for node i takes the form:\n\nmin_x_i f_i(x_i) + g_i(x_i) + λ A_i x_i + fracρ2A_i x_i + sum_ji A_j x_j - c_2^2\n\nThis can be rewritten as:\n\nmin_x_i f_i(x_i) + g_i(x_i) + λ A_i x_i + fracρ2(x_i^T A_i^T A_i x_i + 2A_i^T(sum_ji A_j x_j - c) x_i + textconst)\n\nSpecialized Solver Hierarchy\n\nThe solver attempts to use specialized solvers in order of preference:\n\nLinearSolver: For linear/quadratic subproblems (fastest)\nProximalMappingSolver: For problems with known proximal operators\nJuMPSolver: For general nonlinear problems (most flexible)\n\nKey Features\n\nAutomatic Detection: Automatically selects the best solver for each subproblem\nExact Solutions: Computes exact or high-precision solutions\nPrecomputed Adjoints: Uses EdgeData for efficient repeated computations\nParallel Support: Supports parallel solution of independent subproblems\nAnderson Acceleration: Special handling for Anderson acceleration schemes\n\nPerformance Characteristics\n\nConvergence: Typically requires fewer iterations due to exact solutions\nPer-iteration Cost: Higher than linearized methods but more reliable\nMemory Usage: Moderate (precomputed adjoint mappings)\nScalability: Good for problems with structured subproblems\n\nImplementation Notes\n\nEach node gets its own specialized solver instance\nEdge data is precomputed once during initialization\nAugmented Lagrangian coefficients are recomputed each iteration\nSpecial handling for Anderson acceleration requires modified linear terms\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.SpecializedOriginalADMMSubproblemSolver","page":"ADMM","title":"PDMO.SpecializedOriginalADMMSubproblemSolver","text":"SpecializedOriginalADMMSubproblemSolver\n\nAbstract type for specialized original ADMM subproblem solvers.\n\nThis type represents the interface for specialized solvers that implement the original ADMM subproblem formulation. Each specialized solver must provide the following methods:\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.LinearSolver","page":"ADMM","title":"PDMO.LinearSolver","text":"LinearSolver <: SpecializedOriginalADMMSubproblemSolver\n\nSpecialized solver for ADMM subproblems that reduce to linear systems.\n\nThis solver handles ADMM subproblems with quadratic or linear objective functions and unconstrained domains, which can be solved exactly by solving linear systems rather than using iterative optimization methods. The solver uses sparse matrix factorizations for efficient solution of the resulting linear systems.\n\nMathematical Formulation\n\nThe solver handles subproblems of the form:     min f(x) + ⟨λ, Ax + By^k - b⟩ + (ρ/2)||Ax + By^k - b||²\n\nwhere f(x) is quadratic, linear, or zero:\n\nQuadratic: f(x) = (1/2)x^T Q x + q^T x + r\nLinear: f(x) = q^T x + r  \nZero: f(x) = 0\n\nOptimality Conditions\n\nThe first-order optimality conditions yield linear systems:\n\nWith Q: (ρA^T A + Q)x = -A^T λ - ρA^T(By^k - b) - q\nWithout Q: A^T A x = -A^T λ/ρ - A^T(By^k - b) - q/ρ\n\nSupported Problem Types\n\nObjective Functions: Zero, AffineFunction, QuadraticFunction\nDomain Constraints: Unconstrained or unbounded box constraints\nVariable Types: Vector variables only (no matrix variables)\nConstraint Mappings: Matrix, Identity, and Extraction mappings\n\nFactorization Strategy\n\nThe solver uses adaptive factorization based on matrix properties:\n\nCholesky: For positive definite system matrices (preferred)\nLDLT: For positive semidefinite system matrices (fallback)\nPrecomputed A^T A: Efficient handling of constraint structure\n\nFields\n\nQ::Union{SparseMatrixCSC{Float64, Int64}, Nothing}: Quadratic term matrix\nq::Union{Vector{Float64}, Nothing}: Linear term vector\nAAdjointSelf::SparseMatrixCSC{Float64, Int64}: Precomputed A^T A matrix\ncurrentRho::Float64: Current penalty parameter ρ\nsystemMatrix::SparseMatrixCSC{Float64, Int64}: System matrix for factorization\nfactorization::Union{SparseArrays.CHOLMOD.Factor{Float64}, Nothing}: Matrix factorization\nisPositiveDefinite::Bool: Whether system matrix is positive definite\nrhsBuffer::Vector{Float64}: Working space for right-hand side vectors\n\nConstructor Parameters\n\nnodeID::String: Node identifier in the ADMM graph\nadmmGraph::ADMMBipartiteGraph: The bipartite graph structure\nedgeData::Dict{String, EdgeData}: Precomputed edge information\ninitialRho::Float64: Initial penalty parameter\n\nPerformance Characteristics\n\nComputational Complexity: O(n³) for factorization, O(n²) for solves\nMemory Usage: O(n²) for factorization storage\nConvergence: Exact solution in single step\nScalability: Excellent for problems with sparse structure\n\nImplementation Notes\n\nUses CHOLMOD for high-performance sparse factorizations\nReuses factorizations when Q is present and ρ is fixed\nHandles both positive definite and semidefinite cases\nEfficient A^T A precomputation for constraint structure\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.JuMPSolver","page":"ADMM","title":"PDMO.JuMPSolver","text":"JuMPSolver <: SpecializedOriginalADMMSubproblemSolver\n\nJuMP-based solver for exact ADMM subproblems requiring optimization modeling.\n\nThis solver handles complex ADMM subproblems that cannot be solved in closed form, including those with nonlinear objectives, complex constraints, or specialized structure. It constructs and solves optimization models using the JuMP mathematical programming interface with the Ipopt nonlinear optimizer.\n\nMathematical Formulation\n\nThe solver handles subproblems of the form:     min f(x) + g(x) + ⟨λ, Ax + By^k - b⟩ + (ρ/2)||Ax + By^k - b||²\n\nwhere:\n\nf(x): Smooth objective function (linear, quadratic, or nonlinear)\ng(x): Constraint function (handled via JuMP constraints)\nAx + By^k - b: ADMM coupling constraints\nλ: Dual variables, ρ: Penalty parameter\n\nSupported Problem Types\n\nLinear Objectives: f(x) = c^T x\nQuadratic Objectives: f(x) = (1/2)x^T Q x + c^T x\nNonlinear Objectives: Currently supports ComponentwiseExponentialFunction\nConstraint Mappings: Matrix, Identity, and Extraction mappings\nDomain Constraints: Handled through JuMP constraint system\n\nSolver Configuration\n\nPrimary Optimizer: Ipopt (Interior Point Optimizer)\nLinear Solver: MA27 (if HSL library available) for enhanced performance\nTolerance Settings: High precision for ADMM convergence requirements\nOutput: Silent mode for integration with ADMM framework\n\nFields\n\nmodel::JuMP.AbstractModel: JuMP optimization model\nvar::Dict{String, Vector{JuMP.VariableRef}}: JuMP variables by node\naux::Dict{String, Vector{JuMP.VariableRef}}: Auxiliary variables if needed\nobjExpressions::Vector{Union{JuMP.AffExpr, JuMP.QuadExpr}}: Linear/quadratic objective parts\nblockHasNonlinearSmoothFunction::Bool: Flag for nonlinear objective detection\ncurrentRho::Float64: Current penalty parameter value\n\nConstructor Parameters\n\nnodeID::String: Node identifier in the ADMM graph\nadmmGraph::ADMMBipartiteGraph: The bipartite graph structure\nedgeData::Dict{String, EdgeData}: Precomputed edge information\nrho::Float64: Initial penalty parameter\n\nPerformance Characteristics\n\nComputational Complexity: Depends on problem structure and Ipopt performance\nMemory Usage: JuMP model storage plus solver workspace\nConvergence: Depends on Ipopt convergence for each subproblem\nScalability: Limited by nonlinear solver performance\n\nImplementation Notes\n\nUses HSL MA27 linear solver when available for better performance\nHandles both linear/quadratic and nonlinear objectives seamlessly\nIntegrates with ADMM framework through standardized interface\nProvides automatic model construction from problem structure\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.ProximalMappingSolver","page":"ADMM","title":"PDMO.ProximalMappingSolver","text":"ProximalMappingSolver <: SpecializedOriginalADMMSubproblemSolver\n\nSpecialized solver for ADMM subproblems that reduce to proximal mappings.\n\nThis solver handles a specific class of ADMM subproblems where the objective function is purely regularization (f(x) = 0) and the constraint mapping is a scaled identity. Such subproblems can be efficiently solved using proximal operators rather than general optimization methods.\n\nMathematical Formulation\n\nThe solver handles subproblems of the form:     min g(y) + ⟨λ, Ax^k + By - b⟩ + (ρ/2)||Ax^k + By - b||²\n\nwhere:\n\ng(y): Proximal-friendly regularization function\nB = mI: Scaled identity mapping (m ≠ 0)\nf(y) = 0: Zero smooth objective function\n\nTransformation to Proximal Mapping\n\nBy substituting B = mI, the subproblem becomes:     min g(y) + ⟨m·λ, y⟩ + (ρm²/2)||y - (b - Ax^k)/m||²\n\nThis is equivalent to the proximal mapping:     min g(y) + (1/2γ)||y - z||²\n\nwhere:\n\nz = (b - Ax^k - λ/ρ)/m: Proximal center point\nγ = 1/(ρm²): Proximal parameter\n\nSupported Mapping Types\n\nLinearMappingIdentity: B = cI with scaling coefficient c\nLinearMappingExtraction: B = cE with extraction operator E\nLinearMappingMatrix: B = cI where matrix A is c·I (validated)\n\nConsistency Validation\n\nThe constructor verifies that all constraint mappings for the node are consistent scaled identity operations:\n\nAll mappings must scale the same vector by the same coefficient\nThe scaling coefficient must be non-zero\nInconsistent scaling throws an error\n\nFields\n\nscalingCoefficient::Float64: The scaling coefficient m in B = mI\nproximalPoint::NumericVariable: Working space for proximal center z\ncurrentRho::Float64: Current penalty parameter ρ\ngamma::Float64: Proximal parameter γ = 1/(ρm²)\n\nConstructor Parameters\n\nnodeID::String: Node identifier in the ADMM graph\nadmmGraph::ADMMBipartiteGraph: The bipartite graph structure\nedgeData::Dict{String, EdgeData}: Precomputed edge information\nrho::Float64: Initial penalty parameter\n\nPerformance Characteristics\n\nComputational Complexity: O(n) where n is the variable dimension\nMemory Usage: O(n) for proximal point storage\nConvergence: Depends on proximal operator efficiency\nScalability: Excellent for large-scale problems with appropriate structure\n\nImplementation Notes\n\nRequires f(x) = 0 (Zero objective function)\nValidates mapping consistency during construction\nUses efficient proximal operator implementations\nAutomatic scaling coefficient detection and validation\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.DoublyLinearizedSolver","page":"ADMM","title":"PDMO.DoublyLinearizedSolver","text":"DoublyLinearizedSolver <: AbstractADMMSubproblemSolver\n\nDoubly Linearized Subproblem Solver for ADMM optimization.\n\nThis solver uses a doubly linearized approach where both the objective function gradients and the augmented Lagrangian terms are linearized. It maintains separate proximal stepsizes for left and right nodes (α and β) and uses operator norm estimates for stability.\n\nFields\n\ndualStepsize::Float64: Step size for dual variable updates\nproximalStepsizeAlpha::Float64: Proximal step size for left nodes  \nproximalStepsizeBeta::Float64: Proximal step size for right nodes\nprimalBuffer::Dict{String, NumericVariable}: Buffer for primal computations\ndualBuffer::Dict{String, NumericVariable}: Buffer for dual computations\nmaxLeftLipschitzConstant::Float64: Maximum Lipschitz constant for left node objectives\nmaxRightLipschitzConstant::Float64: Maximum Lipschitz constant for right node objectives\nmaxLeftMatrixAdjointSelfOperatorNorm::Float64: Maximum operator norm for left mappings\nmaxRightMatrixAdjointSelfOperatorNorm::Float64: Maximum operator norm for right mappings\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.AdaptiveLinearizedSolver","page":"ADMM","title":"PDMO.AdaptiveLinearizedSolver","text":"AdaptiveLinearizedSolver <: AbstractADMMSubproblemSolver\n\nAdaptive Linearized ADMM Solver with dynamic step size optimization.\n\nThis solver implements a sophisticated adaptive variant of linearized ADMM where the proximal step size γ is dynamically optimized based on real-time analysis of the optimization landscape. The algorithm maintains separate gradient histories for left and right nodes and uses geometric convergence analysis to balance convergence speed with numerical stability.\n\nMathematical Foundation\n\nThe adaptive linearized ADMM solves subproblems of the form:\n\nLeft nodes: x^{k+1} = prox_{γg}(x^k - γ(∇f(x^k) + A^T u^{k+1}))\nRight nodes: y^{k+1} = prox_{γh}(y^k - γ(∇g(y^k) + B^T u^{k+1}))\n\nwhere γ is adaptively updated by analyzing:\n\nLocal Lipschitz constants Lf, Lg of objective gradients\nStrong convexity parameters ℓf, ℓg when available\nOperator norms ||A^T A||, ||B^T B|| of constraint mappings\nConvergence rate bounds and stability margins\n\nAdaptive Strategy\n\nThe algorithm optimizes γ by solving constrained optimization problems:\n\nPrimal-Dual Balance: Ensure balanced convergence rates between primal and dual variables\nGeometric Adaptation: Incorporate problem-specific curvature information\nStability Constraints: Maintain numerical stability through step size bounds\nConvergence Acceleration: Maximize convergence speed subject to stability\n\nAlgorithm Phases\n\nGradient History Tracking: Maintain ∇f(x^k), ∇f(x^{k-1}), ∇g(y^k), ∇g(y^{k-1})\nConstraint Violation Analysis: Compute Δu = φ(A(x^k - x^{k-1}) + B(y^k - y^{k-1})) + Ax^k + By^k - c\nGeometry Estimation: Estimate local Lipschitz constants and operator norms\nOptimization: Solve cubic equations for optimal γ candidates\nSelection: Choose minimum valid γ among all candidates\n\nFields\n\nproximalStepsizeGamma::Float64: Current adaptive proximal step size γ\nprimalBuffer::Dict{String, NumericVariable}: Working space for primal computations\ndualBuffer::Dict{String, NumericVariable}: Working space for dual computations\nφ::Float64: Golden ratio parameter (≈1.618) for constraint violation weighting\nr::Float64: Primal-dual ratio parameter controlling balance between updates\nprimalprev::Dict{String, NumericVariable}: Previous iteration primal variables\nxgradPrev::Dict{String, NumericVariable}: Previous gradients for left nodes\nxgradCur::Dict{String, NumericVariable}: Current gradients for left nodes\nygradPrev::Dict{String, NumericVariable}: Previous gradients for right nodes\nygradCur::Dict{String, NumericVariable}: Current gradients for right nodes\nifSimple::Bool: Whether to use simplified adaptive scheme\n\nConstructor Parameters\n\ngamma::Float64=1.0: Initial proximal step size\nr::Float64=1.0: Primal-dual balancing parameter\nifSimple::Bool=false: Use simplified adaptive scheme if true\n\nPerformance Characteristics\n\nComputational Complexity: O(n) per iteration plus gradient evaluations\nMemory Usage: O(n) for gradient storage per node\nConvergence Rate: Adaptive between O(1/k) and linear depending on problem structure\nNumerical Stability: Automatic step size bounds prevent divergence\n\nImplementation Notes\n\nUses golden ratio φ = (1 + √5)/2 for optimal constraint violation weighting\nSimplified scheme sets φ = 2 for reduced computational overhead\nGradient buffering enables efficient multi-step analysis\nParallel computation across nodes for scalability\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#Adapters","page":"ADMM","title":"Adapters","text":"","category":"section"},{"location":"S4_api/admm/","page":"ADMM","title":"ADMM","text":"Penalty parameter adaptation strategies for ADMM algorithms.","category":"page"},{"location":"S4_api/admm/#PDMO.AbstractADMMAdapter","page":"ADMM","title":"PDMO.AbstractADMMAdapter","text":"AbstractADMMAdapter\n\nAbstract base type for ADMM penalty parameter adaptation schemes.\n\nADMM adapters are responsible for dynamically adjusting the penalty parameter (ρ) during ADMM iterations to improve convergence rates and numerical stability. The penalty parameter plays a crucial role in balancing the enforcement of constraints versus the optimization of the objective function.\n\nMathematical Background\n\nThe ADMM algorithm solves optimization problems of the form:\n\nmin_xz f(x) + g(z) text subject to  Ax + Bz = c\n\nThe augmented Lagrangian is:\n\nL_ρ(xzλ) = f(x) + g(z) + λ^T(Ax + Bz - c) + fracρ2Ax + Bz - c_2^2\n\nwhere ρ is the penalty parameter that affects:\n\nConvergence speed: Higher ρ can accelerate convergence but may cause numerical issues\nNumerical stability: Lower ρ provides better conditioning but slower convergence\nPrimal-dual balance: Optimal ρ balances primal and dual residual reductions\n\nAdapter Interface\n\nAll concrete adapter types must implement:\n\ninitialize!(adapter, info, admmGraph): Initialize adapter with problem data\nupdatePenalty(adapter, info, admmGraph, iter): Update penalty parameter and return whether changed\n\nCommon Adaptation Strategies\n\nResidual Balancing: Adjust ρ based on primal vs dual residual magnitudes\nSpectral Methods: Use spectral radius estimates to guide parameter selection\nAdaptive Schedules: Predetermined schedules based on iteration counts\nGradient-based: Use gradient information to optimize parameter selection\n\nPerformance Considerations\n\nFrequency: Adapters should not update ρ too frequently (every 5-10 iterations)\nMagnitude: Changes should be moderate (factors of 2-10) to avoid instability\nBounds: ρ should be constrained within reasonable numerical bounds\nConvergence: Adaptation should eventually stabilize as algorithm converges\n\nAvailable Implementations\n\nNullAdapter: No adaptation (constant ρ)\nRBAdapter: Residual balancing adaptation\nSRAAdapter: Spectral radius adaptive adaptation\n\nSee Also\n\nNullAdapter: No-operation adapter for baseline comparison\nRBAdapter: Residual balancing adapter\nSRAAdapter: Spectral radius adaptive adapter\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.NullAdapter","page":"ADMM","title":"PDMO.NullAdapter","text":"NullAdapter <: AbstractADMMAdapter\n\nNo-operation adapter that maintains a constant penalty parameter throughout ADMM iterations.\n\nThis adapter provides a baseline behavior where the penalty parameter ρ remains unchanged from its initial value. It's useful for:\n\nBaseline performance comparisons\nProblems where optimal ρ is known a priori\nDebugging and testing ADMM implementations\nScenarios where adaptation might be harmful\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.RBAdapter","page":"ADMM","title":"PDMO.RBAdapter","text":"RBAdapter <: AbstractADMMAdapter\n\nResidual Balance (RB) adapter for ADMM penalty parameter adaptation.\n\nThis adapter implements the residual balancing strategy which adjusts the penalty parameter ρ based on the relative magnitudes of primal and dual residuals to maintain balanced convergence rates.\n\nMathematical Background\n\nThe ADMM algorithm generates two types of residuals at each iteration:\n\nPrimal residual: r^k = Ax^k + Bz^k - c\nDual residual: s^k = ρA^T B(z^k - z^{k-1})\n\nThe optimal penalty parameter should balance the reduction of both residuals. The RB adapter uses the following strategy:\n\nIf ‖r^k‖₂ > μ ‖s^k‖₂: Increase ρ to emphasize constraint satisfaction\nIf ‖s^k‖₂ > μ ‖r^k‖₂: Decrease ρ to emphasize optimality conditions\nOtherwise: Keep ρ unchanged\n\nAlgorithm Details\n\nAt each iteration, the adapter:\n\nComputes the current primal residual norm: ‖r^k‖₂\nComputes the current dual residual norm: ‖s^k‖₂\nCompares their ratio against the threshold testRatio\nUpdates ρ by multiplying or dividing by adapterRatio\nClamps ρ to be within [ADMM_MIN_RHO, ADMM_MAX_RHO]\n\nParameters\n\ntestRatio::Float64: Threshold for residual ratio comparison (default: 10.0)\nadapterRatio::Float64: Factor for ρ adjustment (default: 2.0)\n\nParameter Guidelines\n\ntestRatio: Typical values 5-20. Higher values make adaptation less aggressive\nadapterRatio: Typical values 1.5-5. Higher values make larger adjustments\n\nConvergence Properties\n\nBalanced Convergence: Maintains similar convergence rates for primal and dual residuals\nRobust Performance: Works well across different problem types and scales\nProven Theory: Backed by convergence analysis\n\nPerformance Characteristics\n\nComputational Cost: O(1) per iteration (just ratio comparison)\nMemory Usage: O(1) - no additional storage required\nStability: Generally stable due to moderate adjustment factors\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.SRAAdapter","page":"ADMM","title":"PDMO.SRAAdapter","text":"SRAAdapter <: AbstractADMMAdapter\n\nSpectral Radius Adaptive (SRA) adapter for ADMM penalty parameter adaptation.\n\nThis adapter implements a sophisticated penalty parameter adaptation strategy based on spectral radius estimation. It analyzes the convergence behavior of dual variables and constraint violations to automatically adjust ρ for optimal convergence rates.\n\nMathematical Background\n\nThe SRA adapter is based on the observation that the optimal penalty parameter ρ is related to the spectral radius of the iteration matrix. The adapter approximates this by analyzing the relationship between consecutive iterates:\n\nFor dual variables: Δy^k = y^{k+1} - y^k For constraint violations: ΔBz^k = B(z^{k+1} - z^k)\n\nThe ratio ‖Δy^k‖₂ / ‖ΔBz^k‖₂ provides an estimate of the appropriate penalty parameter.\n\nAlgorithm Strategy\n\nThe adapter computes:\n\nDual difference norm: ‖y^{k+1} - y^k‖₂\nConstraint mapping difference norm: ‖B(z^{k+1} - z^k)‖₂  \nSpectral ratio: ρ_new = ‖Δy^k‖₂ / ‖ΔBz^k‖₂\n\nSpecial cases are handled when norms are near zero:\n\nIf ‖Δy^k‖₂ ≈ 0 and ‖ΔBz^k‖₂ > 0: Decrease ρ (dual optimality achieved)\nIf ‖Δy^k‖₂ > 0 and ‖ΔBz^k‖₂ ≈ 0: Increase ρ (constraint satisfaction achieved)\nIf both ≈ 0: Keep ρ unchanged (convergence achieved)\n\nParameters\n\nT::Int64: Update frequency (default: 5) - Parameter updated every T iterations\nincreasingFactor::Float64: Factor for increasing ρ in special cases (default: 2.0)\ndecreasingFactor::Float64: Factor for decreasing ρ in special cases (default: 2.0)\n\nAdvantages over RB Adapter\n\nTheoretical Foundation: Based on spectral analysis of ADMM convergence\nAdaptive Estimation: Directly estimates optimal ρ rather than simple ratio balancing\nSophisticated Handling: Special case analysis for near-convergence scenarios\nReduced Oscillations: Updates less frequently to avoid parameter oscillations\n\nPerformance Characteristics\n\nComputational Cost: O(problem_size) per update (computing norms)\nMemory Usage: O(problem_size) for storing difference vectors\nUpdate Frequency: Every T iterations (default: 5)\nStability: Generally more stable than frequent RB updates\n\nParameter Selection Guidelines\n\nT (Update Frequency): \nSmaller values (3-5): More responsive adaptation\nLarger values (8-15): More stable, less frequent updates\nVery large values (>20): May miss important adaptations\nincreasingFactor/decreasingFactor:\nSmaller values (1.5-2.0): Conservative adjustments\nLarger values (2.5-5.0): More aggressive adjustments\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#Accelerators","page":"ADMM","title":"Accelerators","text":"","category":"section"},{"location":"S4_api/admm/","page":"ADMM","title":"ADMM","text":"Acceleration techniques for improving ADMM convergence.","category":"page"},{"location":"S4_api/admm/#PDMO.AbstractADMMAccelerator","page":"ADMM","title":"PDMO.AbstractADMMAccelerator","text":"AbstractADMMAccelerator\n\nAbstract base type for ADMM acceleration schemes that can improve convergence rates.\n\nADMM accelerators implement various techniques to speed up the convergence of the Alternating Direction Method of Multipliers (ADMM) algorithm. Common approaches include:\n\nAnderson acceleration (extrapolation methods)\nHalpern-type acceleration schemes\nNull acceleration (no acceleration applied)\n\nRequired Interface Methods\n\nAll concrete subtypes must implement:\n\ninitialize!(accelerator, info, admmGraph): Initialize the accelerator with ADMM problem data\naccelerateBetweenPrimalUpdates!(accelerator, info, admmGraph): Apply acceleration between primal updates\naccelerateAfterDualUpdates!(accelerator, info): Apply acceleration after dual updates\n\nMathematical Background\n\nADMM acceleration works by combining information from multiple previous iterations to compute improved estimates of the solution. The general form can be written as:\n\nx_k+1 = f(x_k x_k-1  x_k-m)\n\nwhere f is the acceleration function and m is the memory depth.\n\nPerformance Considerations\n\nDifferent accelerators have varying memory requirements\nSome accelerators may increase per-iteration cost while reducing iteration count\nThe effectiveness depends on problem structure and conditioning\n\nSee Also\n\nAndersonAccelerator: Implementation of Anderson acceleration\nAutoHalpernAccelerator: Implementation of Halpern-type acceleration\nNullAccelerator: No-operation accelerator for baseline comparison\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.NullAccelerator","page":"ADMM","title":"PDMO.NullAccelerator","text":"NullAccelerator <: AbstractADMMAccelerator\n\nA no-operation accelerator that maintains default ADMM behavior without any acceleration.\n\nThis accelerator serves as a baseline for comparison with other acceleration schemes. All methods are implemented as no-ops, so the ADMM algorithm proceeds with its standard update rules.\n\nUse Cases\n\nBaseline performance comparison\nDebugging and testing ADMM implementations\nScenarios where acceleration is not desired or beneficial\n\nPerformance\n\nZero computational overhead\nNo memory usage beyond the accelerator object itself\nIdentical convergence behavior to unaccelerated ADMM\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.AndersonAccelerator","page":"ADMM","title":"PDMO.AndersonAccelerator","text":"AndersonAccelerator <: AbstractADMMAccelerator\n\nImplementation of Anderson acceleration for ADMM algorithms with safeguard strategy.\n\nThis extends the original Anderson accelerator with a safeguard mechanism that detects when the acceleration might be causing instability and reverts to previous stable iterates.\n\nSafeguard Strategy\n\nThe safeguard monitors the residual norm progression and triggers when:\n\nw_k  δ w_k-1\n\nwhere:\n\nw_k is the current iterate difference (residual)\nδ is the safeguard threshold parameter\nWhen triggered, the algorithm clears history and reverts to previous iterates\n\nAlgorithm Details\n\nHistory Management: Maintains circular buffers for previous iterates\nResidual Monitoring: Tracks residual norm progression for stability\nSafeguard Triggering: Detects potential instability and takes corrective action\nState Reversion: Reverts to previous stable state when safeguard triggers\nEfficient Updates: Uses QR decomposition with Givens rotations for least squares\n\nParameters\n\nhistoryDepth::Int64: Number of previous iterates to store and use\nbeta::Float64: Mixing parameter controlling acceleration strength\ndelta::Float64: Safeguard threshold parameter (typical values: 0.1-1.0)\n\nPerformance Characteristics\n\nMemory: O(historyDepth × problem_size)\nPer-iteration Cost: O(historyDepth × problem_size)\nStability: Enhanced stability through safeguard mechanism\nConvergence: Robust convergence even for ill-conditioned problems\n\nExample Usage\n\naccelerator = AndersonAccelerator(historyDepth=5, beta=1.0, delta=0.1)\ninitialize!(accelerator, info, admmGraph)\n\n# During ADMM iterations:\naccelerateBetweenPrimalUpdates!(accelerator, info, admmGraph)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/admm/#PDMO.AutoHalpernAccelerator","page":"ADMM","title":"PDMO.AutoHalpernAccelerator","text":"AutoHalpernAccelerator <: AbstractADMMAccelerator\n\nAutomatic Halpern acceleration for ADMM with adaptive activation and period detection.\n\nThis accelerator implements a sophisticated variant of Halpern iteration that automatically detects when to activate acceleration and adaptively manages the acceleration period.\n\nMathematical Background\n\nThe Halpern iteration scheme takes the form:\n\nx_k+1 = frackk+1 T(x_k) + frac1k+1 x_0\n\nwhere:\n\nT(x_k) is the ADMM operator applied to the current iterate\nx_0 is the anchor point (center point) for acceleration\nk is the iteration counter within the acceleration period\n\nAdaptive Features\n\nAutomatic Activation: Detects when acceleration should be enabled\nPeriod Management: Automatically restarts acceleration cycles\nRetrace Detection: Monitors iterate behavior to detect oscillations\nDual-Phase Operation: Switches between exploration and acceleration phases\n\nAlgorithm States\n\nInactive: Standard ADMM updates, monitoring for activation conditions\nSemi-Surpassed: Preparing for acceleration, validating conditions\nActive: Full Halpern acceleration with automatic period management\n\nKey Parameters\n\nmaxPeriod::Int: Maximum iterations in an acceleration cycle\niterDepth::Int: Current position within acceleration cycle\nisActive::Bool: Whether acceleration is currently applied\nisSemiSurpassed::Bool: Whether preparing for activation\n\nRetrace Mechanism\n\nThe accelerator tracks whether iterates are consistently increasing or decreasing. When full retrace is detected (all components have changed direction), it triggers acceleration restart with a new center point.\n\nPerformance Characteristics\n\nMemory: O(problem_size) for tracking iterate history\nComputation: O(problem_size) per iteration\nConvergence: Adaptive behavior can improve robustness over fixed schemes\n\nExample Usage\n\naccelerator = AutoHalpernAccelerator(maxPeriod=100)\ninitialize!(accelerator, info, admmGraph)\n\n# Acceleration is applied automatically during ADMM iterations\naccelerateAfterDualUpdates!(accelerator, info)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"S4_api/functions/","page":"Functions","title":"Functions","text":"This page documents the function types used in the PDMO optimization framework.","category":"page"},{"location":"S4_api/functions/#Abstract-Function-Interface","page":"Functions","title":"Abstract Function Interface","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.AbstractFunction","page":"Functions","title":"PDMO.AbstractFunction","text":"AbstractFunction\n\nAbstract base type for all functions in the Bipartization optimization framework.\n\nThis type serves as the foundation for implementing mathematical functions that can be used in optimization algorithms. It defines a common interface for function evaluation,  gradients, and proximal operators.\n\nInterface Requirements\n\nTo implement a new function type, you must:\n\nDefine a concrete struct that inherits from AbstractFunction\nImplement function evaluation by overriding the call operator\nImplement trait methods to specify function properties\nImplement oracles for smooth/proximal functions as needed\n\nCore Interface Methods\n\nFunction Evaluation\n\n(f::YourFunction)(x::NumericVariable, enableParallel::Bool=false) -> Float64\n\nTrait Methods (override as needed)\n\nisSmooth(::Type{YourFunction}) = true/false     # Has gradient?\nisProximal(::Type{YourFunction}) = true/false   # Has proximal operator?\nisConvex(::Type{YourFunction}) = true/false     # Is convex?\nisSet(::Type{YourFunction}) = true/false        # Is indicator function?\n\nGradient Oracle (if isSmooth = true)\n\ngradientOracle!(grad::NumericVariable, f::YourFunction, x::NumericVariable, enableParallel::Bool=false)\ngradientOracle(f::YourFunction, x::NumericVariable, enableParallel::Bool=false)\n\nProximal Oracle (if isProximal = true)\n\nproximalOracle!(y::NumericVariable, f::YourFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)\nproximalOracle(f::YourFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)\n\nImplementation Example\n\n# Define a simple quadratic function: f(x) = (1/2)||x||²\nstruct SimpleQuadratic <: AbstractFunction\n    # No fields needed for this simple case\nend\n\n# Specify traits\nisSmooth(::Type{SimpleQuadratic}) = true\nisConvex(::Type{SimpleQuadratic}) = true\nisProximal(::Type{SimpleQuadratic}) = true\n\n# Function evaluation\nfunction (f::SimpleQuadratic)(x::NumericVariable, enableParallel::Bool=false)\n    return 0.5 * sum(x.^2)\nend\n\n# Gradient oracle\nfunction gradientOracle!(grad::NumericVariable, f::SimpleQuadratic, x::NumericVariable, enableParallel::Bool=false)\n    grad .= x\nend\n\n# Proximal oracle\nfunction proximalOracle!(y::NumericVariable, f::SimpleQuadratic, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)\n    y .= x ./ (1.0 + gamma)\nend\n\nBuilt-in Function Types\n\nThe framework provides many built-in function types:\n\nBasic Functions: Zero, AffineFunction, QuadraticFunction\nNorms: ElementwiseL1Norm, FrobeniusNormSquare, MatrixNuclearNorm\nIndicators: IndicatorBox, IndicatorBallL2, IndicatorSOC, IndicatorPSD\nWrappers: WrapperScalingTranslationFunction, WrapperScalarInputFunction\nUser-defined: UserDefinedSmoothFunction, UserDefinedProximalFunction\n\nDesign Principles\n\nModularity: Each function type is self-contained\nEfficiency: Support for in-place operations and parallel computation\nFlexibility: Trait-based design allows algorithm selection\nExtensibility: Easy to add new function types\nType Safety: Strong typing helps catch errors early\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.NumericVariable","page":"Functions","title":"PDMO.NumericVariable","text":"NumericVariable\n\nType alias for function arguments in the Bipartization framework.\n\nThis type represents the domain of functions, encompassing both scalar and array inputs. It provides a unified interface for working with optimization variables of different dimensions and structures.\n\nDefinition\n\nNumericVariable = Union{Float64, AbstractArray{Float64, N} where N}\n\nSupported Types\n\nScalar: Float64 - Single real number\nVector: Vector{Float64} - 1D array of real numbers  \nMatrix: Matrix{Float64} - 2D array of real numbers\nTensor: Array{Float64, N} - N-dimensional array of real numbers\n\nExamples\n\n# Scalar variable\nx_scalar::NumericVariable = 3.14\n\n# Vector variable\nx_vector::NumericVariable = [1.0, 2.0, 3.0]\n\n# Matrix variable\nx_matrix::NumericVariable = [1.0 2.0; 3.0 4.0]\n\n# 3D tensor variable\nx_tensor::NumericVariable = rand(2, 3, 4)\n\nUsage in Functions\n\nFunctions that accept NumericVariable arguments can handle different input types:\n\nfunction (f::MyFunction)(x::NumericVariable, enableParallel::Bool=false)\n    if isa(x, Float64)\n        # Handle scalar case\n        return x^2\n    else\n        # Handle array case\n        return sum(x.^2)\n    end\nend\n\nDesign Rationale\n\nFlexibility: Supports various optimization variable structures\nType Safety: Ensures only numeric types are used\nPerformance: Avoids unnecessary type conversions\nCompatibility: Works with Julia's array ecosystem\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.isSmooth","page":"Functions","title":"PDMO.isSmooth","text":"isSmooth(f::AbstractFunction) -> Bool\nisSmooth(::Type{<:AbstractFunction}) -> Bool\n\nTrait checker for the smooth (differentiable) property.\n\nReturns true if the function is smooth (differentiable) everywhere in its domain, false otherwise. Smooth functions can be used in gradient-based optimization algorithms like gradient descent, Newton's method, and quasi-Newton methods.\n\nMathematical Background\n\nA function f is smooth if it is continuously differentiable, meaning:\n\nThe gradient ∇f(x) exists at every point x in the domain\nThe gradient is continuous\n\nImplementation Requirements\n\nIf isSmooth(f) = true, then the function must implement:\n\ngradientOracle!(grad, f, x): In-place gradient computation\ngradientOracle(f, x): Allocating gradient computation (has default implementation)\n\nExamples\n\n# Check if a function is smooth\nf = QuadraticFunction(Q, q, r)\nisSmooth(f)  # Returns true\n\n# Use in algorithm selection\nif isSmooth(f)\n    # Use gradient-based algorithm\n    grad = gradientOracle(f, x)\n    x_new = x - α * grad  # Gradient descent step\nelse\n    # Use derivative-free algorithm\n    x_new = proximalOracle(f, x, γ)  # If proximal is available\nend\n\nBuilt-in Smooth Functions\n\nQuadraticFunction: Gradient is linear\nAffineFunction: Gradient is constant\nComponentwiseExponentialFunction: Gradient is exponential\nZero: Gradient is zero\nFrobeniusNormSquare: Gradient is linear in matrix case\n\nNon-smooth Functions\n\nElementwiseL1Norm: Not differentiable at zero\nMost indicator functions: Not differentiable on boundaries\nMatrixNuclearNorm: Not differentiable when singular values are zero\n\n\n\n\n\nisSmooth(f::WrapperScalingTranslationFunction)\n\nCheck if the wrapped function is smooth (differentiable). Delegates to the original function's smoothness property.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.isProximal","page":"Functions","title":"PDMO.isProximal","text":"isProximal(f::AbstractFunction) -> Bool\nisProximal(::Type{<:AbstractFunction}) -> Bool\n\nTrait checker for the proximal operator capability.\n\nReturns true if the function has an implemented proximal operator, false otherwise. Functions with proximal operators can be used in proximal algorithms like ADMM, forward-backward splitting, and Douglas-Rachford splitting.\n\nMathematical Background\n\nThe proximal operator of a function f is defined as:\n\nprox_{γf}(x) = argmin_z { f(z) + (1/(2γ))||z - x||² }\n\nImplementation Requirements\n\nIf isProximal(f) = true, then the function must implement:\n\nproximalOracle!(y, f, x, γ): In-place proximal operator\nproximalOracle(f, x, γ): Allocating proximal operator (has default implementation)\n\nExamples\n\n# Check if a function has proximal operator\nf = ElementwiseL1Norm()\nisProximal(f)  # Returns true\n\n# Use in algorithm selection\nif isProximal(f)\n    # Use proximal algorithm\n    result = proximalOracle(f, x, γ)\nelse\n    # Use different algorithm approach\n    result = gradientOracle(f, x)\nend\n\nBuilt-in Proximal Functions\n\nElementwiseL1Norm: Soft thresholding\nIndicatorBox: Projection onto box constraints\nIndicatorBallL2: Projection onto L2 ball\nZero: Identity operator\nMany indicator functions: projections onto constraint sets\n\n\n\n\n\nisProximal(f::WrapperScalingTranslationFunction)\n\nCheck if the wrapped function has a proximal operator. Delegates to the original function's proximal property.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.isConvex","page":"Functions","title":"PDMO.isConvex","text":"isConvex(f::AbstractFunction) -> Bool\nisConvex(::Type{<:AbstractFunction}) -> Bool\n\nTrait checker for the convex property.\n\nReturns true if the function is convex, false otherwise. Convex functions have many desirable properties for optimization, including the guarantee that any local minimum is also a global minimum.\n\nMathematical Background\n\nA function f is convex if for all x, y in its domain and λ ∈ [0,1]:\n\nf(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\n\nProperties of Convex Functions\n\nAny local minimum is a global minimum\nThe set of global minimizers forms a convex set\nFirst-order optimality conditions are sufficient\nMany efficient optimization algorithms are guaranteed to converge\n\nExamples\n\n# Check if a function is convex\nf = ElementwiseL1Norm()\nisConvex(f)  # Returns true\n\n# Use in algorithm selection\nif isConvex(f)\n    # Use convex optimization algorithm\n    # Global optimality guarantees apply\nelse\n    # Use general nonlinear optimization\n    # Local optimality only\nend\n\nBuilt-in Convex Functions\n\nNorms: ElementwiseL1Norm, FrobeniusNormSquare\nIndicators: All indicator functions of convex sets\nBasic: Zero, AffineFunction\nQuadratic: QuadraticFunction (if positive semidefinite)\nExponential: ComponentwiseExponentialFunction\n\nNon-convex Functions\n\nSome user-defined functions\nMatrixNuclearNorm with different weights\nFunctions with non-convex constraints\n\nAlgorithm Implications\n\nConvex functions enable global convergence guarantees\nSpecialized convex optimization algorithms can be used\nDuality theory applies for convex functions\nEfficient solution methods are available\n\n\n\n\n\nisConvex(f::WrapperScalingTranslationFunction)\n\nCheck if the wrapped function is convex. Delegates to the original function's convexity property.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.isSet","page":"Functions","title":"PDMO.isSet","text":"isSet(f::AbstractFunction) -> Bool\nisSet(::Type{<:AbstractFunction}) -> Bool\n\nTrait checker for indicator functions of sets.\n\nReturns true if the function is the indicator function of a set, false otherwise. Indicator functions are fundamental in constrained optimization and represent constraints as functions that are 0 inside the constraint set and ∞ outside.\n\nMathematical Background\n\nAn indicator function of a set S is defined as:\n\nI_S(x) = 0    if x ∈ S\nI_S(x) = +∞   if x ∉ S\n\nProperties of Indicator Functions\n\nAlways convex if the underlying set is convex\nThe proximal operator is the projection onto the set\nUsed to represent constraints in optimization problems\nEnable conversion between constrained and unconstrained formulations\n\nExamples\n\n# Check if a function is an indicator function\nf = IndicatorBox([-1.0, -1.0], [1.0, 1.0])\nisSet(f)  # Returns true\n\n# Use in constraint handling\nif isSet(f)\n    # This represents a constraint\n    # Proximal operator is projection onto the set\n    projected_x = proximalOracle(f, x)\nelse\n    # This is a regular objective function\n    function_value = f(x)\nend\n\nBuilt-in Indicator Functions\n\nBox constraints: IndicatorBox\nBall constraints: IndicatorBallL2\nCone constraints: IndicatorSOC, IndicatorRotatedSOC\nMatrix constraints: IndicatorPSD\nLinear constraints: IndicatorLinearSubspace, IndicatorHyperplane\nOrthant constraints: IndicatorNonnegativeOrthant\nCustom constraints: IndicatorSumOfNVariables\n\nRelationship to Proximal Operators\n\nFor indicator functions, the proximal operator is the projection:\n\nprox_{γI_S}(x) = Proj_S(x) = argmin_{y∈S} ||y - x||²\n\nAlgorithm Applications\n\nConstrained optimization: Represent feasible regions\nProjection methods: Direct projection onto constraint sets\nPenalty methods: Soft constraint handling\nADMM: Splitting methods for constrained problems\n\n\n\n\n\nisSet(f::WrapperScalingTranslationFunction)\n\nCheck if the wrapped function is an indicator function of a set. Delegates to the original function's set property.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#First-Order-Oracles","page":"Functions","title":"First Order Oracles","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.proximalOracle","page":"Functions","title":"PDMO.proximalOracle","text":"proximalOracle(f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false) -> NumericVariable\n\nComputes the proximal operator for function f, returning a new array.\n\nThis is the allocating version of the proximal operator that creates and returns a new array containing the result. For performance-critical applications, consider using the in-place version proximalOracle! instead.\n\nArguments\n\nf::AbstractFunction: The function for which to compute the proximal operator\nx::NumericVariable: The input point\ngamma::Float64=1.0: The proximal parameter γ > 0\nenableParallel::Bool=false: Whether to enable parallel computation\n\nReturns\n\nNumericVariable: The result prox_{γf}(x), same type and size as input x\n\nMathematical Definition\n\nComputes: prox{γf}(x) = argminz { f(z) + (1/(2γ))||z - x||² }\n\nImplementation Notes\n\nThe default implementation:\n\nAllocates a new array y = similar(x)\nCalls proximalOracle!(y, f, x, gamma, enableParallel)\nReturns the result\n\nConcrete function types may override this method for more efficient implementations, but most can rely on the default implementation.\n\nExamples\n\n# Basic usage - returns new array\nf = ElementwiseL1Norm(0.5)\nx = [2.0, -3.0, 0.3]\nresult = proximalOracle(f, x, 1.0)  # Returns [1.5, -2.5, 0.0]\n\n# Chaining operations\nf1 = IndicatorBox([-1.0, -1.0], [1.0, 1.0])\nf2 = ElementwiseL1Norm(0.1)\nx = [2.0, -2.0]\nresult = proximalOracle(f2, proximalOracle(f1, x))  # Compose operations\n\n# Different input types\nf_matrix = IndicatorPSD(3)\nX = rand(3, 3)\nY = proximalOracle(f_matrix, X)  # Returns projected matrix\n\n# Scalar inputs (for appropriate functions)\nf_scalar = ElementwiseL1Norm(1.0)\nx_scalar = 2.0\nresult_scalar = proximalOracle(f_scalar, x_scalar)  # Returns 1.0\n\nPerformance Considerations\n\nMemory allocation: Creates new array on each call\nPrefer in-place version: Use proximalOracle! for better performance\nTemporary arrays: Consider pre-allocating arrays for repeated calls\nMemory pressure: May cause garbage collection in tight loops\n\nAlgorithm Applications\n\nProximal gradient methods: Forward-backward splitting\nADMM: Alternating direction method of multipliers\nDouglas-Rachford: Splitting methods\nPrimal-dual methods: Condat-Vu, Chambolle-Pock algorithms\n\nCommon Use Cases\n\nConstraint projection: Project onto feasible sets\nRegularization: Apply regularization operators\nDenoising: Soft thresholding for sparse signals\nMatrix completion: Project onto low-rank or PSD constraints\n\n\n\n\n\nproximalOracle(f::IndicatorPSD, \n              x::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, \n              gamma::Float64 = 1.0, \n              enableParallel::Bool=false)\n\nNon-mutating version of the proximal operator. Returns same type as input (sparse or dense).\n\n\n\n\n\nproximalOracle(f::IndicatorSumOfNVariables, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false) -> NumericVariable\n\nComputes and returns the proximal operator (projection) of the indicator function, i.e. the projection of x onto the set\n\nx₁ + x₂ + … + xₙ = rhs.\n\n\n\n\n\nNon-mutating version of the proximal operator\n\n\n\n\n\nproximalOracle(f::WrapperScalingTranslationFunction, x::NumericVariable, gamma::Float64, enableParallel::Bool=false)\n\nCompute the proximal operator of f(x) = g(coe·x + translation).\n\nReturns: prox{γf}(x) = (prox{γ·coe², g}(translation + coe·x) - translation) / coe\n\nArguments\n\nf::WrapperScalingTranslationFunction: The wrapped function\nx::NumericVariable: Input point for the proximal operator\ngamma::Float64: Proximal parameter\nenableParallel::Bool=false: Whether to enable parallel computation\n\nReturns\n\nNumericVariable: Result of the proximal operator\n\nExamples\n\n# Proximal operator of scaled L2 ball: f(x) = I_{||2x||₂ ≤ 1}(x)\ng = IndicatorBallL2(1.0)\nf = WrapperScalingTranslationFunction(g, 2.0, 0.0)\nresult = proximalOracle(f, [1.0, 1.0], 1.0)  # Projects onto scaled ball\n\nThrows\n\nErrorException: If original function doesn't have proximal operator\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.proximalOracle!","page":"Functions","title":"PDMO.proximalOracle!","text":"proximalOracle!(y::NumericVariable, f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)\n\nIn-place computation of the proximal operator for function f.\n\nThe proximal operator is a fundamental concept in convex optimization and is defined as:\n\nprox_{γf}(x) = argmin_z { f(z) + (1/(2γ))||z - x||² }\n\nThis function computes the result in-place, storing it in the pre-allocated output y.\n\nArguments\n\ny::NumericVariable: Pre-allocated output buffer (modified in-place)\nf::AbstractFunction: The function for which to compute the proximal operator\nx::NumericVariable: The input point\ngamma::Float64=1.0: The proximal parameter γ > 0\nenableParallel::Bool=false: Whether to enable parallel computation\n\nMathematical Background\n\nThe proximal operator generalizes the concept of projection onto a set:\n\nFor indicator functions: prox{γIS}(x) = Proj_S(x) (projection onto set S)\nFor L1 norm: prox{γ||·||₁}(x) = softthreshold(x, γ) (soft thresholding)\nFor quadratic functions: Has explicit closed-form solution\n\nImplementation Requirements\n\nFunctions with isProximal(f) = true must implement this method. The implementation should:\n\nValidate input dimensions and parameter values\nCompute the proximal operator efficiently\nStore the result in the pre-allocated output y\nHandle edge cases and numerical stability\nOptionally support parallel computation\n\nExamples\n\n# Basic usage with indicator function\nf = IndicatorBox([-1.0, -1.0], [1.0, 1.0])\nx = [2.0, -2.0]\ny = similar(x)\nproximalOracle!(y, f, x, 1.0)  # Projects onto box, y = [1.0, -1.0]\n\n# L1 norm with soft thresholding\nf = ElementwiseL1Norm(0.5)\nx = [2.0, -3.0, 0.3]\ny = similar(x)\nproximalOracle!(y, f, x, 1.0)  # y = [1.5, -2.5, 0.0]\n\n# Matrix functions\nf = IndicatorPSD(3)\nX = rand(3, 3)\nY = similar(X)\nproximalOracle!(Y, f, X, 1.0)  # Projects onto PSD cone\n\nPerformance Considerations\n\nPre-allocate output buffer to avoid memory allocation\nUse in-place operations within the implementation\nConsider cache efficiency for large arrays\nParallel computation can be beneficial for large-scale problems\n\nCommon Proximal Operators\n\nIndicator functions: Projection onto constraint sets\nL1 norm: Soft thresholding operator\nL2 ball: Projection onto ball (scaling if outside)\nZero function: Identity operator\nQuadratic functions: Require solving linear systems\n\nError Handling\n\nInput validation for dimensions and parameter ranges\nNumerical stability considerations\nAppropriate handling of edge cases (zero gamma, boundary cases)\n\n\n\n\n\nproximalOracle!(y::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, \n                f::IndicatorPSD, \n                x::Union{Matrix{Float64}, SparseMatrixCSC{Float64}}, \n                gamma::Float64 = 1.0, \n                enableParallel::Bool=false)\n\nProject onto the PSD cone. Works with both dense and sparse matrices. The projection is performed by:\n\nSymmetrizing the matrix\nComputing eigendecomposition\nSetting negative eigenvalues to zero\nReconstructing the matrix\n\n\n\n\n\nproximalOracle!(y::NumericVariable, f::IndicatorSumOfNVariables, x::NumericVariable, gamma::Float64=1.0, enableParallel::Bool=false)\n\nComputes the proximal operator (i.e. the projection) of the indicator function in-place, storing the result in y.\n\nFor the scalar case (when rhs is a Number), it is assumed that x is a vector of length numberVariables, and the projection onto { x : sum(x) = rhs } subtracts the uniform shift\n\nshift = (sum(x) - rhs) / numberVariables\n\nfrom each entry. For the non-scalar case, x is assumed to be an array whose first dimension is size(rhs, 1) * numberVariables and remaining dimensions match rhs. The function computes the elementwise residual\n\nres = (sum of blocks) - rhs\n\nand then computes\n\nshift = res / numberVariables\n\nwhich is subtracted from every block. No additional dimension checking is performed for performance reasons.\n\n\n\n\n\nProximal operator for weighted nuclear norm. The solution is given by U * diag(max(0, σᵢ - γbᵢ)) * Vᵀ where U, σᵢ, V come from the SVD of x.\n\n\n\n\n\nproximalOracle!(y::NumericVariable, f::WrapperScalingTranslationFunction, x::NumericVariable, gamma::Float64, enableParallel::Bool=false)\n\nIn-place computation of the proximal operator of f(x) = g(coe·x + translation).\n\nMathematical Background\n\nFor f(x) = g(coe·x + translation), the proximal operator is: prox{γf}(z) = argminx { g(coe·x + translation) + (1/(2γ))||x - z||² }\n\nThrough variable substitution u = coe·x + translation, this becomes: prox{γf}(z) = (prox{γ·coe², g}(translation + coe·z) - translation) / coe\n\nAlgorithm\n\nTransform input: buffer = coe·x + translation\nApply original proximal: prox_{γ·coe², g}(buffer)\nTransform back: (result - translation) / coe\n\nArguments\n\ny::NumericVariable: Output buffer for the result (modified in-place)\nf::WrapperScalingTranslationFunction: The wrapped function\nx::NumericVariable: Input point for the proximal operator\ngamma::Float64: Proximal parameter\nenableParallel::Bool=false: Whether to enable parallel computation\n\nThrows\n\nErrorException: If original function doesn't have proximal operator, or dimension mismatches occur\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.gradientOracle","page":"Functions","title":"PDMO.gradientOracle","text":"gradientOracle(f::AbstractFunction, x::NumericVariable, enableParallel::Bool=false) -> NumericVariable\n\nComputes the gradient of function f at point x, returning a new array.\n\nThis is the allocating version of the gradient computation that creates and returns a new array containing the gradient. For performance-critical applications, consider using the in-place version gradientOracle! instead.\n\nArguments\n\nf::AbstractFunction: The function for which to compute the gradient\nx::NumericVariable: The point at which to evaluate the gradient\nenableParallel::Bool=false: Whether to enable parallel computation\n\nReturns\n\nNumericVariable: The gradient ∇f(x), same type and size as input x\n\nMathematical Definition\n\nFor smooth functions: ∇f(x) where each component is ∂f/∂xᵢ For vector functions: Returns the Jacobian or gradient matrix\n\nImplementation Notes\n\nThe default implementation:\n\nAllocates a new array grad = similar(x)\nCalls gradientOracle!(grad, f, x, enableParallel)\nReturns the result\n\nConcrete function types may override this method for more efficient implementations, but most can rely on the default implementation.\n\nExamples\n\n# Basic usage with quadratic function\nQ = sparse([2.0 0.0; 0.0 2.0])\nq = [1.0, 1.0]\nf = QuadraticFunction(Q, q, 0.0)\nx = [1.0, 2.0]\ngrad = gradientOracle(f, x)  # Returns [3.0, 5.0]\n\n# Gradient descent step\nf = QuadraticFunction(Q, q, 0.0)\nx = [1.0, 2.0]\nα = 0.1  # Step size\ngrad = gradientOracle(f, x)\nx_new = x - α * grad  # Gradient descent update\n\n# Matrix function gradients\nA = rand(10, 5)\nb = rand(10, 3)\nf = FrobeniusNormSquare(A, b, 5, 3)\nX = rand(5, 3)\ngrad_X = gradientOracle(f, X)  # Returns 5×3 gradient matrix\n\n# Chaining with function evaluation\nf = ComponentwiseExponentialFunction([1.0, 2.0])\nx = [0.0, 1.0]\nval = f(x)  # Function value\ngrad = gradientOracle(f, x)  # Gradient at same point\n\nPerformance Considerations\n\nMemory allocation: Creates new array on each call\nPrefer in-place version: Use gradientOracle! for better performance\nTemporary arrays: Consider pre-allocating arrays for repeated calls\nMemory pressure: May cause garbage collection in tight loops\nAutomatic differentiation: Consider AD tools for complex functions\n\nAlgorithm Applications\n\nSteepest descent: x{k+1} = xk - α∇f(x_k)\nMomentum methods: Incorporate previous gradient information\nAdam optimizer: Adaptive gradient methods\nLine search: Determine optimal step sizes\nQuasi-Newton: Approximate Hessian from gradients\n\nCommon Use Cases\n\nOptimization: First-order optimization algorithms\nMachine learning: Backpropagation and gradient-based training\nSensitivity analysis: Study function behavior\nRoot finding: Newton's method for systems of equations\nNumerical integration: Gradient-based quadrature\n\nNumerical Considerations\n\nGradient magnitude: Large gradients may indicate poor scaling\nNumerical derivatives: Consider finite difference approximations\nAutomatic differentiation: Tools like ForwardDiff.jl or ReverseDiff.jl\nCondition number: Well-conditioned problems have stable gradients\n\nError Handling\n\nFunctions must be smooth (isSmooth(f) = true)\nInput validation for dimensions and types\nHandling of numerical issues (NaN, Inf)\nMeaningful error messages for debugging\n\n\n\n\n\ngradientOracle(f::WrapperScalingTranslationFunction, x::NumericVariable, enableParallel::Bool=false)\n\nCompute the gradient ∇f(x) = coe · ∇g(coe·x + translation).\n\nArguments\n\nf::WrapperScalingTranslationFunction: The wrapped function\nx::NumericVariable: Point at which to evaluate the gradient\nenableParallel::Bool=false: Whether to enable parallel computation\n\nReturns\n\nNumericVariable: The gradient vector/matrix\n\nThrows\n\nErrorException: If original function is not smooth\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.gradientOracle!","page":"Functions","title":"PDMO.gradientOracle!","text":"gradientOracle!(grad::NumericVariable, f::AbstractFunction, x::NumericVariable, enableParallel::Bool=false)\n\nIn-place computation of the gradient of function f at point x.\n\nThis function computes the gradient (or subgradient for non-smooth functions) and stores the result in the pre-allocated output buffer grad.\n\nArguments\n\ngrad::NumericVariable: Pre-allocated output buffer for gradient (modified in-place)\nf::AbstractFunction: The function for which to compute the gradient\nx::NumericVariable: The point at which to evaluate the gradient\nenableParallel::Bool=false: Whether to enable parallel computation\n\nMathematical Background\n\nFor smooth functions, the gradient is defined as:\n\n∇f(x) = lim_{h→0} [f(x+h) - f(x)] / h\n\nFor vector/matrix functions, this becomes the vector/matrix of partial derivatives.\n\nImplementation Requirements\n\nFunctions with isSmooth(f) = true must implement this method. The implementation should:\n\nValidate input dimensions match between x and grad\nCompute the gradient efficiently\nStore the result in the pre-allocated output grad\nHandle numerical stability issues\nOptionally support parallel computation\n\nExamples\n\n# Basic usage with quadratic function\nQ = [2.0 0.0; 0.0 2.0]\nq = [1.0, 1.0]\nf = QuadraticFunction(sparse(Q), q, 0.0)\nx = [1.0, 2.0]\ngrad = similar(x)\ngradientOracle!(grad, f, x)  # grad = Q*x + q = [3.0, 5.0]\n\n# Affine function (constant gradient)\nA = [2.0, 3.0]\nf = AffineFunction(A, 0.0)\nx = [1.0, 1.0]\ngrad = similar(x)\ngradientOracle!(grad, f, x)  # grad = A = [2.0, 3.0]\n\n# Matrix function\nA = rand(10, 5)\nb = rand(10, 3)\nf = FrobeniusNormSquare(A, b, 5, 3)\nX = rand(5, 3)\ngrad = similar(X)\ngradientOracle!(grad, f, X)  # grad = 2*A'*(A*X - b)\n\nPerformance Considerations\n\nMemory efficiency: Uses pre-allocated buffer to avoid allocations\nIn-place operations: Implementation should minimize temporary arrays\nVectorization: Take advantage of BLAS/LAPACK when possible\nParallel computation: Can be beneficial for large-scale problems\nCache efficiency: Consider memory access patterns\n\nCommon Gradient Patterns\n\nLinear functions: Gradient is constant (independent of x)\nQuadratic functions: Gradient is linear in x\nLeast squares: Gradient involves matrix-vector products\nExponential functions: Gradient involves exponential evaluations\nComposite functions: Use chain rule\n\nNumerical Considerations\n\nFinite precision: Be aware of numerical errors\nScaling: Consider function scaling for numerical stability\nCondition numbers: Well-conditioned problems have more stable gradients\nOverflow/underflow: Handle extreme values appropriately\n\nAlgorithm Applications\n\nGradient descent: Steepest descent optimization\nNewton's method: Second-order optimization\nQuasi-Newton methods: BFGS, L-BFGS\nConjugate gradient: Iterative linear system solvers\nTrust region methods: Model-based optimization\n\nError Handling\n\nValidate that x and grad have compatible dimensions\nCheck for numerical issues (NaN, Inf)\nProvide meaningful error messages for dimension mismatches\nHandle edge cases gracefully\n\n\n\n\n\ngradientOracle!(y::NumericVariable, f::WrapperScalingTranslationFunction, x::NumericVariable, enableParallel::Bool=false)\n\nIn-place computation of the gradient ∇f(x) = coe · ∇g(coe·x + translation).\n\nMathematical Background\n\nFor f(x) = g(coe·x + translation), the chain rule gives: ∇f(x) = ∇g(coe·x + translation) · coe\n\nArguments\n\ny::NumericVariable: Output buffer for the gradient (modified in-place)\nf::WrapperScalingTranslationFunction: The wrapped function\nx::NumericVariable: Point at which to evaluate the gradient\nenableParallel::Bool=false: Whether to enable parallel computation\n\nThrows\n\nErrorException: If original function is not smooth, or dimension mismatches occur\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.proximalOracleOfConjugate","page":"Functions","title":"PDMO.proximalOracleOfConjugate","text":"proximalOracleOfConjugate(f::AbstractFunction, x::NumericVariable, gamma::Float64=1.0, \n                         enableParallel::Bool=false) -> NumericVariable\n\nCompute the proximal operator of the convex conjugate f* using Moreau's identity.\n\nThis function returns prox{γf}(x) where f is the convex conjugate of f. It uses the Moreau identity: prox{γf*}(x) = x - γ * prox_{f/γ}(x/γ).\n\nArguments\n\nf::AbstractFunction: The function whose conjugate proximal operator to compute\nx::NumericVariable: Input point\ngamma::Float64=1.0: Proximal parameter (must be positive)  \nenableParallel::Bool=false: Whether to enable parallel computation\n\nReturns\n\nNumericVariable: Result prox_{γf*}(x), same type and size as input x\n\nMathematical Background\n\nThe proximal operator of the convex conjugate is fundamental in convex optimization:\n\nFor f*(y) = sup_x [⟨x,y⟩ - f(x)], the conjugate function\nprox{γf*}(x) = argminy [½||y-x||² + γf*(y)]\n\nThe Moreau identity provides an efficient way to compute this without explicitly constructing the conjugate function, requiring only the proximal operator of f.\n\nErrors\n\nThrows error if f does not have a proximal oracle\nThrows error if gamma ≤ 0\n\nApplications\n\nDual variable updates in primal-dual algorithms\nConstraint handling where f is an indicator function\nRegularization in optimization problems\nImage processing and signal processing applications\n\nExamples\n\n# For L1 norm f(x) = ||x||₁\n# The conjugate f*(y) = indicator of ||y||∞ ≤ 1\ny = proximalOracleOfConjugate(l1_norm, x, gamma)  # Projects onto ℓ∞ ball\n\n# For indicator of convex set f = δ_C\n# The conjugate f*(y) = σ_C(y) (support function)\ny = proximalOracleOfConjugate(indicator_C, x, gamma)  # Scaled projection\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.proximalOracleOfConjugate!","page":"Functions","title":"PDMO.proximalOracleOfConjugate!","text":"proximalOracleOfConjugate!(y::NumericVariable, f::AbstractFunction, x::NumericVariable, \n                          gamma::Float64=1.0, enableParallel::Bool=false)\n\nIn-place computation of the proximal operator of the convex conjugate f* using Moreau's identity.\n\nThis function computes y = prox{γf}(x) where f is the convex conjugate of f. It uses the Moreau identity: prox{γf*}(x) = x - γ * prox_{f/γ}(x/γ).\n\nArguments\n\ny::NumericVariable: Output variable (modified in-place)\nf::AbstractFunction: The function whose conjugate proximal operator to compute\nx::NumericVariable: Input point\ngamma::Float64=1.0: Proximal parameter (must be positive)\nenableParallel::Bool=false: Whether to enable parallel computation\n\nEffects\n\nModifies y in-place to contain prox_{γf*}(x)\nRequires f to have a proximal oracle implementation\n\nMathematical Background\n\nThe Moreau identity relates the proximal operators of a function and its conjugate:\n\nprox{γf*}(x) + γ * prox{f/γ}(x/γ) = x\n\nThis identity allows computing the proximal operator of f* when only the proximal operator of f is available, which is common in primal-dual algorithms.\n\nErrors\n\nThrows error if f does not have a proximal oracle\nThrows error if x and y have different sizes\nThrows error if gamma ≤ 0\nThrows error for scalar inputs (use scalar version of proximal operators)\n\nApplications\n\nPrimal-dual splitting algorithms (ADMM, Condat-Vu)\nComputing dual proximal steps when only primal proximal is available\nImplementing algorithms that require conjugate function evaluations\n\nExample\n\n# For constraint f = indicator of convex set C\n# prox_{γf*}(x) computes projection onto γC\nproximalOracleOfConjugate!(y, f, x, gamma)\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#PDMO.estimateLipschitzConstant","page":"Functions","title":"PDMO.estimateLipschitzConstant","text":"estimateLipschitzConstant(f::AbstractFunction, x::NumericVariable; maxTrials::Int=50, \n                         minStepSize::Float64=1e-6, maxStepSize::Float64=1.0) -> Float64\n\nEstimate the Lipschitz constant of the gradient ∇f at point x using multiple sampling strategies.\n\nThis function provides robust estimation of the Lipschitz constant L such that  ||∇f(x) - ∇f(y)|| ≤ L||x - y|| for points near x. It handles special cases with exact solutions and uses multiple numerical strategies for general functions.\n\nArguments\n\nf::AbstractFunction: The function for which to estimate the Lipschitz constant\nx::NumericVariable: The point around which to estimate the constant\nmaxTrials::Int=50: Maximum number of sampling trials for estimation\nminStepSize::Float64=1e-6: Minimum step size for finite differences\nmaxStepSize::Float64=1.0: Maximum step size for finite differences\n\nReturns\n\nFloat64: Estimated Lipschitz constant (conservative upper bound)\n\nAlgorithm\n\nThe function uses a multi-strategy approach:\n\nExact Cases\n\nQuadraticFunction: Returns 2||Q|| where Q is the Hessian matrix\nAffineFunction/Zero: Returns 0 (constant gradient)\n\nNumerical Estimation (3 strategies)\n\nRandom Directions: Sample random unit directions with multiple scales\nCoordinate Directions: Test axis-aligned perturbations (for problems ≤ 1000 dimensions)  \nAdaptive Scaling: Use gradient magnitude to inform step size selection\n\nStatistical Robustness\n\nCollects estimates from all strategies\nRemoves outliers using percentile-based filtering\nReturns conservative estimate (90th percentile or 1.2× median)\nHandles edge cases with appropriate fallbacks\n\nExamples\n\n# For a quadratic function f(x) = 0.5x'Qx\nL = estimateLipschitzConstant(f, x0)  # Returns 2*opnorm(Q)\n\n# For a general smooth function\nL = estimateLipschitzConstant(f, x0, maxTrials=100)  # More thorough sampling\n\nNotes\n\nThe estimate is intentionally conservative to ensure algorithm stability\nFor high-dimensional problems, coordinate sampling is limited for efficiency\nMultiple strategies provide robustness across different function types\nHandles both scalar and vector inputs appropriately\n\n\n\n\n\n","category":"function"},{"location":"S4_api/functions/#Functions-implemented-in-PDMO.jl","page":"Functions","title":"Functions implemented in PDMO.jl","text":"","category":"section"},{"location":"S4_api/functions/#Basic-Functions","page":"Functions","title":"Basic Functions","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.Zero","page":"Functions","title":"PDMO.Zero","text":"Zero()\n\nRepresents the zero function f(x) = 0 for all x.\n\nMathematical Definition\n\nf(x) = 0 for all x ∈ ℝⁿ\n\nProperties\n\nSmooth: Yes, constant functions are infinitely differentiable\nConvex: Yes, constant functions are convex\nProximal: Yes, has explicit proximal operator\n\nMathematical Properties\n\nGradient: ∇f(x) = 0 (zero vector/scalar)\nProximal Operator: prox_γf(x) = x (identity function)\n\nExamples\n\n# Zero function\nf = Zero()\nx = [1.0, 2.0, 3.0]\nval = f(x)  # Returns 0.0\n\n# Gradient is always zero\ngrad = gradientOracle(f, x)  # Returns [0.0, 0.0, 0.0]\n\n# Proximal operator is identity\nprox_x = proximalOracle(f, x)  # Returns [1.0, 2.0, 3.0]\n\nApplications\n\nNeutral elements in optimization\nBaseline functions in algorithms\nSimplified formulations\nAlgorithm testing and verification\nInitialization in iterative methods\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.AffineFunction","page":"Functions","title":"PDMO.AffineFunction","text":"AffineFunction(A::NumericVariable, r::Float64=0.0)\n\nRepresents an affine function of the form f(x) = ⟨A, x⟩ + r, where A is a coefficient vector/matrix and r is a scalar offset.\n\nMathematical Definition\n\nFor vector input x: f(x) = A'x + r  \nFor scalar input x: f(x) = A*x + r\n\nArguments\n\nA::NumericVariable: Coefficient vector/matrix/scalar\nr::Float64=0.0: Scalar offset term\n\nProperties\n\nSmooth: Yes, gradient is constant\nConvex: Yes, affine functions are convex\nProximal: Yes, has explicit proximal operator\n\nMathematical Properties\n\nGradient: ∇f(x) = A (constant)\nProximal Operator: prox_γf(x) = x - γA\n\nExamples\n\n# Linear function f(x) = 2x₁ + 3x₂ + 1\nA = [2.0, 3.0]\nr = 1.0\nf = AffineFunction(A, r)\nx = [1.0, 2.0]\nval = f(x)  # Returns 2*1 + 3*2 + 1 = 9\n\n# Scalar function f(x) = 5x + 2\nf = AffineFunction(5.0, 2.0)\nval = f(3.0)  # Returns 5*3 + 2 = 17\n\nApplications\n\nLinear constraints in optimization\nObjective functions in linear programming\nPenalty terms in regularization\nBuilding blocks for more complex functions\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.QuadraticFunction","page":"Functions","title":"PDMO.QuadraticFunction","text":"QuadraticFunction(Q::SparseMatrixCSC{Float64, Int64}, q::Vector{Float64}, r::Float64)\n\nRepresents a quadratic function of the form f(x) = (1/2)x'Qx + q'x + r.\n\nMathematical Definition\n\nf(x) = (1/2)x'Qx + q'x + r\n\nwhere:\n\nQ is a symmetric matrix (Hessian)\nq is a linear term vector\nr is a scalar offset\n\nArguments\n\nQ::SparseMatrixCSC{Float64, Int64}: Quadratic coefficient matrix (should be symmetric)\nq::Vector{Float64}: Linear coefficient vector\nr::Float64: Scalar offset term\n\nConstructors\n\nQuadraticFunction(Q, q, r): Full specification\nQuadraticFunction(n::Int64): Zero quadratic function of dimension n\n\nProperties\n\nSmooth: Yes, quadratic functions are infinitely differentiable\nConvex: Yes, if Q is positive semidefinite\nProximal: No, proximal operator not implemented (requires solving linear system)\n\nMathematical Properties\n\nGradient: ∇f(x) = Qx + Q'x + q = (Q + Q')x + q\nHessian: ∇²f(x) = Q + Q'\n\nExamples\n\n# Quadratic function f(x) = x₁² + x₂² + x₁ + 2x₂ + 3\nQ = sparse([1.0 0.0; 0.0 1.0])  # Identity matrix\nq = [1.0, 2.0]\nr = 3.0\nf = QuadraticFunction(Q, q, r)\nx = [1.0, 1.0]\nval = f(x)  # Returns 1 + 1 + 1 + 2 + 3 = 8\n\n# Zero quadratic function\nf = QuadraticFunction(2)  # f(x) = 0 for x ∈ ℝ²\n\nApplications\n\nQuadratic programming\nLeast squares problems\nTrust region methods\nNewton's method approximations\nModel predictive control\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#Norm-Functions","page":"Functions","title":"Norm Functions","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.ElementwiseL1Norm","page":"Functions","title":"PDMO.ElementwiseL1Norm","text":"ElementwiseL1Norm(coefficient::Float64=1.0)\n\nRepresents the element-wise L1 norm function f(x) = coefficient * ||x||₁.\n\nMathematical Definition\n\nf(x) = coefficient * ∑ᵢ |xᵢ|\n\nwhere ||x||₁ is the L1 norm (sum of absolute values).\n\nArguments\n\ncoefficient::Float64=1.0: Positive scaling coefficient\n\nProperties\n\nSmooth: No, not differentiable at zero\nConvex: Yes, L1 norm is convex\nProximal: Yes, has explicit proximal operator (soft thresholding)\n\nMathematical Properties\n\nSubdifferential: ∂f(x) = coefficient * sign(x) (element-wise)\nProximal Operator: Soft thresholding operator\nprox_γf(x)ᵢ = sign(xᵢ) * max(0, |xᵢ| - γ*coefficient)\n\nExamples\n\n# Standard L1 norm\nf = ElementwiseL1Norm()\nx = [1.0, -2.0, 3.0]\nval = f(x)  # Returns |1| + |-2| + |3| = 6\n\n# Scaled L1 norm with coefficient 0.5\nf = ElementwiseL1Norm(0.5)\nx = [4.0, -6.0]\nval = f(x)  # Returns 0.5 * (4 + 6) = 5\n\n# Proximal operator (soft thresholding)\nf = ElementwiseL1Norm(1.0)\nx = [2.0, -3.0, 0.5]\nprox_x = proximalOracle(f, x, 1.0)  # γ = 1.0\n# Returns [1.0, -2.0, 0.0] (soft thresholding with threshold 1.0)\n\nApplications\n\nSparse regression (LASSO)\nCompressed sensing\nFeature selection\nRegularization in machine learning\nSignal denoising\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.FrobeniusNormSquare","page":"Functions","title":"PDMO.FrobeniusNormSquare","text":"FrobeniusNormSquare(A::Matrix{Float64}, b::Union{Matrix{Float64}, Vector{Float64}}, numberRows::Int64, numberColumns::Int64, coe::Float64=0.5)\n\nRepresents the squared Frobenius norm of the residual AX - b.\n\nMathematical Definition\n\nf(X) = coe * ||AX - b||²_F\n\nwhere:\n\nA is an L × m coefficient matrix\nX is an m × n variable matrix (or vector if n=1)\nb is an L × n target matrix (or L-dimensional vector if n=1)\n||·||F denotes the Frobenius norm: ||M||²F = ∑ᵢⱼ M²ᵢⱼ = trace(M'M)\n\nArguments\n\nA::Matrix{Float64}: Coefficient matrix (L × m)\nb::Union{Matrix{Float64}, Vector{Float64}}: Target matrix (L × n) or vector (L)\nnumberRows::Int64: Number of rows in X (m)\nnumberColumns::Int64: Number of columns in X (n), set to 1 if b is a vector\ncoe::Float64=0.5: Positive scaling coefficient\n\nProperties\n\nSmooth: Yes, quadratic functions are infinitely differentiable\nConvex: Yes, squared Frobenius norm is convex\nProximal: Yes, has explicit proximal operator via linear system solution\n\nMathematical Properties\n\nGradient: ∇f(X) = 2 * coe * A'(AX - b)\nProximal Operator: Solution to (I + 2γcoeA'A)Y = X + 2γcoeA'b\n\nInternal Structure\n\nThe struct pre-computes and caches:\n\nATransA = A'A: Gram matrix for efficient repeated computations\nFactorization of the proximal system matrix for fast linear solves\nBuffer arrays to minimize memory allocations\n\nImplementation Details\n\nAutomatic problem detection: Handles both vector (n=1) and matrix (n>1) cases\nEfficient factorization: Uses Cholesky when possible, falls back to LU\nFactorization caching: Reuses factorization when γ parameter doesn't change\nMemory management: Pre-allocated buffers for all intermediate computations\n\nExamples\n\n# Least squares problem: minimize ||Ax - b||²\nA = rand(10, 5)  # 10 constraints, 5 variables\nb = rand(10)     # Target vector\nf = FrobeniusNormSquare(A, b, 5, 1, 0.5)  # Note: coe=0.5 gives (1/2)||Ax-b||²\nx = rand(5)\nval = f(x)  # Function value\ngrad = gradientOracle(f, x)  # Gradient: A'(Ax - b)\n\n# Matrix least squares: minimize ||AX - B||²_F\nA = rand(10, 5)    # Linear operator\nB = rand(10, 3)    # Target matrix\nf = FrobeniusNormSquare(A, B, 5, 3, 1.0)\nX = rand(5, 3)\nval = f(X)  # Function value\ngrad = gradientOracle(f, X)  # Gradient matrix\n\n# Proximal operator (useful in optimization algorithms)\nf = FrobeniusNormSquare(A, b, 5, 1, 1.0)\nx_current = rand(5)\nγ = 0.1  # Proximal parameter\nx_prox = proximalOracle(f, x_current, γ)  # Proximal step\n\nAlgorithm Applications\n\nLeast squares regression: Direct formulation of ||Ax - b||²\nRidge regression: Add L2 regularization\nMatrix completion: Frobenius norm data fitting term\nImage denoising: Data fidelity term in variational methods\nSystem identification: Parameter estimation problems\nProximal gradient methods: Smooth term in composite optimization\nADMM: Quadratic penalty terms\n\nOptimization Context\n\nThis function commonly appears in:\n\n# Composite optimization problem\nminimize f(x) + g(x)\n# where f(x) = (1/2)||Ax - b||² (smooth)\n# and g(x) is some regularizer (possibly non-smooth)\n\nAlgorithms like proximal gradient descent alternate between:\n\nGradient step on f: x̃ = x - α∇f(x)\nProximal step on g: x⁺ = prox_g(x̃)\n\nPerformance Characteristics\n\nFunction evaluation: O(mn + Ln) operations\nGradient computation: O(mn + Ln) operations  \nProximal operator: O(m³) for factorization + O(m²n) for solve\nMemory usage: O(m²) for factorization + O(mn + Ln) for buffers\n\nNumerical Considerations\n\nCondition number: Well-conditioned when A'A is well-conditioned\nFactorization choice: Cholesky (faster) vs LU (more robust)\nScaling: Consider rescaling A and b for numerical stability\nCaching: Factorization is cached and reused when γ doesn't change\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.MatrixNuclearNorm","page":"Functions","title":"PDMO.MatrixNuclearNorm","text":"MatrixNuclearNorm(b, rows, cols)\n\nWeighted nuclear norm of a matrix with component-wise weights, defined as:     ||W||_{b,*} = ∑ᵢ bᵢσᵢ(W)\n\nwhere σᵢ(W) are the singular values of W, b ∈ ℝᵐ₊ is a vector of positive weights  (m = min(rows,cols)), and the sum goes up to the number of singular values.\n\nArguments\n\nb::Vector{Float64}: Vector of positive weights, one per singular value\nrows::Int64: Number of rows in the matrix\ncols::Int64: Number of columns in the matrix\n\nProperties\n\nProximal: Yes\nProximal Operator: Component-wise soft thresholding of singular values\nConvex: No. Only when all entries of b are equal\n\nExample\n\n# For a 3×2 matrix, we need 2 weights (min(3,2) = 2)\nb = [1.0, 2.0]  # Different weights for different singular values\nf = MatrixNuclearNorm(b, 3, 2)\nX = [1.0 2.0; 3.0 4.0; 5.0 6.0]\nval = f(X)  # Computes weighted nuclear norm\n\nNote: The number of weights in b must equal min(rows, cols), as this is the  maximum possible number of non-zero singular values.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.WeightedMatrixL1Norm","page":"Functions","title":"PDMO.WeightedMatrixL1Norm","text":"WeightedMatrixL1Norm(A, inNonnegativeOrthant=false)\n\nWeighted L1 norm of a matrix, defined as:     ||A ⊙ x||₁ = ∑ᵢⱼ A{i,j}|x{i,j}|\n\nWhen inNonnegativeOrthant is true, the function becomes:     ||A ⊙ x||₁ + indicator(x ≥ 0)\n\nwhere A is a sparse matrix of non-negative weights and ⊙ denotes element-wise multiplication.\n\nArguments\n\nA::SparseMatrixCSC{Float64, Int64}: Matrix of non-negative weights\ninNonnegativeOrthant::Bool=false: If true, restricts the domain to the non-negative orthant\n\nProperties\n\nConvex: Yes\nProximal: Yes\nProximal Operator: \nIf inNonnegativeOrthant=false: Element-wise soft thresholding with weights A\nIf inNonnegativeOrthant=true: Project onto non-negative orthant after soft thresholding with weights A\n\nExample\n\nA = sparse([1.0 2.0; 3.0 4.0])\nf = WeightedMatrixL1Norm(A)            # Standard weighted L1 norm\ng = WeightedMatrixL1Norm(A, true)      # Weighted L1 norm restricted to non-negative orthant\nx = [1.0 -1.0; 2.0 -2.0]\nval = f(x)  # Computes weighted L1 norm\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#Indicator-Functions","page":"Functions","title":"Indicator Functions","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.IndicatorBallL2","page":"Functions","title":"PDMO.IndicatorBallL2","text":"IndicatorBallL2(r::Float64)\n\nIndicator function of the L2 ball with radius r.\n\nMathematical Definition\n\nThe indicator function of the set B₂(r) = {x : ||x||₂ ≤ r}:\n\nf(x) = 0    if ||x||₂ ≤ r f(x) = +∞   otherwise\n\nArguments\n\nr::Float64: Radius of the L2 ball (must be positive)\n\nProperties\n\nSmooth: No, not differentiable on the boundary\nConvex: Yes, indicator functions of convex sets are convex\nProximal: Yes, has explicit proximal operator (projection onto ball)\nSet Indicator: Yes, this is an indicator function\n\nMathematical Properties\n\nProximal Operator: Projection onto the L2 ball\nIf ||x||₂ ≤ r: prox_f(x) = x\nIf ||x||₂ > r: prox_f(x) = (r/||x||₂) * x\n\nExamples\n\n# Unit L2 ball (radius 1)\nf = IndicatorBallL2(1.0)\nx = [0.5, 0.5]\nval = f(x)  # Returns 0.0 since ||x||₂ = √0.5 < 1\n\n# Point outside the ball\nx = [2.0, 2.0]\nval = f(x)  # Returns +∞ since ||x||₂ = 2√2 > 1\n\n# Proximal operator (projection onto ball)\nf = IndicatorBallL2(1.0)\nx = [3.0, 4.0]  # ||x||₂ = 5\nprox_x = proximalOracle(f, x)  # Returns [0.6, 0.8] (normalized to unit length)\n\nApplications\n\nConstraint sets in optimization\nRegularization in machine learning\nTrust region methods\nRobust optimization\nSignal processing (bounded energy constraints)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorBox","page":"Functions","title":"PDMO.IndicatorBox","text":"IndicatorBox(lb::NumericVariable, ub::NumericVariable)\n\nIndicator function of a box constraint set [lb, ub].\n\nMathematical Definition\n\nThe indicator function of the box constraint set: {x : lb ≤ x ≤ ub} (element-wise inequalities)\n\nf(x) = 0    if lb ≤ x ≤ ub (element-wise) f(x) = +∞   otherwise\n\nArguments\n\nlb::NumericVariable: Lower bound vector/scalar\nub::NumericVariable: Upper bound vector/scalar\n\nProperties\n\nSmooth: No, not differentiable on the boundary\nConvex: Yes, indicator functions of convex sets are convex\nProximal: Yes, has explicit proximal operator (projection onto box)\nSet Indicator: Yes, this is an indicator function\n\nMathematical Properties\n\nProximal Operator: Element-wise projection onto the box\nprox_f(x) = clamp(x, lb, ub) = max(lb, min(x, ub))\n\nExamples\n\n# Unit box constraint [-1, 1]ⁿ\nlb = [-1.0, -1.0]\nub = [1.0, 1.0]\nf = IndicatorBox(lb, ub)\nx = [0.5, -0.5]\nval = f(x)  # Returns 0.0 since x is within bounds\n\n# Point outside the box\nx = [2.0, -2.0]\nval = f(x)  # Returns +∞ since x violates bounds\n\n# Proximal operator (projection onto box)\nf = IndicatorBox([-1.0, -1.0], [1.0, 1.0])\nx = [2.0, -2.0]\nprox_x = proximalOracle(f, x)  # Returns [1.0, -1.0] (clamped to bounds)\n\nApplications\n\nVariable bounds in optimization\nConstraint sets in quadratic programming\nImage processing (pixel value bounds)\nControl systems (actuator limits)\nPortfolio optimization (position limits)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorHyperplane","page":"Functions","title":"PDMO.IndicatorHyperplane","text":"IndicatorHyperplane(slope::Vector{Float64}, intercept::Float64)\n\nRepresents the indicator function of a hyperplane defined by ⟨slope, x⟩ = intercept.\n\nMathematical Definition\n\nThe indicator function of the hyperplane H = {x : ⟨a,x⟩ = b}:\n\nI_H(x) = begincases\n0  textif  langle ax rangle = b \n+infty  textotherwise\nendcases\n\nwhere a is the normal vector (slope) and b is the intercept.\n\nArguments\n\nslope::Vector{Float64}: Normal vector of the hyperplane (must be non-zero)\nintercept::Float64: Right-hand side of the hyperplane equation\n\nProperties\n\nSmooth: No, not differentiable (indicator function)\nConvex: Yes, indicator functions of convex sets are convex\nProximal: Yes, proximal operator is projection onto the hyperplane\nSet Indicator: Yes, represents the constraint set ⟨a,x⟩ = b\n\nMathematical Properties\n\nHyperplane equation: ⟨slope, x⟩ = intercept\nNormal vector: slope vector points orthogonal to the hyperplane\nProximal operator (projection): P_H(x) = x - (⟨a,x⟩ - b) · a/||a||²\n\nGeometric Interpretation\n\nHyperplane: (n-1)-dimensional affine subspace in ℝⁿ\nNormal vector: slope defines the orientation of the hyperplane\nDistance from origin: |intercept|/||slope|| when slope is normalized\nProjection: Orthogonal projection onto the hyperplane\n\nInternal Structure\n\nThe constructor pre-computes:\n\nscaledSlope = slope / ||slope||²: Normalized direction for efficient projection\nValidation: slope vector cannot be zero or empty\n\nImplementation Details\n\nNumerical stability: Pre-computes normalized slope to avoid repeated divisions\nEfficient projection: Uses pre-computed scaledSlope in proximal operator\nTolerance handling: Uses FeasTolerance for numerical feasibility checks\nMemory efficiency: Minimal storage with pre-computed values\n\nExamples\n\n# Hyperplane x₁ + 2x₂ = 3\nslope = [1.0, 2.0]\nintercept = 3.0\nf = IndicatorHyperplane(slope, intercept)\n\n# Check if point is on hyperplane\nx_on = [1.0, 1.0]  # 1*1 + 2*1 = 3 ✓\nval = f(x_on)  # Returns 0.0\n\nx_off = [0.0, 0.0]  # 1*0 + 2*0 = 0 ≠ 3 ✗\nval = f(x_off)  # Returns +∞\n\n# Project point onto hyperplane\nx = [2.0, 3.0]  # Not on hyperplane: 1*2 + 2*3 = 8 ≠ 3\nx_proj = proximalOracle(f, x)  # Projects x onto hyperplane\n\n# Verify projection is on hyperplane\n@assert abs(dot(slope, x_proj) - intercept) < 1e-10\n\n# 2D example: line x - y = 1\nf_line = IndicatorHyperplane([1.0, -1.0], 1.0)\nx = [0.0, 0.0]\nx_proj = proximalOracle(f_line, x)  # Projects to line x - y = 1\n\n# 3D example: plane x + y + z = 1\nf_plane = IndicatorHyperplane([1.0, 1.0, 1.0], 1.0)\nx = [2.0, 2.0, 2.0]\nx_proj = proximalOracle(f_plane, x)  # Projects to plane\n\nAlgorithm Applications\n\nEquality constraints: Represent linear equality constraints in optimization\nProjection methods: Project iterates onto constraint hyperplanes\nADMM: Enforce linear equality constraints\nFeasibility problems: Find points satisfying linear equations\nMethod of alternating projections: Between multiple hyperplanes\nLinear programming: Represent equality constraints\nSupport vector machines: Separating hyperplanes\n\nOptimization Context\n\nCommonly appears in constrained optimization:\n\nminimize f(x)\nsubject to ⟨a,x⟩ = b  # Represented by IndicatorHyperplane\n\nOr in penalty/augmented Lagrangian methods:\n\nminimize f(x) + (μ/2)||⟨a,x⟩ - b||² + IndicatorHyperplane(a,b)(x)\n\nProjection Formula\n\nThe projection of point x onto hyperplane H = {z : ⟨a,z⟩ = b} is:\n\nP_H(x) = x - ((⟨a,x⟩ - b) / ||a||²) · a\n\nThis formula:\n\nComputes the signed distance from x to the hyperplane: (⟨a,x⟩ - b) / ||a||\nMoves x by this distance in the negative normal direction: a / ||a||²\n\nSpecial Cases\n\nOrigin-centered hyperplane: intercept = 0 gives ⟨a,x⟩ = 0\nCoordinate hyperplane: slope = eᵢ gives xᵢ = intercept\n45-degree line (2D): slope = [1,1] gives x₁ + x₂ = intercept\n\nNumerical Considerations\n\nNon-zero slope: Constructor validates ||slope|| > ZeroTolerance\nScaling invariance: (α·slope, α·intercept) represents the same hyperplane\nNumerical stability: Pre-computed scaledSlope avoids numerical issues\nTolerance: Uses FeasTolerance for feasibility checking\n\nRelationship to Other Constraints\n\nLinear subspace: IndicatorLinearSubspace for Ax = b (multiple equations)\nHalf-space: Related to ⟨a,x⟩ ≤ b constraints\nBox constraints: Coordinate hyperplanes form box constraint boundaries\nPolytopes: Intersection of multiple hyperplanes and half-spaces\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorLinearSubspace","page":"Functions","title":"PDMO.IndicatorLinearSubspace","text":"IndicatorLinearSubspace(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64})\n\nRepresents the indicator function of a linear subspace defined by the equation Ax = b.\n\nThe indicator function is defined as:\n\nI_Ax=b(x) = begincases\n0  textif  Ax = b \ninfty  textotherwise\nendcases\n\nArguments\n\nA::SparseMatrixCSC{Float64, Int64}: The constraint matrix\nb::Vector{Float64}: The right-hand side vector\n\nFields\n\nA: The constraint matrix\nb: The right-hand side vector\nU: Left singular vectors from SVD decomposition\nS: Singular values from SVD decomposition\nV: Right singular vectors from SVD decomposition\nrank: Numerical rank of matrix A\nisFullRank: Boolean indicating if A has full row rank\nprojectionMatrix: Pre-computed matrix for projection:\nIf full rank: stores (AA')^{-1}\nIf rank deficient: stores A^+ (pseudoinverse)\n\nNotes\n\nSVD decomposition and projection matrices are computed once during initialization for efficiency\nThe proximal operator (projection) is computed differently based on whether A has full row rank:\nFor full rank: y = x - A'(AA')^{-1}(Ax - b)\nFor rank deficient: y = x - A^+(Ax - b), where A^+ is the pseudoinverse\nNumerical rank is determined using a tolerance based on machine epsilon\n\nExample\n\nA = sparse([1.0 2.0; 3.0 4.0])\nb = [1.0, 2.0]\nf = IndicatorLinearSubspace(A, b)\nx = [0.0, 0.0]\nproj = proximalOracle(f, x)  # Projects x onto the subspace Ax = b\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorNonnegativeOrthant","page":"Functions","title":"PDMO.IndicatorNonnegativeOrthant","text":"IndicatorNonnegativeOrthant()\n\nIndicator function of the nonnegative orthant ℝⁿ₊.\n\nMathematical Definition\n\nThe indicator function of the nonnegative orthant:\n\nI_{ℝⁿ₊}(x) = 0     if xᵢ ≥ 0 for all i ∈ {1,...,n}\nI_{ℝⁿ₊}(x) = +∞    otherwise\n\nThis represents the constraint that all components of x must be non-negative.\n\nConstructor\n\nIndicatorNonnegativeOrthant()\n\nNo parameters needed - the constraint applies to any dimensional input.\n\nProperties\n\nSmooth: No, not differentiable at the boundary (xᵢ = 0)\nConvex: Yes, the nonnegative orthant is a convex cone\nProximal: Yes, proximal operator is element-wise projection\nSet Indicator: Yes, represents the constraint set ℝⁿ₊\n\nMathematical Properties\n\nConstraint set: ℝⁿ₊ = {x ∈ ℝⁿ : xᵢ ≥ 0 ∀i}\nGeometric interpretation: First orthant in ℝⁿ (all coordinates non-negative)\nProximal operator: Element-wise projection: [prox_f(x)]ᵢ = max(xᵢ, 0)\nConjugate function: Sum of negative parts: f*(y) = ∑ᵢ max(-yᵢ, 0)\n\nGeometric Interpretation\n\nOrthant: One of 2ⁿ orthants in ℝⁿ (the \"positive\" one)\nBoundary: Coordinate hyperplanes where xᵢ = 0\nInterior: Points where xᵢ > 0 for all i\nConvex cone: Closed under positive linear combinations\n\nImplementation Details\n\nDimension-agnostic: Works with any input dimension\nTolerance-aware: Uses FeasTolerance for numerical feasibility checks\nParallel support: Implements parallel computation for large vectors\nMemory efficient: In-place operations, no temporary allocations\n\nExamples\n\n# Create constraint function\nf = IndicatorNonnegativeOrthant()\n\n# Check feasibility\nx_feasible = [1.0, 2.0, 0.0]  # All components ≥ 0\nval = f(x_feasible)  # Returns 0.0\n\nx_infeasible = [1.0, -1.0, 2.0]  # Contains negative component\nval = f(x_infeasible)  # Returns +∞\n\n# Project onto nonnegative orthant\nx = [2.0, -3.0, 0.5, -1.0]\nx_proj = proximalOracle(f, x)  # Returns [2.0, 0.0, 0.5, 0.0]\n\n# In-place projection (more efficient)\nx = [2.0, -3.0, 0.5, -1.0]\ny = similar(x)\nproximalOracle!(y, f, x)  # y = [2.0, 0.0, 0.5, 0.0]\n\n# Works with any dimension\nx_1d = [-5.0]\nproj_1d = proximalOracle(f, x_1d)  # Returns [0.0]\n\nx_high_dim = randn(1000)  # Random 1000-dimensional vector\nproj_high = proximalOracle(f, x_high_dim)  # Projects all components\n\n# Parallel computation for large vectors\nx_large = randn(10000)\nproj_parallel = proximalOracle(f, x_large, 1.0, true)  # enableParallel=true\n\nAlgorithm Applications\n\nNon-negativity constraints: Enforce xᵢ ≥ 0 in optimization\nProjected gradient descent: Project gradient steps onto feasible region\nADMM: Enforce non-negativity in alternating minimization\nSupport vector machines: Non-negative dual variables\nNon-negative matrix factorization: Constrain factor matrices\nPortfolio optimization: Non-negative asset weights\nImage processing: Non-negative pixel intensities\nCompressed sensing: Sparse non-negative signal recovery\n\nOptimization Context\n\nCommonly appears in constrained optimization:\n\nminimize f(x)\nsubject to x ≥ 0  # Component-wise non-negativity\n\nThis constraint is often handled via:\n\nProjected methods: Project iterates onto ℝⁿ₊\nPenalty methods: Add barrier/penalty terms\nPrimal-dual methods: Use dual variables for constraints\n\nProjection Properties\n\nThe projection P_{ℝⁿ₊}(x) has several important properties:\n\nIdempotent: P(P(x)) = P(x)\nNon-expansive: ||P(x) - P(y)|| ≤ ||x - y||\nSeparable: [P(x)]ᵢ depends only on xᵢ\nMonotone: xᵢ ≤ yᵢ ⟹ [P(x)]ᵢ ≤ [P(y)]ᵢ\n\nPerformance Characteristics\n\nFunction evaluation: O(n) complexity\nProximal operator: O(n) complexity\nMemory usage: O(1) additional storage\nParallel scaling: Excellent (independent components)\n\nSpecial Cases and Extensions\n\nBox constraints: Combine with upper bounds for IndicatorBox\nSimplex constraints: Add ∑xᵢ = 1 constraint\nCone constraints: Extension to more general convex cones\nInteger constraints: Combine with integrality requirements\n\nNumerical Considerations\n\nTolerance handling: Uses FeasTolerance for boundary points\nFloating-point precision: Robust to small numerical errors\nParallel threshold: Uses parallelization for vectors > 1000 elements\nZero preservation: Exactly preserves zero values (no numerical drift)\n\nRelationship to Other Constraints\n\nIndicatorBox: Generalization with both lower and upper bounds\nIndicatorBallL2: Different geometry (curved vs. polyhedral boundary)\nIndicatorSOC: Generalization to second-order cone constraints\nLinear inequalities: Subset of general linear inequality constraints\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorPSD","page":"Functions","title":"PDMO.IndicatorPSD","text":"IndicatorPSD(dim::Int64)\n\nRepresents the indicator function of the positive semidefinite cone.\n\nThe indicator function of the positive semidefinite cone is defined as:\n\nI_mathcalS_+^n(X) = begincases\n0  textif  X in mathcalS_+^n \ninfty  textotherwise\nendcases\n\nwhere mathcalS_+^n = X in mathbbR^n times n  X = X^T X succeq 0 is the cone of  symmetric positive semidefinite matrices.\n\nMathematical Properties\n\nConvex: The PSD cone is convex, making this a convex indicator function\nClosed: The PSD cone is closed, ensuring well-defined projections\nSelf-dual: The PSD cone is self-dual under the trace inner product\nProximal: The proximal operator corresponds to projection onto the PSD cone\n\nArguments\n\ndim::Int64: The dimension n of the n×n matrix space (must be ≥ 1)\n\nFields\n\ndim::Int64: Matrix dimension (n×n)\nX::Matrix{Float64}: Internal buffer for dense matrix computations\n\nFunction Properties\n\nisProximal(IndicatorPSD): true - admits efficient proximal operator\nisConvex(IndicatorPSD): true - indicator of convex cone\nisSet(IndicatorPSD): true - indicator function of a set\n\nProximal Operator\n\nThe proximal operator (projection onto PSD cone) is computed via:\n\nSymmetrization: Ensure input matrix is symmetric: barX = (X + X^T)2\nEigendecomposition: Compute barX = QLambda Q^T\nProjection: Set negative eigenvalues to zero: Lambda_+ = max(Lambda 0)\nReconstruction: Return textproj(X) = QLambda_+ Q^T\n\nImplementation Details\n\nSupports both dense (Matrix{Float64}) and sparse (SparseMatrixCSC{Float64}) matrices\nUses symmetric eigendecomposition for numerical stability\nEnforces perfect symmetry in output to avoid numerical artifacts\nTolerances: ZeroTolerance for symmetry, FeasTolerance for positive semidefiniteness\n\nApplications\n\nSemidefinite Programming: Constraint X ⪰ 0 in optimization\nCovariance Estimation: Ensuring positive semidefinite covariance matrices\nGram Matrix Constraints: In kernel methods and matrix completion\nRelaxations: Convex relaxation of rank constraints\n\nExamples\n\n# Create 3×3 PSD indicator\nf = IndicatorPSD(3)\n\n# Test with positive semidefinite matrix\nX_psd = [2.0 1.0 0.0; 1.0 2.0 0.0; 0.0 0.0 1.0]\n@assert f(X_psd) == 0.0  # In PSD cone\n\n# Test with indefinite matrix\nX_indef = [1.0 2.0; 2.0 1.0]  # Has negative eigenvalue\n@assert f(X_indef) == Inf  # Not in PSD cone\n\n# Project indefinite matrix onto PSD cone\nX_proj = proximalOracle(f, X_indef)\n@assert f(X_proj) == 0.0  # Projection is PSD\n\nPerformance Notes\n\nEigendecomposition is O(n³) - dominant computational cost\nDense matrices preferred for efficiency; sparse matrices converted internally\nPre-allocated buffers reduce memory allocation overhead\nSymmetric views used when possible to exploit structure\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorRotatedSOC","page":"Functions","title":"PDMO.IndicatorRotatedSOC","text":"IndicatorRotatedSOC(dim::Int64)\n\nRepresents the indicator function of the rotated second-order cone (also known as the quadratic cone).\n\nThe indicator function of the rotated second-order cone is defined as:\n\nI_mathcalQ^n(x) = begincases\n0  textif  x in mathcalQ^n \ninfty  textotherwise\nendcases\n\nwhere the rotated second-order cone is defined as:\n\nmathcalQ^n = (uvw) in mathbbR times mathbbR times mathbbR^n-2  w_2^2 leq 2uv u geq 0 v geq 0\n\nfor n geq 3, where u = x_1, v = x_2, and w = x_3n.\n\nMathematical Properties\n\nConvex: The rotated SOC is a convex cone, making this a convex indicator function\nSelf-dual: The rotated SOC is self-dual under the standard inner product\nClosed: The rotated SOC is closed, ensuring well-defined projections\nPointed: Has a non-empty interior and is pointed (contains no lines)\n\nArguments\n\ndim::Int64: The dimension n of the ambient space (must be ≥ 3)\n\nFields\n\ndim::Int64: Dimension of the ambient space\nsqrt2::Float64: Pre-computed √2 constant for efficiency\n\nFunction Properties\n\nisProximal(IndicatorRotatedSOC): true - admits efficient proximal operator\nisConvex(IndicatorRotatedSOC): true - indicator of convex cone\nisSet(IndicatorRotatedSOC): true - indicator function of a set\n\nProximal Operator\n\nThe proximal operator (projection onto rotated SOC) is computed via transformation to standard SOC:\n\nTransform to standard SOC: For point (uvw), compute:\ns = u + v (sum)\nt = u - v (difference)  \nz = sqrt2w (scaled vector)\nProject onto standard SOC: Project (stz) onto (stz)  (tz)_2 leq s\nTransform back: From projected (stz), compute:\nu = max(0 (s + t)2)\nv = max(0 (s - t)2)\nw = zsqrt2\n\nImplementation Details\n\nUses linear transformation to reduce to standard SOC projection\nEnforces non-negativity constraints explicitly after transformation\nPre-computes √2 for numerical efficiency\nHandles edge cases and maintains numerical stability\n\nApplications\n\nQuadratic Programming: Quadratic constraints x^T Q x leq t with Q succeq 0\nGeometric Programming: Posynomial constraints in convex form\nRobust Optimization: Ellipsoidal uncertainty sets\nSignal Processing: Power constraints in communication systems\nFinance: Mean-variance portfolio optimization with transaction costs\n\nExamples\n\n# Create 4D rotated SOC\nf = IndicatorRotatedSOC(4)\n\n# Test point inside the cone\nx_in = [2.0, 1.0, 1.0, 1.0]  # ||[1.0, 1.0]||₂² = 2 ≤ 2·2·1 = 4\n@assert f(x_in) == 0.0  # Inside cone\n\n# Test point outside the cone\nx_out = [1.0, 1.0, 2.0, 2.0]  # ||[2.0, 2.0]||₂² = 8 > 2·1·1 = 2\n@assert f(x_out) == Inf  # Outside cone\n\n# Project onto cone\nx_proj = proximalOracle(f, x_out)\n@assert f(x_proj) == 0.0  # Projection is in cone\n\n# Boundary case: on the boundary\nx_bound = [1.0, 2.0, 2.0, 0.0]  # ||[2.0, 0.0]||₂² = 4 = 2·1·2\n@assert f(x_bound) == 0.0  # On boundary\n\nGeometric Interpretation\n\nThe rotated second-order cone represents the set of points where the squared norm of the  vector part is bounded by twice the product of two non-negative scalars. Key properties:\n\nQuadratic constraint: The defining constraint is quadratic in the variables\nHyperbolic: The boundary forms a hyperbolic surface in higher dimensions\nGeometric mean: Related to the geometric mean constraint sqrtuv geq w_2sqrt2\n\nRelationship to Standard SOC\n\nThe rotated SOC is related to the standard SOC via the linear transformation:\n\nbeginpmatrix u  v  w endpmatrix mapsto beginpmatrix u+v  u-v  sqrt2w endpmatrix\n\nThis transformation preserves the cone structure and enables efficient projection algorithms.\n\nPerformance Notes\n\nProjection requires O(n) operations via SOC transformation\nMemory allocation minimal - uses pre-allocated buffers\nNumerically stable for well-conditioned problems\nHandles degenerate cases (zero components) gracefully\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorSOC","page":"Functions","title":"PDMO.IndicatorSOC","text":"IndicatorSOC(dim::Int64, radiusIndex::Int64)\n\nRepresents the indicator function of the second-order cone (also known as the Lorentz cone).\n\nThe indicator function of the second-order cone is defined as:\n\nI_mathcalL^n(x) = begincases\n0  textif  x in mathcalL^n \ninfty  textotherwise\nendcases\n\nwhere the second-order cone is defined as:\n\nIf radiusIndex = n: mathcalL^n = x in mathbbR^n  x_1n-1_2 leq x_n\nIf radiusIndex = 1: mathcalL^n = x in mathbbR^n  x_2n_2 leq x_1\n\nMathematical Properties\n\nConvex: The SOC is a convex cone, making this a convex indicator function\nSelf-dual: The SOC is self-dual under the standard inner product\nClosed: The SOC is closed, ensuring well-defined projections\nPointed: The SOC has a non-empty interior and is pointed (contains no lines)\n\nArguments\n\ndim::Int64: The dimension n of the ambient space (must be ≥ 2)\nradiusIndex::Int64: Position of the radius variable (must be 1 or dim)\n\nFields\n\ndim::Int64: Dimension of the ambient space\nradiusIndex::Int64: Index of the radius variable (1 or dim)\n\nFunction Properties\n\nisProximal(IndicatorSOC): true - admits efficient proximal operator\nisConvex(IndicatorSOC): true - indicator of convex cone\nisSet(IndicatorSOC): true - indicator function of a set\n\nProximal Operator\n\nThe proximal operator (projection onto SOC) is computed analytically:\n\nFor a point x with radius component r and vector component v:\n\nIf v_2 leq r: x is already in the cone, so textproj(x) = x\nIf v_2 leq -r: x is in the polar cone, so textproj(x) = 0\nOtherwise: textproj(x) = fracv_2 + r2v_2beginpmatrix v  v_2 endpmatrix\n\nImplementation Details\n\nProjection formula handles both radiusIndex = 1 and radiusIndex = dim cases\nUses tolerance FeasTolerance for membership testing\nEfficient O(n) implementation avoiding matrix operations\nHandles edge cases (zero norm, negative radius) robustly\n\nApplications\n\nSecond-Order Cone Programming (SOCP): Constraint Ax + b_2 leq c^T x + d\nRobust Optimization: Uncertainty sets and robust constraints\nSignal Processing: Beamforming and antenna array design\nMachine Learning: Support vector machines with nonlinear kernels\nControl Theory: Linear matrix inequalities and stability analysis\n\nExamples\n\n# Create 3D SOC with radius at last position\nf = IndicatorSOC(3, 3)\n\n# Test point inside the cone\nx_in = [0.5, 0.3, 1.0]  # ||[0.5, 0.3]||₂ = 0.583 < 1.0\n@assert f(x_in) == 0.0  # Inside cone\n\n# Test point outside the cone  \nx_out = [2.0, 1.0, 1.0]  # ||[2.0, 1.0]||₂ = 2.236 > 1.0\n@assert f(x_out) == Inf  # Outside cone\n\n# Project onto cone\nx_proj = proximalOracle(f, x_out)\n@assert f(x_proj) == 0.0  # Projection is in cone\n\n# Alternative: radius at first position\nf_alt = IndicatorSOC(3, 1)\nx_alt = [1.0, 0.5, 0.3]  # ||[0.5, 0.3]||₂ = 0.583 < 1.0\n@assert f_alt(x_alt) == 0.0  # Inside cone\n\nGeometric Interpretation\n\nThe second-order cone is the set of points where the Euclidean norm of the vector part  is bounded by the scalar radius part. It has several equivalent representations:\n\nIce cream cone: The familiar \"ice cream cone\" shape in 3D\nEpigraph: Epigraph of the Euclidean norm function\nIntersection: Can be represented as intersection of half-spaces\n\nPerformance Notes\n\nProjection requires O(n) operations (single norm computation)\nMemory allocation minimal - uses in-place operations when possible\nNumerically stable for well-conditioned problems\nHandles degenerate cases (zero radius, zero vector) gracefully\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.IndicatorSumOfNVariables","page":"Functions","title":"PDMO.IndicatorSumOfNVariables","text":"(f::IndicatorSumOfNVariables)(x::NumericVariable, enableParallel::Bool=false) -> Float64\n\nEvaluates the indicator function for the constraint\n\nx₁ + x₂ + … + xₙ = rhs\n\nIf rhs is a scalar, then x is expected to be a vector of length numberVariables and the constraint is sum(x) ≈ rhs (within FeasTolerance). If rhs is not a scalar, then x is assumed to be an array whose first dimension is size(rhs, 1) * numberVariables and the remaining dimensions match rhs. In that case, the function reshapes each block (along the first dimension) to the shape of rhs, sums them elementwise, and compares the result with rhs. Returns 0.0 if the constraint is satisfied (within tolerance) and Inf otherwise.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#User-Defined-Functions","page":"Functions","title":"User-Defined Functions","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.ComponentwiseExponentialFunction","page":"Functions","title":"PDMO.ComponentwiseExponentialFunction","text":"ComponentwiseExponentialFunction(coefficients::Vector{Float64})\n\nRepresents a component-wise exponential function f(x) = ∑ᵢ aᵢ exp(xᵢ).\n\nMathematical Definition\n\nf(x) = ∑ᵢ aᵢ exp(xᵢ)\n\nwhere a = coefficients is a vector of non-negative weights.\n\nArguments\n\ncoefficients::Vector{Float64}: Vector of non-negative coefficients\n\nConstructors\n\nComponentwiseExponentialFunction(coefficients): With specified coefficients\nComponentwiseExponentialFunction(n::Int64): With unit coefficients (ones vector)\n\nProperties\n\nSmooth: Yes, exponential functions are infinitely differentiable\nConvex: Yes, exponential functions are convex\nProximal: No, proximal operator requires Lambert W function (not implemented)\n\nMathematical Properties\n\nGradient: ∇f(x) = [a₁ exp(x₁), a₂ exp(x₂), ..., aₙ exp(xₙ)]\nHessian: ∇²f(x) = diag(a₁ exp(x₁), a₂ exp(x₂), ..., aₙ exp(xₙ))\n\nExamples\n\n# Standard exponential function f(x) = exp(x₁) + exp(x₂)\nf = ComponentwiseExponentialFunction([1.0, 1.0])\nx = [0.0, 1.0]\nval = f(x)  # Returns exp(0) + exp(1) = 1 + e ≈ 3.718\n\n# Weighted exponential function f(x) = 2exp(x₁) + 3exp(x₂)\nf = ComponentwiseExponentialFunction([2.0, 3.0])\nx = [0.0, 0.0]\nval = f(x)  # Returns 2*1 + 3*1 = 5\n\n# Gradient computation\ngrad = gradientOracle(f, x)  # Returns [2.0, 3.0]\n\nApplications\n\nExponential utility functions\nLog-sum-exp approximations\nEntropic regularization\nBarrier methods in optimization\nStatistical modeling (exponential families)\n\nNote\n\nThe proximal operator requires the Lambert W function and is not currently implemented.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.UserDefinedProximalFunction","page":"Functions","title":"PDMO.UserDefinedProximalFunction","text":"UserDefinedProximalFunction(func::Function, proximalFunc::Function, convex::Bool=true)\n\nWrapper for user-defined functions that have a proximal operator.\n\nThis allows users to define custom functions by providing both the function evaluation and its proximal operator, enabling integration with proximal algorithms.\n\nArguments\n\nfunc::Function: Function evaluation f(x) → Float64\nproximalFunc::Function: Proximal operator (x, γ) → result\nconvex::Bool=true: Whether the function is convex\n\nProperties\n\nSmooth: No, proximal functions are typically non-smooth\nConvex: User-specified (default true)\nProximal: Yes, by definition\n\nMathematical Properties\n\nFunction evaluation: f(x) provided by user\nProximal operator: prox_γf(x) provided by user\n\nExamples\n\n# L1 Norm Function\n# f(x) = λ||x||₁ (L1 norm with coefficient λ)\nλ = 0.5\nfunc = x -> λ * sum(abs.(x))\nproximalFunc = (x, gamma) -> sign.(x) .* max.(abs.(x) .- gamma * λ, 0.0)  # Soft thresholding\nf = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true\n\n# Indicator Function of Box Constraints\n# f(x) = I_{[a,b]}(x) (indicator function of box [a,b])\na, b = -1.0, 1.0\nfunc = x -> all(a .<= x .<= b) ? 0.0 : Inf\nproximalFunc = (x, gamma) -> clamp.(x, a, b)  # Projection onto box\nf = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true\n\n# Custom Regularization Function\n# f(x) = α * g(x) where g has known proximal operator\nα = 0.1\nfunc = x -> α * myCustomFunction(x)\nproximalFunc = (x, gamma) -> myCustomProximal(x, gamma * α)\nf = UserDefinedProximalFunction(func, proximalFunc, true)  # convex=true\n\nIntegration with Bipartization\n\n# In your optimization problems\nblock_x = BlockVariable(xID)\nblock_x.f = UserDefinedProximalFunction(myFunc, myProximal, true)\naddBlockVariable!(nlp, block_x)\n\nRequirements\n\nfunc(x) must return a Float64 value\nproximalFunc(x, gamma) must return a result of the same type as x\nBoth functions must be consistent with the mathematical definition\nThe proximal operator must satisfy: proxγf(x) = argminz { f(z) + (1/(2γ))||z - x||² }\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.UserDefinedSmoothFunction","page":"Functions","title":"PDMO.UserDefinedSmoothFunction","text":"UserDefinedSmoothFunction(func::Function, gradientFunc::Function, convex::Bool=true)\n\nWrapper for user-defined smooth functions that have a gradient.\n\nThis allows users to define custom smooth functions by providing both the function  evaluation and its gradient, enabling integration with gradient-based algorithms.\n\nArguments\n\nfunc::Function: Function evaluation f(x) → Float64\ngradientFunc::Function: Gradient function x → ∇f(x)\nconvex::Bool=true: Whether the function is convex\n\nProperties\n\nSmooth: Yes, by definition\nConvex: User-specified (default true)\nProximal: No, user-defined functions typically don't have proximal oracles\n\nMathematical Properties\n\nFunction evaluation: f(x) provided by user\nGradient: ∇f(x) provided by user\n\nExamples\n\n# Simple Quadratic Function\n# f(x) = x₁² + 2x₂² + x₁x₂\nfunc = x -> x[1]^2 + 2*x[2]^2 + x[1]*x[2]\ngradientFunc = x -> [2*x[1] + x[2], 4*x[2] + x[1]]\nf = UserDefinedSmoothFunction(func, gradientFunc, true)  # convex=true\n\n# Non-convex Function\n# f(x) = sin(x₁) + cos(x₂)\nfunc = x -> sin(x[1]) + cos(x[2])\ngradientFunc = x -> [cos(x[1]), -sin(x[2])]\nf = UserDefinedSmoothFunction(func, gradientFunc, false)  # convex=false\n\n# Rosenbrock Function (classic optimization test function)\n# f(x) = (1-x₁)² + 100(x₂-x₁²)²\nfunc = x -> (1-x[1])^2 + 100*(x[2]-x[1]^2)^2\ngradientFunc = x -> [-2*(1-x[1]) - 400*x[1]*(x[2]-x[1]^2), 200*(x[2]-x[1]^2)]\nf = UserDefinedSmoothFunction(func, gradientFunc, false)  # non-convex\n\nIntegration with Bipartization\n\n# In your optimization problems\nblock_x = BlockVariable(xID)\nblock_x.f = UserDefinedSmoothFunction(myFunc, myGradient, true)\naddBlockVariable!(nlp, block_x)\n\nRequirements\n\nfunc(x) must return a Float64 value\ngradientFunc(x) must return a gradient of the same type as x\nBoth functions must be consistent with the mathematical definition\nThe gradient must satisfy: ∇f(x) = lim_{h→0} [f(x+h) - f(x)]/h\nFor correctness, consider using automatic differentiation tools to compute gradients\n\nApplications\n\nCustom objective functions\nDomain-specific regularization terms\nPhysics-informed optimization\nMachine learning loss functions\nEngineering design optimization\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#Wrapper-Functions","page":"Functions","title":"Wrapper Functions","text":"","category":"section"},{"location":"S4_api/functions/#PDMO.WrapperScalarInputFunction","page":"Functions","title":"PDMO.WrapperScalarInputFunction","text":"WrapperScalarInputFunction(originalFunction::AbstractFunction)\n\nWrapper that adapts scalar-input functions to work with vector interfaces.\n\nThis wrapper takes a function that operates on scalars and adapts it to work with 1-dimensional vectors, enabling integration with vector-based optimization algorithms.\n\nArguments\n\noriginalFunction::AbstractFunction: Function that operates on scalar inputs\n\nProperties\n\nSmooth: Inherits from original function\nConvex: Inherits from original function  \nProximal: Inherits from original function\nSet: Inherits from original function\n\nMathematical Properties\n\nAll properties are inherited from the original function, with input/output adaptation:\n\nFunction evaluation: f([x]) = originalFunction(x)\nGradient: ∇f([x]) = [∇originalFunction(x)]\nProximal operator: proxf([x]) = [proxoriginalFunction(x)]\n\nInput/Output Format\n\nInput: Vector of length 1, e.g., [x]\nOutput: Scalar for function evaluation, vector of length 1 for gradient/proximal\n\nExamples\n\n# Adapt a scalar indicator function to vector interface\n# Original function: f(x) = I_{[0,1]}(x) for scalar x\noriginal_f = IndicatorBox(0.0, 1.0)  # Scalar version\nvector_f = WrapperScalarInputFunction(original_f)\n\n# Usage with vector input\nx = [0.5]  # Vector of length 1\nval = vector_f(x)  # Returns 0.0\ngrad = gradientOracle(vector_f, x)  # Returns [0.0] (if smooth)\nprox = proximalOracle(vector_f, x)  # Returns [0.5] (projection)\n\n# Invalid usage\nx = [0.5, 1.0]  # Vector of length 2\n# val = vector_f(x)  # ERROR: input must be vector of length 1\n\nUse Cases\n\nAdapting scalar functions for vector-based algorithms\nInterfacing with optimization solvers that expect vector inputs\nBuilding composite functions with mixed scalar/vector components\nLegacy code integration\n\nLimitations\n\nOnly works with vector inputs of length 1\nAdds slight computational overhead due to wrapping\nError checking is performed at runtime\nOriginal function must properly handle scalar inputs\n\nImplementation Notes\n\nThe wrapper performs the following adaptations:\n\nExtracts scalar from length-1 vector input\nCalls original function with scalar\nWraps scalar result back into vector format (for gradients/proximal)\nDelegates all trait checks to the original function\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/#PDMO.WrapperScalingTranslationFunction","page":"Functions","title":"PDMO.WrapperScalingTranslationFunction","text":"WrapperScalingTranslationFunction\n\nA wrapper that represents a transformed function of the form f(x) = g(coe·x + translation), where g is the original function, coe is a positive scaling coefficient, and translation is an additive shift.\n\nThis transformation is useful for:\n\nScaling and shifting existing functions\nImplementing variable transformations in optimization\nCreating scaled versions of constraint functions\nHandling affine transformations of variables\n\nFields\n\noriginalFunction::AbstractFunction: The original function g\ncoe::Float64: The scaling coefficient (must be positive)\ntranslation::NumericVariable: The translation vector/scalar\nbuffer::NumericVariable: Internal buffer for computations\n\nMathematical Properties\n\nFor f(x) = g(coe·x + translation):\n\nFunction evaluation: f(x) = g(coe·x + translation)\nGradient (if g is smooth): ∇f(x) = coe · ∇g(coe·x + translation)\nProximal operator: prox{γf}(z) = (prox{γ·coe², g}(translation + coe·z) - translation) / coe\n\nExamples\n\n# Create a scaled and shifted L2 ball: ||2x + [1,1]||₂ ≤ 1\ng = IndicatorBallL2(1.0)\nf = WrapperScalingTranslationFunction(g, 2.0, [1.0, 1.0])\n\n# Create a shifted quadratic: (1/2)(x - 2)²\ng = QuadraticFunction(1.0, 0.0, 0.0)  # (1/2)x²\nf = WrapperScalingTranslationFunction(g, 1.0, -2.0)  # (1/2)(x + (-2))² = (1/2)(x - 2)²\n\n\n\n\n\n","category":"type"},{"location":"S4_api/functions/","page":"Functions","title":"Functions","text":"","category":"page"},{"location":"S2_algorithms/ADMM/#Alternating-Direction-Method-of-Multipliers-(ADMM)","page":"ADMM","title":"Alternating Direction Method of Multipliers (ADMM)","text":"","category":"section"},{"location":"S2_algorithms/ADMM/#Introduction-and-Solution-Process-Overview","page":"ADMM","title":"Introduction and Solution Process Overview","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The Alternating Direction Method of Multipliers (ADMM) is a powerful optimization algorithm that decomposes complex problems into simpler subproblems. The original ADMM solves convex optimization problems in the standard two-block form:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"beginaligned\nmin_xz quad  f(x) + h(y) \ntextst quad  Ax + By = c\nendaligned","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The algorithm alternates between three updates:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"beginalign\n   x^k+1 =  argmin_x  f(x) + langle u^k Ax + By^k - crangle + fracrho2Ax + By^k - c^2  notag \n   y^k+1 =  argmin_y h(y) + langle u^k Ax^k+1 + By - crangle + fracrho2Ax^k+1 + By - c^2 notag \n   u^k+1 =  u^k + rho(Ax^k+1 + By^k+1 - c) notag \nendalign","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"where rho  0 is the penalty parameter and u is the dual variable. When f and A (resp. h and B) admit certain  block-angular structures, the update of x^k+1 (resp. y^k+1) can be further parallelized. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"In practice, many optimization problems naturally arise in multiblock form with more than two variable blocks:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"beginaligned\nmin_x_1ldotsx_n quad  sum_j=1^n f_j(x_j) + g_j(x_j) \ntextst quad  sum_j in 1cdotsn A_ij x_j = b_i   j in 1cdots m\nendaligned","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"where A_ij is non-zero if and only constraint i involes block x_j.  PDMO.jl implements a comprehensive ADMM framework designed to solve multiblock optimization problems through an automated three-stage process:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"User Input: A user-defined MultiblockProblem together with a set of algorithmic options, i.e., ADMM subproblem solver, adapter, and accelerator. \nBipartization: Automatic reformulation using graph-based algorithms to convert the input problem into bipartite form, on which standard ADMM and variants","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"can be readily applied. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"ADMM Execution: The selected ADMM variant solves the reformulated problem with usder-specified algorithmic options. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"We provide more details regarding these three stages in the following sections. ","category":"page"},{"location":"S2_algorithms/ADMM/#MultiblockProblem:-A-Unified-Structure-for-Block-structued-Problems","page":"ADMM","title":"MultiblockProblem: A Unified Structure for Block-structued Problems","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"A MultiblockProblem is a container for multiblock optimization problems, maintaining collections of block variables and block constraints. The structure serves as the primary input to PDMO.jl.","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"mutable struct MultiblockProblem\n    blocks::Vector{BlockVariable}\n    constraints::Vector{BlockConstraint}\nend","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"blocks: A vector containing all block variables in the problem\nconstraints: A vector containing all block constraints connecting the variables","category":"page"},{"location":"S2_algorithms/ADMM/#BlockVariable","page":"ADMM","title":"BlockVariable","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"A BlockVariable represents an individual optimization variable block with its associated objective functions:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"mutable struct BlockVariable \n    id::BlockID                    # Unique identifier (Int64 or String)\n    f::AbstractFunction            # Smooth function component\n    g::AbstractFunction            # Nonsmooth/proximal function component  \n    val::NumericVariable           # Current variable value\nend","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Components:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"id: Unique identifier for the block (can be integer or string)\nf: Smooth function component f_i (e.g., quadratic, affine, exponential functions)\ng: Nonsmooth function component g_i handled via proximal operators (e.g., indicator functions, norms)\nval: Current value of the variable (scalar or vector)","category":"page"},{"location":"S2_algorithms/ADMM/#BlockConstraint","page":"ADMM","title":"BlockConstraint","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"A BlockConstraint represents equality constraints connecting multiple block variables:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"mutable struct BlockConstraint \n    id::BlockID                                    # Unique identifier\n    involvedBlocks::Vector{BlockID}                # Block IDs participating in constraint\n    mappings::Dict{BlockID, AbstractMapping}       # Linear mappings for each block\n    rhs::NumericVariable                           # Right-hand side value\nend","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Components:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"id: Unique identifier for the constraint\ninvolvedBlocks: Vector of block IDs that participate in this constraint\nmappings: Dictionary mapping each block ID to its linear transformation\nrhs: Right-hand side of the equality constraint","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Each constraint enforces the relationship:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"sum_i in textinvolvedBlocks (textmappingsi)(x_i) = textrhs ","category":"page"},{"location":"S2_algorithms/ADMM/#Graph-based-Bipartization","page":"ADMM","title":"Graph-based Bipartization","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Many optimization problems naturally arise in multiblock form where constraints involve more than two variable blocks.  Since a direct application of ADMM to multiblick problem may fail to converge, PDMO.jl automatically converts these  problems into bipartite form, i.e., a formulation where there are only two blocks, while each block consists of one or more sub-blocks. We briefly outline the procedures used by PDMO.jl to achieve this goal. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"To begin with, PDMO.jl will map a MultiblockProblem instance onto a graph with the following procedures:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"For each BlockVariable, introduce a new node in the graph. \nFor each BlockConstraint involving exactly 2 blocks, add an edge between the two corresponding nodes. \nFor each BlockConstraint involving more than 2 blocks, introduce a new node for this constraint, and connect this node with every node representing an involved block.  ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"In this way, a graph representation of a MultiblockProblem instance is constructed. As an illustrative example, consider a multiblock problem:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"beginaligned\n    min_x_1 x_2 x_3 quad  f_1(x_1) + f_2(x_2) + f_3(x_3)\n    mathrmstquad   A_1x_1 + A_2x_2 +A_3x_3 = a  \n     B_1x_1 + B_2x_2 = b\n     C_2x_2 + C_3x_3 = c\nendaligned","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"whose corresponding graph is show as the following: ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"<div style=\"text-align: center; margin: 20px 0;\">\n  <svg width=\"400\" height=\"300\" viewBox=\"0 0 400 300\">\n    <!-- Variable and constraint nodes -->\n    <circle cx=\"100\" cy=\"80\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"100\" y=\"85\" text-anchor=\"middle\" font-size=\"16\">x₁</text>\n    \n    <circle cx=\"100\" cy=\"220\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"100\" y=\"225\" text-anchor=\"middle\" font-size=\"16\">x₃</text>\n    \n    <circle cx=\"300\" cy=\"80\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"300\" y=\"85\" text-anchor=\"middle\" font-size=\"16\">x₂</text>\n    \n    <circle cx=\"300\" cy=\"220\" r=\"25\" fill=\"#FFA500\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"300\" y=\"225\" text-anchor=\"middle\" font-size=\"16\">C</text>\n    <text x=\"300\" y=\"275\" text-anchor=\"middle\" font-size=\"12\">A₁x₁ + A₂x₂ + A₃x₃ = a</text>\n    \n    <!-- Edges -->\n    <line x1=\"125\" y1=\"80\" x2=\"275\" y2=\"80\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"125\" y1=\"220\" x2=\"275\" y2=\"80\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"275\" y1=\"220\" x2=\"125\" y2=\"80\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"300\" y1=\"195\" x2=\"300\" y2=\"105\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"275\" y1=\"220\" x2=\"125\" y2=\"220\" stroke=\"#000\" stroke-width=\"2\"/>\n  </svg>\n</div>","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Edges (x_1 x_2) and (x_2 x_3) represent constraints B_1x_1 + B_2x_2 = b and C_2x_2 + C_3x_3 = c, respectively.  Node C corresponds to a new BlockVariable that has a proximal-friendly component g: the indicator function of the set ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Y = (y_1 y_2 y_3) y_1 + y_2 + y_3 = 0","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The edge between C and x_i represent the artifical constraint A_ix_i - y_i = 0 for i in 123. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"PDMO.jl includes algorithms to convert a graph into bipartite form using a key operation called edge subdivision, which replaces an edge by a path of length 2. More specificly, for an edge e with endpoints denoted by x_i and x_j, introduce a new node, delete the original e, and connect the new node with with n_i and n_j respectively. Continuing with the previous example, we can remove the edge between C and x_2, introduc a new node in green, and this new node with C and x_2 respectively. The resulting graph becomes bipartite. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"<div style=\"text-align: center; margin: 20px 0;\">\n  <svg width=\"500\" height=\"350\" viewBox=\"0 0 500 350\">\n    <!-- Left Partition -->\n    <text x=\"80\" y=\"320\" text-anchor=\"middle\" font-size=\"16\" font-weight=\"bold\">Left Partition</text>\n    <circle cx=\"80\" cy=\"60\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"80\" y=\"65\" text-anchor=\"middle\" font-size=\"16\">x₁</text>\n    <circle cx=\"80\" cy=\"150\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"80\" y=\"155\" text-anchor=\"middle\" font-size=\"16\">x₃</text>\n    <circle cx=\"80\" cy=\"240\" r=\"25\" fill=\"#90EE90\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"80\" y=\"245\" text-anchor=\"middle\" font-size=\"12\">C-x₂</text>\n    \n    <!-- Right Partition -->\n    <text x=\"420\" y=\"320\" text-anchor=\"middle\" font-size=\"16\" font-weight=\"bold\">Right Partition</text>\n    <circle cx=\"420\" cy=\"60\" r=\"25\" fill=\"#ADD8E6\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"420\" y=\"65\" text-anchor=\"middle\" font-size=\"16\">x₂</text>\n    <circle cx=\"420\" cy=\"150\" r=\"25\" fill=\"#FFA500\" stroke=\"#000\" stroke-width=\"2\"/>\n    <text x=\"420\" y=\"155\" text-anchor=\"middle\" font-size=\"16\">C</text>\n    \n    <!-- Edges -->\n    <line x1=\"105\" y1=\"65\" x2=\"395\" y2=\"65\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"105\" y1=\"65\" x2=\"395\" y2=\"155\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"105\" y1=\"155\" x2=\"395\" y2=\"65\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"105\" y1=\"155\" x2=\"395\" y2=\"155\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"105\" y1=\"245\" x2=\"395\" y2=\"65\" stroke=\"#000\" stroke-width=\"2\"/>\n    <line x1=\"105\" y1=\"245\" x2=\"395\" y2=\"155\" stroke=\"#000\" stroke-width=\"2\"/>\n    \n    <!-- Partition boundary -->\n    <line x1=\"250\" y1=\"40\" x2=\"250\" y2=\"290\" stroke=\"#888\" stroke-width=\"3\" stroke-dasharray=\"10,5\"/>\n  </svg>\n</div>","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The newly introduce node (in green) also introduce a new BlockVariable with a proximal-friendly function g: the indicator function of the set of a free variable z, which has the same shape as A_2x_2 and y_2. The previous edge between x_2 and C, representing A_2x_2 - y_2 = 0, is replaced by two new edges: the one between x_2 and the new node represents A_2x_2-z = 0, and the other one between C and the new node represents y_2-z=0. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The edge subdivision operation essentially introduces auxiliary variables to break complicated couplings between variables and constraints. The goal is to construct an equivalent reformulation of the original problem, where all BlockVariables can be assigned either to the left or to the right, and each constraint couples exactly one BlockVariable on the left, and another on the right. Currently PDMO.jl supports the following bipartization algorithms based on edge splitting. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Algorithm Procedure\nMILP_BIPARTIZATION (1) Formulate as MILP with binary variables (2) Add constraints for valid bipartition (3) Minimize operator norms and graph complexity\nBFS_BIPARTIZATION (1) Traverse graph level-by-level using BFS (2) Assign nodes to alternating partitions (3) Split edges when conflicts detected\nDFS_BIPARTIZATION (1) Traverse graph depth-first using DFS (2) Assign nodes to alternating partitions (3) Split edges when conflicts detected\nSPANNING_TREE_BIPARTIZATION (1) Construct spanning tree (2) 2-color the spanning tree (3) Split back edges only if endpoints have same partition","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The specific bipartization algorithm can be selected as a keyword argument to the function runBipartiteADMM via ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"# suppose the following arguments have been constructed: \n#  1. mbp: a MultiblockProblem of interest\n#  2. param: user-specified ADMM parameters\nresult = runBipartiteADMM(mbp, param;\n    bipartizationAlgorithm = BFS_BIPARTIZATION\n    # or bipartizationAlgorithm = MILP_BIPARTIZATION\n    # or bipartizationAlgorithm = DFS_BIPARTIZATION\n    # or bipartizationAlgorithm = SPANNING_TREE_BIPARTIZATION\n)","category":"page"},{"location":"S2_algorithms/ADMM/#ADMM-Algorithmic-Components","page":"ADMM","title":"ADMM Algorithmic Components","text":"","category":"section"},{"location":"S2_algorithms/ADMM/#Subproblem-Solvers","page":"ADMM","title":"Subproblem Solvers","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"PDMO.jl implements multiple subproblem solvers corresponding to different ADMM variants. ","category":"page"},{"location":"S2_algorithms/ADMM/#Original-ADMM-Subproblem-Solver-(OriginalADMMSubproblemSolver)","page":"ADMM","title":"Original ADMM Subproblem Solver (OriginalADMMSubproblemSolver)","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The OriginalADMMSubproblemSolver solves the ADMM subproblems exactly, i.e., without any linearization techniques, ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"  x^k+1 = argmin_x f(x) + g(x) + langle u^k Ax + By^k - crangle + fracrho2Ax+ By^k-c^2","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"using specialized methods based on automatic problem structure detection.","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.solver = OriginalADMMSubproblemSolver()","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"To handle diverse problem characteristics and computational requirements, OriginalADMMSubproblemSolver might further invoke different specialized solvers for different nodal subproblems: ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"LinearSolver: A linear solver based on Cholesky or LDL' decomposition for subproblems equivalent to linear systems\nProximalMappingSolver: A solver for subproblems reducible to proximal mappings of corresponding g\nJuMPSolver: A general-purpose solver using JuMP optimization modeling and Ipopt. Note: supports for function modeling in JuMP are limited at the moment and under active development. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"When a specialized solver cannot be determined for a nodal problem, PDMO.jl will switch to the doubly linearized solver. ","category":"page"},{"location":"S2_algorithms/ADMM/#Doubly-Linearized-Solver-(DoublyLinearizedSolver)","page":"ADMM","title":"Doubly Linearized Solver (DoublyLinearizedSolver)","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The DoublyLinearizedSolver implements the Doubly Linearized ADMM for updating primal variables: ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"beginaligned\nx^k+1 =  mathrmprox_alpha g(x^k - alpha (nabla f(x^k) + A^top u^k + rho A^top(Ax^k+By^k-c))) \nendaligned","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"where alpha 0 is a proximal coefficient. Since linearization is applied to the original smooth function f as well as the augmented Lagrangian terms, this update only invokes gradient oracles of f and proximal oracles of g. See Chapter 8 of Ryu and Yin [1] for details. The DoublyLinearizedSolver can be initialized as follows. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.solver = DoublyLinearizedSolver()","category":"page"},{"location":"S2_algorithms/ADMM/#Adaptive-Linearized-Solver-(AdaptiveLinearizedSolver)","page":"ADMM","title":"Adaptive Linearized Solver (AdaptiveLinearizedSolver)","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The AdaptiveLinearizedSolver is a newly developed method that combines linearization with adaptive step size mechanisms for robust performance. Details of this method will be released soon. The AdaptiveLinearizedSolver can be initialized as follows.","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.solver = AdaptiveLinearizedSolver()","category":"page"},{"location":"S2_algorithms/ADMM/#Penalty-Adapter","page":"ADMM","title":"Penalty Adapter","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Penalty parameter adapters dynamically adjust the penalty parameter rho during ADMM iterations to improve convergence. PDMO.jl currently implements two penalty adapters: ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Residual Balancing Adapter (RBAdapter): Balances primal and dual residuals by adjusting rho based on their ratio to improve convergence stability.","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam() \nparam.adapter = RBAdapter(\n  testRatio = 10.0,   # Threshold ratio for primal and dual residuals\n  adapterRatio = 2.0) # Factor by which to multiply/divide ρ ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Spectral Radius Approximation Adapter (SRAAdapter): Uses spectral analysis of iteration history to adaptively update rho based on convergence rate estimation. See [2] for more details. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam() \nparam.adapter = SRAAdapter(\n  T=5,                   # History length\n  increasingFactor=2.0,  # Factor by which to multiply ρ\n  decreasingFactor=2.0)  # Factor by which to divide ρ","category":"page"},{"location":"S2_algorithms/ADMM/#Accelerator","page":"ADMM","title":"Accelerator","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Acceleration schemes enhance ADMM convergence by exploiting iteration history. PDMO.jl currently implements two acceleraton schemes: ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Anderson Accelerator (AndersonAccelerator): See [3] for more details. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.accelerator = AndersonAccelerator() ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Auto Halpern Accelerator (AutoHalpernAccelerator): A newly developed restarted Halpern acceleration scheme for ADMM. Details will be released soon. ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.accelerator = AutoHalpernAccelerator()","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Note: Although the modular design enables flexible combination of algorithmic components, not all combinations of solver, adapter, and accelerator are equally effective. Some may result in slower convergence or numerical issues. Please experiment thoughtfully and validate performance for your specific use case.","category":"page"},{"location":"S2_algorithms/ADMM/#Termination-Criteria","page":"ADMM","title":"Termination Criteria","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"PDMO.jl implements comprehensive termination criteria with multiple levels:","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Level 1: Basic Termination","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Optimality: Primal and dual residuals satisfy tolerances\nIteration limit: Maximum iterations reached\nTime limit: Wall-clock time limit exceeded\nNumerical errors: NaN or Inf values detected","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Level 2: Advanced Problem Classification (Under active development) ","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Infeasibility detection: Problem has no feasible solution\nUnboundedness detection: Objective function is unbounded below\nIll-posed problem detection: Problem is weakly infeasible, or has non-zero duality gap","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"The system automatically classifies problems into categories (Case A-F) based on convergence behavior patterns, helping users understand why ADMM may not converge on certain problem instances.","category":"page"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"param = ADMMParam()\nparam.enablePathologyCheck = true  # Enable advanced problem classification\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S2_algorithms/ADMM/#References","page":"ADMM","title":"References","text":"","category":"section"},{"location":"S2_algorithms/ADMM/","page":"ADMM","title":"ADMM","text":"Ryu, E. K., & Yin, W. (2022). Large-scale convex optimization: algorithms & analyses via monotone operators. Cambridge University Press.\nMcCann, M. T., & Wohlberg, B. (2024). Robust and Simple ADMM Penalty Parameter Selection. IEEE Open Journal of Signal Processing, 5, 402-420.\nPollock, S., & Rebholz, L. G. (2023). Filtering for Anderson acceleration. SIAM Journal on Scientific Computing, 45(4), A1571-A1590.","category":"page"},{"location":"S4_api/mappings/#Mappings-API","page":"Mappings","title":"Mappings API","text":"","category":"section"},{"location":"S4_api/mappings/","page":"Mappings","title":"Mappings","text":"This document describes the mappings available in the optimization framework.","category":"page"},{"location":"S4_api/mappings/#Abstract-Mapping-Interface","page":"Mappings","title":"Abstract Mapping Interface","text":"","category":"section"},{"location":"S4_api/mappings/#PDMO.AbstractMapping","page":"Mappings","title":"PDMO.AbstractMapping","text":"AbstractMapping\n\nThis module defines the abstract interface for mappings in the optimization framework.\n\nMappings represent linear operators that transform variables from one space to another. They are a fundamental component used in formulating and solving optimization problems.\n\nInterface Functions\n\nAll concrete mapping types should implement:\n\nForward operator: (L::ConcreteMapping)(x, ret, add=false) - In-place application\nForward operator: (L::ConcreteMapping)(x) - Out-of-place application\nAdjoint operator: adjoint!(L, y, ret, add=false) - In-place adjoint application\nAdjoint operator: adjoint(L, y) - Out-of-place adjoint application\ncreateAdjointMapping(L) - Creates a new mapping that represents the adjoint\noperatorNorm2(L) - Computes the operator norm (largest singular value)\n\nAvailable Mappings\n\nNullMapping: A placeholder mapping that does nothing\nLinearMappingIdentity: A scaled identity mapping\nLinearMappingExtraction: Extracts a subset of elements from input array\nLinearMappingMatrix: Matrix-based linear mapping\n\nExamples\n\n# Create an identity mapping with coefficient 2.0\nL = LinearMappingIdentity(2.0)\n\n# Apply the mapping to a vector\nx = [1.0, 2.0, 3.0]\ny = L(x)  # Returns [2.0, 4.0, 6.0]\n\n# Create the adjoint mapping\nL_adj = createAdjointMapping(L)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/mappings/#Interface-Functions","page":"Mappings","title":"Interface Functions","text":"","category":"section"},{"location":"S4_api/mappings/","page":"Mappings","title":"Mappings","text":"All mapping types implement function call operators:","category":"page"},{"location":"S4_api/mappings/","page":"Mappings","title":"Mappings","text":"(L::ConcreteMapping)(x::NumericVariable) - Out-of-place application\n(L::ConcreteMapping)(x::NumericVariable, ret::NumericVariable, add::Bool = false) - In-place application","category":"page"},{"location":"S4_api/mappings/","page":"Mappings","title":"Mappings","text":"All mapping types implement adjoint operators that extend Julia's adjoint and adjoint! functions:","category":"page"},{"location":"S4_api/mappings/","page":"Mappings","title":"Mappings","text":"adjoint!(L::ConcreteMapping, y::NumericVariable, ret::NumericVariable, add::Bool = false) - In-place adjoint\nadjoint(L::ConcreteMapping, y::NumericVariable) - Out-of-place adjoint","category":"page"},{"location":"S4_api/mappings/#PDMO.operatorNorm2","page":"Mappings","title":"PDMO.operatorNorm2","text":"operatorNorm2(L::AbstractMapping)\n\nCompute the operator norm  of the mapping.\n\nReturns\n\nFloat64: The operator norm of the mapping\n\n\n\n\n\noperatorNorm2(L::LinearMappingIdentity)\n\nCompute the operator norm (largest singular value) of the scaled identity mapping.\n\nArguments\n\nL::LinearMappingIdentity: The mapping for which to compute the operator norm.\n\nReturns\n\nThe absolute value of the scaling coefficient coe.\n\nImplementation Details\n\nFor a scaled identity mapping, the operator norm is simply the absolute value of the scaling coefficient, since all singular values are equal to this value.\n\n\n\n\n\noperatorNorm2(L::LinearMappingExtraction)\n\nCompute the operator norm (largest singular value) of the extraction mapping.\n\nReturns\n\nThe absolute value of the scaling coefficient coe.\n\nImplementation Details\n\nFor an extraction mapping, the operator norm is simply the absolute value of the scaling coefficient, since the mapping only scales the extracted values.\n\n\n\n\n\noperatorNorm2(L::LinearMappingMatrix)\n\nCompute the operator norm (largest singular value) of the matrix-based linear mapping.\n\nArguments\n\nL::LinearMappingMatrix: The mapping for which to compute the operator norm.\n\nReturns\n\nThe operator norm of the matrix L.A. For small matrices, this is the exact value. For large matrices, this is a fast upper bound estimate.\n\nImplementation Details\n\nUses a hybrid approach based on matrix size:\n\nFor small matrices (nnz < 10,000): Uses Arpack.svds for exact computation\nFor large matrices: Uses sqrt(||A||1 * ||A||∞) as a fast, robust upper bound\n\nThis approach provides accuracy for small problems while avoiding convergence  issues for large sparse matrices.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/mappings/#Basic-Mapping-Types","page":"Mappings","title":"Basic Mapping Types","text":"","category":"section"},{"location":"S4_api/mappings/#NullMapping","page":"Mappings","title":"NullMapping","text":"","category":"section"},{"location":"S4_api/mappings/#PDMO.NullMapping","page":"Mappings","title":"PDMO.NullMapping","text":"NullMapping <: AbstractMapping\n\nA null mapping that does nothing. Placeholder in SolverData. \n\n\n\n\n\n","category":"type"},{"location":"S4_api/mappings/#LinearMappingIdentity","page":"Mappings","title":"LinearMappingIdentity","text":"","category":"section"},{"location":"S4_api/mappings/#PDMO.LinearMappingIdentity","page":"Mappings","title":"PDMO.LinearMappingIdentity","text":"LinearMappingIdentity(coe::Float64=1.0)\n\nConstructs a linear mapping that applies a scaled identity transformation.\n\nThe mapping is defined as:\n\ny = coe * x\n\nwhere both the input and output have the same dimension.\n\nArguments\n\ncoe::Float64: The scaling coefficient (must be nonzero). Defaults to 1.0.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/mappings/#LinearMappingExtraction","page":"Mappings","title":"LinearMappingExtraction","text":"","category":"section"},{"location":"S4_api/mappings/#PDMO.LinearMappingExtraction","page":"Mappings","title":"PDMO.LinearMappingExtraction","text":"LinearMappingExtraction(dim::Tuple, coe::Float64, indexStart::Int64, indexEnd::Int64)\n\nConstructs a linear mapping that extracts a subset of the input array along the first dimension and applies a scaling coefficient. The mapping is defined as:\n\nL(x) = coe * x[indexStart:indexEnd, :, ...]\n\nwhere dim specifies the dimensions of the input array.\n\nArguments\n\ndim::Tuple: The dimensions of the full input array.\ncoe::Float64: The scaling coefficient.\nindexStart::Int64: The starting index for extraction along the first dimension.\nindexEnd::Int64: The ending index for extraction along the first dimension.\n\nAn assertion is made that 1 ≤ indexStart ≤ indexEnd ≤ dim[1].\n\nFor example, suppose we have the following constraint in the original problem:\n\nA1(x1) + A2(x2) + A3(x3) = b,\n\nwhere x1, x2, x3 are the variables, the pipeline will break the constriant into \n\nA1(x1) - z1 = 0 \nA2(x2) - z2 = 0 \nA3(x3) - z3 = 0\nz in {[z1,z2,z3]: z1 + z2 + z3 = b}\n\nThen this mapping extracts zi from z, i.e., Ai(xi) + Li(z) = 0, where Li(z) = -zi \n\n\n\n\n\n","category":"type"},{"location":"S4_api/mappings/#LinearMappingMatrix","page":"Mappings","title":"LinearMappingMatrix","text":"","category":"section"},{"location":"S4_api/mappings/#PDMO.LinearMappingMatrix","page":"Mappings","title":"PDMO.LinearMappingMatrix","text":"LinearMappingMatrix(A::SparseMatrixCSC{Float64, Int64})\n\nConstructs a linear mapping defined by a sparse matrix A.\n\nThe mapping is defined as:\n\ny = A * x\n\nwhere x can be a vector or a matrix. If x is a matrix, the mapping is applied to each column.\n\nFields\n\nA: A SparseMatrixCSC{Float64, Int64} representing the linear operator.\ninputDim: The number of columns of A (the dimension of input vectors).\noutputDim: The number of rows of A (the dimension of output vectors).\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#AdaPDM","page":"AdaPDM","title":"AdaPDM","text":"","category":"section"},{"location":"S4_api/pdm/","page":"AdaPDM","title":"AdaPDM","text":"This page documents the Adaptive Primal-Dual Method (AdaPDM) algorithm components in PDMO.jl.","category":"page"},{"location":"S4_api/pdm/#Parameters-and-Iteration-Information","page":"AdaPDM","title":"Parameters and Iteration Information","text":"","category":"section"},{"location":"S4_api/pdm/#PDMO.AbstractAdaPDMParam","page":"AdaPDM","title":"PDMO.AbstractAdaPDMParam","text":"AbstractAdaPDMParam\n\nAbstract base type for AdaPDM algorithm parameters.\n\nAll concrete AdaPDM parameter types should inherit from this abstract type.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.AdaPDMParam","page":"AdaPDM","title":"PDMO.AdaPDMParam","text":"AdaPDMParam <: AbstractAdaPDMParam\n\nParameters for the Adaptive Primal-Dual Method.\n\nFields\n\nt::Float64: Primal/dual step size ratio\nstepSizeEpsilon::Float64: Step size parameter epsilon, i.e., 1e-6 in the paper\nstepSizeNu::Float64: Step size parameter nu, i.e., 1.2 in the paper\ninitialGamma::Float64: Initial gamma, i.e., gamma_0\ninitialGammaPrev::Float64: Initial gamma prev, i.e., gamma_{-1}\ninitialNormEstimate::Float64: ||A|| or its estimate eta_0\npresTolL2::Float64: Primal residual tolerance in L2 norm\ndresTolL2::Float64: Dual residual tolerance in L2 norm\npresTolLInf::Float64: Primal residual tolerance in LInf norm\ndresTolLInf::Float64: Dual residual tolerance in LInf norm\nmaxIter::Int64: Maximum number of iterations\nlogInterval::Int64: Interval for logging information\ntimeLimit::Float64: Time limit in seconds\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.AdaPDMPlusParam","page":"AdaPDM","title":"PDMO.AdaPDMPlusParam","text":"AdaPDMPlusParam <: AbstractAdaPDMParam\n\nParameters for the Adaptive Primal-Dual Method Plus, which extends the basic AdaPDM with additional  parameters for line search and operator norm estimation.\n\nFields\n\nt::Float64: Primal/dual step size ratio\nstepSizeEpsilon::Float64: Step size parameter epsilon, i.e., 1e-6 in the paper\nstepSizeNu::Float64: Step size parameter nu, i.e., 1.2 in the paper\ninitialGamma::Float64: Initial gamma, i.e., gamma_0\ninitialGammaPrev::Float64: Initial gamma prev, i.e., gamma_{-1}\ninitialNormEstimate::Float64: ||A|| or its estimate eta_0\nbacktrackingFactor::Float64: Backtracking factor for linesearch, i.e., r = 2.0 in the paper\nnormEstimateShrinkingFactor::Float64: Shrinking factor for sequence of eta, i.e., 0.95 in the paper\npresTolL2::Float64: Primal residual tolerance in L2 norm\ndresTolL2::Float64: Dual residual tolerance in L2 norm\npresTolLInf::Float64: Primal residual tolerance in LInf norm\ndresTolLInf::Float64: Dual residual tolerance in LInf norm\nmaxIter::Int64: Maximum number of iterations\nlogInterval::Int64: Interval for logging information\ntimeLimit::Float64: Time limit in seconds\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.MalitskyPockParam","page":"AdaPDM","title":"PDMO.MalitskyPockParam","text":"MalitskyPockParam <: AbstractAdaPDMParam\n\nParameters for the Malitsky-Pock primal-dual algorithm with adaptive step sizes.\n\nThe Malitsky-Pock algorithm is an adaptive primal-dual method that automatically adjusts step sizes during iteration using backtracking line search. It is designed for solving composite optimization problems of the form: minimize f(x) + g(Ax), where f and g are convex functions and A is a linear operator.\n\nFields\n\nAlgorithm Parameters\n\ninitialTheta::Float64: Initial primal step size ratio (θ₀)\ninitialSigma::Float64: Initial dual step size (σ₀)\nbacktrackDescentRatio::Float64: Backtracking descent ratio ∈ (0,1)\nt::Float64: Ratio parameter for primal-dual step size relationship\nmu::Float64: Backtracking parameter ∈ (0,1) for step size reduction\n\nConvergence Tolerances\n\npresTolL2::Float64: Primal residual tolerance in L2 norm\ndresTolL2::Float64: Dual residual tolerance in L2 norm\npresTolLInf::Float64: Primal residual tolerance in L∞ norm\ndresTolLInf::Float64: Dual residual tolerance in L∞ norm\n\nIteration Control\n\nlineSearchMaxIter::Int64: Maximum iterations for backtracking line search\nmaxIter::Int64: Maximum number of algorithm iterations\nlogInterval::Int64: Interval for logging algorithm progress\ntimeLimit::Float64: Time limit in seconds\n\nAlgorithm Theory\n\nThe Malitsky-Pock algorithm uses adaptive step sizes with the following key features:\n\nAdaptive Step Sizes: Automatically adjusts θₖ and σₖ based on progress\nBacktracking Line Search: Ensures sufficient decrease in objective\nConvergence Guarantee: Proven convergence without knowing operator norms\n\nThe algorithm updates follow:\n\nx̄ᵏ = xᵏ + θₖ(xᵏ - xᵏ⁻¹)  \nyᵏ⁺¹ = prox_{σₖg*}(yᵏ + σₖAx̄ᵏ)\nxᵏ⁺¹ = prox_{τₖf}(xᵏ - τₖA^Tyᵏ⁺¹)\n\nWhere step sizes are adapted based on backtracking conditions.\n\nAdvantages\n\nParameter-Free: No need to estimate operator norms or Lipschitz constants\nRobust: Adaptive step sizes handle problem conditioning automatically\nEfficient: Often converges faster than fixed step size methods\n\nReferences\n\nMalitsky, Y., & Pock, T. (2018). A first-order primal-dual algorithm with linesearch\nChambolle, A., & Pock, T. (2011). A first-order primal-dual algorithm for convex problems\n\nSee also: CondatVuParam, AdaPDMParam, runAdaPDM\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.CondatVuParam","page":"AdaPDM","title":"PDMO.CondatVuParam","text":"CondatVuParam <: AbstractAdaPDMParam\n\nParameters for the Condat-Vũ primal-dual algorithm.\n\nThe Condat-Vũ algorithm is a primal-dual method for solving composite optimization problems of the form: minimize f(x) + g(Ax), where f is smooth and g is proximable. It is particularly well-suited for problems where the linear operator A has a known operator norm.\n\nFields\n\nAlgorithm Parameters\n\nprimalStepSize::Float64: Step size for primal variable updates (α)\ndualStepSize::Float64: Step size for dual variable updates (β)  \nopNormEstimate::Float64: Estimate of the operator norm ||A||\nLipschitzConstantEstimate::Float64: Estimate of the Lipschitz constant of ∇f\n\nConvergence Tolerances\n\npresTolL2::Float64: Primal residual tolerance in L2 norm\ndresTolL2::Float64: Dual residual tolerance in L2 norm\npresTolLInf::Float64: Primal residual tolerance in L∞ norm\ndresTolLInf::Float64: Dual residual tolerance in L∞ norm\n\nIteration Control\n\nmaxIter::Int64: Maximum number of iterations\nlogInterval::Int64: Interval for logging algorithm progress\ntimeLimit::Float64: Time limit in seconds\n\nAlgorithm Theory\n\nThe Condat-Vũ algorithm uses the following update scheme:\n\nx^{k+1} = prox_{αf}(x^k - α A^T y^k)\ny^{k+1} = prox_{βg*}(y^k + β A(2x^{k+1} - x^k))\n\nThe step sizes must satisfy: αβ||A||² < 1 for convergence.\n\nReferences\n\nCondat, L. (2013). A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms\nVũ, B. C. (2013). A splitting algorithm for dual monotone inclusions involving cocoercive operators\n\nSee also: AdaPDMParam, MalitskyPockParam, runAdaPDM\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.AdaPDMIterationInfo","page":"AdaPDM","title":"PDMO.AdaPDMIterationInfo","text":"AdaPDMIterationInfo\n\nStructure to store and track the information about the iterations of the Adaptive Primal-Dual Method.\n\nFields\n\npresL2::Vector{Float64}: Primal residuals in L2 norm at each iteration\ndresL2::Vector{Float64}: Dual residuals in L2 norm at each iteration\npresLInf::Vector{Float64}: Primal residuals in L-infinity norm at each iteration\ndresLInf::Vector{Float64}: Dual residuals in L-infinity norm at each iteration\nlagrangianObj::Vector{Float64}: Lagrangian objective values at each iteration\nnumberBacktracks::Vector{Int64}: Number of backtracks in the linesearch for each iteration \nprimalSol::Dict{String, NumericVariable}: Current primal solution (x^{k+1})\nprimalSolPrev::Dict{String, NumericVariable}: Previous primal solution (x^k)\nprimalBuffer1::Dict{String, NumericVariable}: Buffer for primal variable computations\nprimalBuffer2::Dict{String, NumericVariable}: Additional buffer for primal variable computations\nlineSearchPrimalBuffer::Dict{String, NumericVariable}: Buffer for line search operations\ndualSol::NumericVariable: Current dual solution (y^{k+1})\ndualSolPrev::NumericVariable: Previous dual solution (y^k)\nbufferAx::NumericVariable: Buffer for Ax^{k+1}\nbufferAxPrev::NumericVariable: Buffer for Ax^k\nlineSearchDualBuffer::NumericVariable: Buffer for line search operations\ndualBuffer::NumericVariable: Buffer for dual variable computations\nprimalStepSize::Float64: Current primal step size (gamma_{k+1})\nprimalStepSizePrev::Float64: Previous primal step size (gamma_k)\ndualStepSize::Float64: Current dual step size (sigma_{k+1})\nopNormEstimate::Float64: Estimate of the operator norm\nstopIter::Int64: Iteration at which the algorithm stopped\ntotalTime::Float64: Total execution time\nterminationStatus::AdaPDMTerminationStatus: Status indicating how the algorithm terminated\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.AdaPDMTerminationStatus","page":"AdaPDM","title":"PDMO.AdaPDMTerminationStatus","text":"AdaPDMTerminationStatus\n\nEnum representing the termination status of the Adaptive Primal-Dual Method.\n\nValues\n\nADA_PDM_TERMINATION_UNSPECIFIED: Termination status not yet determined\nADA_PDM_TERMINATION_OPTIMAL: Converged to optimal solution\nADA_PDM_TERMINATION_ITERATION_LIMIT: Reached maximum number of iterations\nADA_PDM_TERMINATION_TIME_LIMIT: Reached time limit\nADA_PDM_TERMINATION_UNBOUNDED: Problem is unbounded\nADA_PDM_TERMINATION_UNKNOWN: Terminated for unknown reason\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#Utility-Functions","page":"AdaPDM","title":"Utility Functions","text":"","category":"section"},{"location":"S4_api/pdm/#PDMO.computeNormEstimate","page":"AdaPDM","title":"PDMO.computeNormEstimate","text":"computeNormEstimate(mbp::MultiblockProblem)\n\nCompute the operator norm estimate for the given multiblock problem.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem to compute the operator norm estimate for.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.getAdaPDMName","page":"AdaPDM","title":"PDMO.getAdaPDMName","text":"getAdaPDMName(param::AbstractAdaPDMParam)\n\nGet the name of the AdaPDM algorithm based on the parameter type.\n\nArguments\n\nparam::AbstractAdaPDMParam: The parameter object to get the name for.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#Update-Functions","page":"AdaPDM","title":"Update Functions","text":"","category":"section"},{"location":"S4_api/pdm/","page":"AdaPDM","title":"AdaPDM","text":"Algorithm-specific update functions for dual and primal solutions.","category":"page"},{"location":"S4_api/pdm/#PDMO.updateDualSolution!","page":"AdaPDM","title":"PDMO.updateDualSolution!","text":"updateDualSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::AdaPDMParam)\n\nUpdate the dual solution and step sizes for the standard Adaptive Primal-Dual Method.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\nparam::AdaPDMParam: The parameters for the AdaPDM algorithm\n\nDetails\n\nThis function:\n\nComputes Lipschitz and cocoercivity estimates based on current iterates\nCalculates the new primal step size (gamma) using three different criteria and takes the minimum\nComputes the dual step size (sigma) based on the primal step size and the parameter t\nUpdates the proximal center for the conjugate proximal oracle\nPerforms the proximal oracle step to compute the new dual solution\nUpdates the step size information in the iteration info object\n\nThe new gamma is computed as the minimum of:\n\nv1: Controlled increase based on previous step size\nv2: Based on operator norm estimate\nv3: Based on parameter delta, xi, and epsilon calculations\n\n\n\n\n\nupdateDualSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::AdaPDMPlusParam)\n\nUpdate the dual solution and step sizes for the Adaptive Primal-Dual Method Plus, which includes adaptive operator norm estimation and backtracking line search.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\nparam::AdaPDMPlusParam: The parameters for the AdaPDMPlus algorithm\n\nDetails\n\nThis function:\n\nComputes Lipschitz and cocoercivity estimates based on current iterates\nPerforms an adaptive line search to find an appropriate operator norm estimate:\nStarts with a reduced operator norm estimate from the previous iteration\nCalculates a new primal step size based on this estimate\nChecks if the estimate satisfies a specific criterion involving the dual iterates\nIf not, increases the estimate and repeats\nUpdates the dual solution using the proximal oracle with the computed step size\nUpdates the step size and operator norm estimate information in the iteration info object\n\nThe line search helps ensure robustness by adaptively finding appropriate step sizes that maintain algorithm stability while potentially allowing larger steps than the standard AdaPDM algorithm.\n\n\n\n\n\nupdateDualSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::MalitskyPockParam)\n\nUpdates the dual solution in the Malitsky-Pock algorithm. This function:\n\nPrepares the proximal center for the conjugate proximal oracle\nStores the current dual solution in the previous dual solution buffer\nApplies the proximal oracle of the conjugate function to compute the new dual solution\n\n\n\n\n\nupdateDualSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::CondatVuParam)\n\nUpdates the dual solution in the Condat-Vu algorithm. This function:\n\nPrepares the proximal center for the conjugate proximal oracle\nStores the current dual solution in the previous dual solution buffer\nApplies the proximal oracle of the conjugate function to compute the new dual solution\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.updatePrimalSolution!","page":"AdaPDM","title":"PDMO.updatePrimalSolution!","text":"updatePrimalSolution!(block::BlockVariable, mbp::MultiblockProblem, info::AdaPDMIterationInfo)\n\nUpdate the primal solution for a specific block using the proximal oracle.\n\nArguments\n\nblock::BlockVariable: The block variable to update\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\n\nDetails\n\nThis function:\n\nComputes the gradient of the objective function at the current solution\nPrepares the proximal center by applying the gradient and adjoint operators\nSaves the current solution as the previous solution\nUpdates the primal solution using the proximal oracle\n\nThe function is used by both AdaPDM and AdaPDMPlus algorithms to update individual block variables.\n\n\n\n\n\nupdatePrimalSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::AdaPDMParam)\n\nUpdate the primal solutions for all blocks in the multiblock problem using the proximal oracle.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\nparam::AdaPDMParam: The parameters for the AdaPDM algorithm\n\nDetails\n\nThis function updates the primal solution for each block in the multiblock problem by calling the updatePrimalSolution! function for each block. The function iterates over all blocks except the last one, which corresponds to the dual variable.\n\n\n\n\n\nupdatePrimalSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::AdaPDMPlusParam)\n\nUpdate the primal solutions for all blocks in the multiblock problem using the proximal oracle.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\nparam::AdaPDMPlusParam: The parameters for the AdaPDMPlus algorithm\n\nDetails\n\nThis function updates the primal solution for each block in the multiblock problem by calling the updatePrimalSolution! function for each block. The function iterates over all blocks except the last one, which corresponds to the dual variable.\n\n\n\n\n\nupdatePrimalSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::MalitskyPockParam)\n\nUpdates the primal solution in the Malitsky-Pock algorithm. This function:\n\nComputes the new step sizes based on the adaptive scheme\nComputes gradients for each primal block\nPerforms line search to find suitable step sizes satisfying the descent condition\nUpdates the primal and dual step sizes and solutions\n\nThe line search is based on a sufficient decrease condition that balances the change in objective value with the squared norm of the primal variable difference.\n\n\n\n\n\nupdatePrimalSolution!(mbp::MultiblockProblem, info::AdaPDMIterationInfo, param::CondatVuParam)\n\nUpdates the primal solution in the Condat-Vu algorithm. This function:\n\nComputes the new step sizes based on the adaptive scheme\nComputes gradients for each primal block\nPerforms line search to find suitable step sizes satisfying the descent condition\nUpdates the primal and dual step sizes and solutions\n\nThe line search is based on a sufficient decrease condition that balances the change in objective value with the squared norm of the primal variable difference.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.setupInitialPrimalDualStepSize!","page":"AdaPDM","title":"PDMO.setupInitialPrimalDualStepSize!","text":"setupInitialPrimalDualStepSize!(info::AdaPDMIterationInfo, param::AdaPDMParam)\n\nSetup the initial primal and dual step sizes for the AdaPDM algorithm.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object to initialize\nparam::AdaPDMParam: The parameters for the AdaPDM algorithm\n\nDetails\n\nThis function initializes the primal step size to the initial gamma value from the parameters, the previous primal step size to the initial gamma previous value, and sets the dual step size to zero. These values are used in the first iteration of the algorithm before adaptive step sizes are computed.\n\n\n\n\n\nsetupInitialPrimalDualStepSize!(info::AdaPDMIterationInfo, param::AdaPDMPlusParam)\n\nSetup the initial primal and dual step sizes for the AdaPDMPlus algorithm.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object to initialize\nparam::AdaPDMPlusParam: The parameters for the AdaPDMPlus algorithm\n\nDetails\n\nThis function initializes the primal step size to the initial gamma value from the parameters, the previous primal step size to the initial gamma previous value, and sets the dual step size to zero. These values are used in the first iteration of the algorithm before adaptive step sizes are computed.\n\n\n\n\n\nsetupInitialPrimalDualStepSize!(info::AdaPDMIterationInfo, param::MalitskyPockParam)\n\nInitializes the primal and dual step sizes for the Malitsky-Pock algorithm. The primal step size is initially set to 0, while the dual step size is set based on  the parameters provided in param. The previous dual step size is set as a fraction  of the initial dual step size, controlled by the initialTheta parameter.\n\n\n\n\n\nsetupInitialPrimalDualStepSize!(info::AdaPDMIterationInfo, param::CondatVuParam)\n\nInitializes the primal and dual step sizes for the Condat-Vu algorithm. The primal step size is initially set to the primal step size provided in param, while the dual step size is set to the dual step size provided in param. The previous primal and dual step sizes are set to the initial values.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#Iteration-Utilities","page":"AdaPDM","title":"Iteration Utilities","text":"","category":"section"},{"location":"S4_api/pdm/","page":"AdaPDM","title":"AdaPDM","text":"Common functions for iteration management and computations.","category":"page"},{"location":"S4_api/pdm/#PDMO.computeAdaPDMDualResiduals!","page":"AdaPDM","title":"PDMO.computeAdaPDMDualResiduals!","text":"computeAdaPDMDualResiduals!(info::AdaPDMIterationInfo, mbp::MultiblockProblem, param::AbstractAdaPDMParam)\n\nCompute the dual residuals for the Adaptive Primal-Dual Method.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object to update\nmbp::MultiblockProblem: The multiblock problem being solved\nparam::AbstractAdaPDMParam: Parameters for the algorithm\n\nDetails\n\nThis function computes the dual residuals by:\n\nFor Malitsky-Pock algorithm, calculating y^{k+1} - bar{y}^{k+1}\nFor each block, computing gradient differences and step-size adjusted variable differences\nFor Malitsky-Pock, applying the adjoint mapping\nComputing both L2 and L-infinity norms of the residuals\nStoring the computed residuals in the iteration info object\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.computeAdaPDMPrimalResiduals!","page":"AdaPDM","title":"PDMO.computeAdaPDMPrimalResiduals!","text":"computeAdaPDMPrimalResiduals!(info::AdaPDMIterationInfo, mbp::MultiblockProblem, param::AbstractAdaPDMParam)\n\nCompute the primal residuals for the Adaptive Primal-Dual Method.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object to update\nmbp::MultiblockProblem: The multiblock problem being solved\nparam::AbstractAdaPDMParam: Parameters for the algorithm\n\nDetails\n\nThis function:\n\nComputes Ax^{k+1} and stores it in dualBuffer\nCalculates Ax^k - Ax^{k+1}\nFor non-Malitsky-Pock algorithms, applies step size ratio adjustments\nComputes (1/sigma)*(y^k - y^{k+1}) + step-size adjusted differences\nCalculates both L2 and L-infinity norms of the residuals\nStores the computed residuals in the iteration info object\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.computePDMResidualsAndObjective!","page":"AdaPDM","title":"PDMO.computePDMResidualsAndObjective!","text":"computePDMResidualsAndObjective!(info::AdaPDMIterationInfo, mbp::MultiblockProblem, param::AbstractAdaPDMParam)\n\nCompute the primal and dual residuals and the objective value after an iteration.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object to update\nmbp::MultiblockProblem: The multiblock problem being solved\nparam::AbstractAdaPDMParam: Parameters for the algorithm\n\nDetails\n\nThis function:\n\nComputes and stores Ax^{k+1} in the buffer\nCalculates primal residuals by calling computeAdaPDMPrimalResiduals!\nComputes dual residuals by calling computeAdaPDMDualResiduals!\nCalculates the Lagrangian objective value\nUpdates the iteration info with all computed values\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.computePartialObjective!","page":"AdaPDM","title":"PDMO.computePartialObjective!","text":"computePartialObjective!(info::AdaPDMIterationInfo, mbp::MultiblockProblem)\n\nCompute the partial objective value f(x) + g(x) and update bufferAx with Ax.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration info object containing current solutions\nmbp::MultiblockProblem: The multiblock problem being solved\n\nReturns\n\nFloat64: The partial objective value\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.computeLipschitzAndCocoercivityEstimate","page":"AdaPDM","title":"PDMO.computeLipschitzAndCocoercivityEstimate","text":"computeLipschitzAndCocoercivityEstimate(mbp::MultiblockProblem, info::AdaPDMIterationInfo)\n\nCompute the Lipschitz and cocoercivity constants of the problem based on current iterations.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem being solved\ninfo::AdaPDMIterationInfo: The current iteration information\n\nReturns\n\nFloat64, Float64: Estimated Lipschitz constant and cocoercivity constant\n\nDetails\n\nThis function estimates the Lipschitz constant of the gradient and the cocoercivity constant by examining the difference between current and previous iterations.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.prepareProximalCenterForConjugateProximalOracle!","page":"AdaPDM","title":"PDMO.prepareProximalCenterForConjugateProximalOracle!","text":"prepareProximalCenterForConjugateProximalOracle!(info::AdaPDMIterationInfo, dualStepSize::Float64, ratio1::Float64, ratio2::Float64)\n\nPrepare the proximal center for the conjugate proximal oracle.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object\ndualStepSize::Float64: The dual step size (sigma)\nratio1::Float64: Coefficient for bufferAx (typically 1.0 + newGamma/gamma)\nratio2::Float64: Coefficient for bufferAxPrev (typically newGamma/gamma)\n\nDetails\n\nThis function prepares the proximal center for the conjugate proximal oracle by calculating:     dualBuffer <- y^{k} + dualStepSize * (ratio1 * Ax^{k+1} - ratio2 * Ax^k)\n\nThe result is stored in info.dualBuffer for subsequent use in the proximal oracle computation.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.prepareProximalCenterForPrimalUpdate!","page":"AdaPDM","title":"PDMO.prepareProximalCenterForPrimalUpdate!","text":"prepareProximalCenterForPrimalUpdate!(info::AdaPDMIterationInfo, blockID::BlockID, mbp::MultiblockProblem, primalStepSize::Float64, dualBuffer::NumericVariable, gradientBuffer::NumericVariable)\n\nPrepare the proximal center in primalBuffer1 for primal variable update.\n\nArguments\n\ninfo::AdaPDMIterationInfo: The iteration information object\nblockID::BlockID: The ID of the block to update\nmbp::MultiblockProblem: The multiblock problem being solved\nprimalStepSize::Float64: The primal step size (gamma)\ndualBuffer::NumericVariable: The dual variable to use (typically info.dualSol)\ngradientBuffer::NumericVariable: Buffer containing the gradient (typically info.primalBuffer2)\n\nDetails\n\nThis function prepares the proximal center for the primal update by calculating:     primalBuffer1 <- x^k - primalStepSize * (∇f(x^k) + A'y^k)\n\nThe result is stored in info.primalBuffer1[blockID] for subsequent use in the proximal oracle computation.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#Termination-Criteria","page":"AdaPDM","title":"Termination Criteria","text":"","category":"section"},{"location":"S4_api/pdm/","page":"AdaPDM","title":"AdaPDM","text":"Termination checking and status management.","category":"page"},{"location":"S4_api/pdm/#PDMO.AdaPDMTerminationCriteria","page":"AdaPDM","title":"PDMO.AdaPDMTerminationCriteria","text":"AdaPDMTerminationCriteria\n\nStructure containing termination criteria and status for the Adaptive Primal-Dual Method.\n\nThis structure defines the convergence tolerances and limits used to determine when the AdaPDM algorithm should terminate. It tracks both the criteria values and the current termination status.\n\nFields\n\npresTolL2::Float64: Primal residual tolerance in L2 norm\npresTolLInf::Float64: Primal residual tolerance in L∞ norm  \ndresTolL2::Float64: Dual residual tolerance in L2 norm\ndresTolLInf::Float64: Dual residual tolerance in L∞ norm\nmaxIter::Int64: Maximum number of iterations allowed\ntimeLimit::Float64: Maximum execution time in seconds\nterminated::Bool: Whether the algorithm has terminated\n\nTermination Conditions\n\nThe algorithm terminates when ANY of the following conditions is met:\n\nOptimal: All residuals fall below their respective tolerances\nIteration Limit: Maximum number of iterations reached\nTime Limit: Maximum execution time exceeded\nNumerical Error: NaN or Inf detected in residuals\nUnbounded: Problem is detected as unbounded (placeholder)\n\nConstructor\n\nAdaPDMTerminationCriteria(;\n    presTolL2::Float64 = 1e-4,\n    presTolLInf::Float64 = 1e-4,\n    dresTolL2::Float64 = 1e-4,\n    dresTolLInf::Float64 = 1e-4,\n    maxIter::Int64 = 1000,\n    timeLimit::Float64 = 3600.0,\n    terminated::Bool = false)\n\nSee also: AbstractAdaPDMParam, AdaPDMIterationInfo, checkTerminationCriteria\n\n\n\n\n\n","category":"type"},{"location":"S4_api/pdm/#PDMO.checkTerminationCriteria","page":"AdaPDM","title":"PDMO.checkTerminationCriteria","text":"checkTerminationCriteria(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria)\n\nCheck all termination criteria and update the termination status in the criteria object.\n\nThis function performs a comprehensive check of all possible termination conditions including:\n\nStandard optimality conditions (primal/dual residual tolerances)\nIteration and time limits  \nNumerical errors (NaN/Inf detection)\nAdvanced problem classification (infeasibility, unboundedness, ill-posed cases)\n\nThe function first updates termination metrics if available, then checks termination conditions in a specific order. If metrics are available, it also checks for advanced problem classifications (Cases B, C, D, E, F) that indicate various types of  problematic optimization problems.\n\nArguments\n\ninfo::ADMMIterationInfo: Current iteration information including residuals and timing\ncriteria::ADMMTerminationCriteria: Termination criteria object containing tolerances and metrics\n\nSide Effects\n\nUpdates criteria.terminated to true if any termination condition is met\nUpdates info.terminationStatus with the specific termination reason\nUpdates info.stopIter with the iteration number when termination occurs\nCalls updateTerminationMetrics! if metrics are available\n\nTermination Conditions Checked\n\nOptimal termination: All residuals satisfy tolerances\nIteration limit: Maximum iterations reached\nTime limit: Maximum time exceeded\nNumerical errors: NaN or Inf values detected\nBasic infeasibility/unboundedness: Stub checks (always false)\nAdvanced classification (if metrics available):\nCase F: Infeasible problem\nCase E: Lower unbounded problem  \nCase D: Lower bounded but ADMM may not be applicable\nCase C: Lower bounded but no optimal solution\nCase B: Has optimal solution but ADMM may not be suitable\n\nNotes\n\nThis function does not return anything - check criteria.terminated to determine if stopping\nAdvanced classification requires criteria.metrics to be non-null\nClassification is based on sophisticated convergence behavior analysis\n\n\n\n\n\ncheckTerminationCriteria(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck all termination criteria and update termination status.\n\nThis function systematically checks all possible termination conditions in order of priority and updates the algorithm status accordingly. It sets the stop iteration if any termination condition is met.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information to check\ncriteria::AdaPDMTerminationCriteria: Termination criteria and status\n\nTermination Check Order\n\nOptimal termination: All residuals below tolerances\nIteration limit: Maximum iterations reached\nTime limit: Maximum execution time exceeded\nNumerical errors: NaN or Inf detected in residuals\nUnboundedness: Problem detected as unbounded (placeholder)\n\nSide Effects\n\nUpdates info.terminationStatus based on termination reason\nSets criteria.terminated = true if any criterion is met\nSets info.stopIter to current iteration number upon termination\n\nUsage\n\n# Check termination after each iteration\ncheckTerminationCriteria(info, criteria)\nif criteria.terminated\n    break\nend\n\nSee also: checkOptimalTermination, checkIterationLimit, checkTimeLimit\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.checkOptimalTermination","page":"AdaPDM","title":"PDMO.checkOptimalTermination","text":"checkOptimalTermination(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria) -> Bool\n\nCheck if the algorithm should terminate because optimality criteria have been met. Updates the termination status in info and sets criteria.terminated = true if successful. Returns true if optimality criteria are met, false otherwise.\n\n\n\n\n\ncheckOptimalTermination(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck if the algorithm has reached optimal termination based on residual tolerances.\n\nThis function determines if all primal and dual residuals (in both L2 and L∞ norms) have fallen below their respective tolerance thresholds, indicating convergence.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information containing residuals\ncriteria::AdaPDMTerminationCriteria: Termination criteria containing tolerances\n\nReturns\n\nBool: true if optimal termination achieved, false otherwise\n\nSide Effects\n\nIf termination is achieved:\n\nSets info.terminationStatus to ADA_PDM_TERMINATION_OPTIMAL\nSets criteria.terminated to true\n\nTermination Condition\n\nReturns true when ALL of the following hold:\n\npresL2 ≤ presTolL2\npresLInf ≤ presTolLInf  \ndresL2 ≤ dresTolL2\ndresLInf ≤ dresTolLInf\n\nSee also: checkTerminationCriteria, AdaPDMTerminationStatus\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.checkIterationLimit","page":"AdaPDM","title":"PDMO.checkIterationLimit","text":"checkIterationLimit(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria) -> Bool\n\nCheck if the algorithm has reached the iteration limit. Updates the termination status in info and sets criteria.terminated = true if limit reached. Returns true if iteration limit reached, false otherwise.\n\n\n\n\n\ncheckIterationLimit(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck if the algorithm has reached the maximum iteration limit.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information\ncriteria::AdaPDMTerminationCriteria: Termination criteria containing maximum iterations\n\nReturns\n\nBool: true if iteration limit reached, false otherwise\n\nSide Effects\n\nIf iteration limit is reached:\n\nSets info.terminationStatus to ADA_PDM_TERMINATION_ITERATION_LIMIT\nSets criteria.terminated to true\n\nSee also: checkTerminationCriteria, AdaPDMTerminationStatus\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.checkTimeLimit","page":"AdaPDM","title":"PDMO.checkTimeLimit","text":"checkTimeLimit(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria) -> Bool\n\nCheck if the algorithm has reached the time limit. Updates the termination status in info and sets criteria.terminated = true if limit reached. Returns true if time limit reached, false otherwise.\n\n\n\n\n\ncheckTimeLimit(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck if the algorithm has exceeded the maximum execution time limit.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information containing total time\ncriteria::AdaPDMTerminationCriteria: Termination criteria containing time limit\n\nReturns\n\nBool: true if time limit exceeded, false otherwise\n\nSide Effects\n\nIf time limit is exceeded:\n\nSets info.terminationStatus to ADA_PDM_TERMINATION_TIME_LIMIT\nSets criteria.terminated to true\n\nSee also: checkTerminationCriteria, AdaPDMTerminationStatus\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.checkNumericalError","page":"AdaPDM","title":"PDMO.checkNumericalError","text":"checkNumericalError(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria) -> Bool\n\nCheck for numerical errors (NaN or Inf values) in the residuals. Updates the termination status in info and sets criteria.terminated = true if errors found. Returns true if numerical errors are detected, false otherwise.\n\n\n\n\n\ncheckNumericalError(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck if the algorithm has encountered numerical errors (NaN or Inf values).\n\nThis function detects numerical instability by checking if any of the residual values contain NaN or Inf, which typically indicates numerical breakdown.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information containing residuals\ncriteria::AdaPDMTerminationCriteria: Termination criteria object\n\nReturns\n\nBool: true if numerical errors detected, false otherwise\n\nSide Effects\n\nIf numerical errors are detected:\n\nSets info.terminationStatus to ADA_PDM_TERMINATION_UNKNOWN\nSets criteria.terminated to true\n\nChecked Values\n\nThe function checks for NaN or Inf in:\n\nPrimal residuals (L2 and L∞)\nDual residuals (L2 and L∞)\n\nSee also: checkTerminationCriteria, AdaPDMTerminationStatus\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.checkUnboundedness","page":"AdaPDM","title":"PDMO.checkUnboundedness","text":"checkUnboundedness(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria) -> Bool\n\nStub function for unboundedness checking without metrics.\n\nThis version always returns false as it doesn't have access to the advanced  metrics needed for unboundedness detection. The actual unboundedness detection is performed by the 3-argument version that takes metrics as input.\n\nArguments\n\ninfo::ADMMIterationInfo: Current iteration information\ncriteria::ADMMTerminationCriteria: Termination criteria object\n\nReturns\n\nBool: Always returns false (no unboundedness detection without metrics)\n\n\n\n\n\ncheckUnboundedness(info::ADMMIterationInfo, criteria::ADMMTerminationCriteria, metrics::ADMMTerminationMetrics) -> Bool\n\nCheck for unboundedness in the optimization problem using advanced metrics analysis.\n\nThis function detects if the optimization problem is unbounded below (Case E) based on the  convergence behavior analysis performed by the termination metrics. It checks if the problem has been classified as Case E, which indicates that the objective function can decrease without bound.\n\nArguments\n\ninfo::ADMMIterationInfo: Current iteration information\ncriteria::ADMMTerminationCriteria: Termination criteria object\nmetrics::ADMMTerminationMetrics: Metrics object containing problem classification\n\nReturns\n\nBool: true if unboundedness is detected, false otherwise\n\nSide Effects\n\nSets info.terminationStatus = ADMM_TERMINATION_UNBOUNDED if unbounded\nSets criteria.terminated = true if unbounded\n\nNotes\n\nOnly triggers if info.stopIter < 0 (algorithm hasn't terminated yet)\nBased on problem classification from convergence behavior analysis\nSpecifically detects lower unboundedness (Case E)\n\n\n\n\n\ncheckUnboundedness(info::AdaPDMIterationInfo, criteria::AdaPDMTerminationCriteria)\n\nCheck if the problem is unbounded (placeholder implementation).\n\nThis function is a placeholder for detecting unbounded problems. Currently always returns false as unboundedness detection is not implemented.\n\nArguments\n\ninfo::AdaPDMIterationInfo: Current iteration information\ncriteria::AdaPDMTerminationCriteria: Termination criteria object\n\nReturns\n\nBool: Always false (not implemented)\n\nNote\n\nThis is a placeholder for future implementation of unboundedness detection. Potential approaches include monitoring objective value divergence or variable magnitude growth.\n\nSee also: checkTerminationCriteria, AdaPDMTerminationStatus\n\n\n\n\n\n","category":"function"},{"location":"S4_api/pdm/#PDMO.getTerminationStatus","page":"AdaPDM","title":"PDMO.getTerminationStatus","text":"getTerminationStatus(status::ADMMTerminationStatus) -> String\n\nConvert an ADMMTerminationStatus enum value to a human-readable string description.\n\n\n\n\n\ngetTerminationStatus(status::AdaPDMTerminationStatus)\n\nConvert termination status enum to human-readable string.\n\nArguments\n\nstatus::AdaPDMTerminationStatus: Termination status enum value\n\nReturns\n\nString: Human-readable description of termination status\n\nSee also: AdaPDMTerminationStatus, AdaPDMIterationInfo\n\n\n\n\n\n","category":"function"},{"location":"S3_examples/LeastL1Norm/#Least-L1-Norm-Benchmark-Results","page":"Least L1 Norm","title":"Least L1 Norm Benchmark Results","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/#Problem-Formulation","page":"Least L1 Norm","title":"Problem Formulation","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"The Least L1 Norm problem aims to minimize the L1 norm of the residual:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"min_x Ax - b_1","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"where:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"A is an m × n matrix \nb is an m-dimensional vector\nx is the n-dimensional optimization variable","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"For ADMM decomposition, this is reformulated as a two-block problem:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"beginaligned\nmin_x z quad  z_1 \nmathrmst quad  Ax - z = b\nendaligned","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"This reformulation enables efficient distributed solving using bipartite ADMM.","category":"page"},{"location":"S3_examples/LeastL1Norm/#Implementation-Examples","page":"Least L1 Norm","title":"Implementation Examples","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/#JuMP-Implementation","page":"Least L1 Norm","title":"JuMP Implementation","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Here's how to implement the Least L1 Norm problem using JuMP with the epigraph formulation:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"function solveLeastL1NormByJuMP(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, solver_optimizer=SCS.Optimizer; warmup=false)\n\n    m, n = size(A)\n\n    # Create JuMP model with specified optimizer\n    model = Model(solver_optimizer)\n    set_silent(model)\n    \n    # Define optimization variable\n    @variable(model, x[1:n])\n    \n    # Define auxiliary variables for L1 norm ||Ax - b||_1\n    # We use the epigraph formulation: ||Ax - b||_1 <= sum(t), -t <= Ax - b <= t\n    @variable(model, t[1:m] >= 0)\n    \n    # Define objective: sum(t) which represents ||Ax - b||_1\n    @objective(model, Min, sum(t))\n    \n    # Define constraints for L1 norm: -t <= Ax - b <= t (split into two constraints)\n    residual = A * x - b\n    @constraint(model, residual .<= t)\n    @constraint(model, residual .>= -t)\n    \n    # Solve the problem and get solver time directly from JuMP\n    optimize!(model)\n    solver_time = solve_time(model)\n    \n    status = termination_status(model)\n    optimal_value = objective_value(model)\n    optimal_x = value.(x)\n    \n    return optimal_value, solver_time\nend","category":"page"},{"location":"S3_examples/LeastL1Norm/#Multiblock-Problem-Implementation","page":"Least L1 Norm","title":"Multiblock Problem Implementation","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Here's how to formulate the same problem as a multiblock problem for ADMM using the PDMO framework:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"function generateLeastL1Norm(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64})\n    mbp = MultiblockProblem() \n\n    numberRows numberCols = size(A)\n\n    # x block \n    block_x = BlockVariable()  \n    block_x.val = randn(numberCols)\n    xID = addBlockVariable!(mbp, block_x)\n\n    # z block \n    block_z = BlockVariable()\n    block_z.g = ElementwiseL1Norm()\n    block_z.val = A * mbp.blocks[1].val - b \n    zID = addBlockVariable!(mbp, block_z)\n\n    # constraint: Ax-z = b\n    constr = BlockConstraint() \n    addBlockMappingToConstraint!(constr, xID, LinearMappingMatrix(A))\n    addBlockMappingToConstraint!(constr, zID, LinearMappingIdentity(-1.0))\n    constr.rhs = b\n    addBlockConstraint!(mbp, constr)\n\n    return mbp\n\nend","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"After generating the multiblock problem, you can solve it using various ADMM configurations:","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"# ADMM with RB Adapter\nparam = ADMMParam(\n    adapter = RBAdapter(testRatio=10.0, adapterRatio=2.0),\n    presTolL2 = Inf,\n    dresTolL2 = Inf,\n    presTolLInf = 1e-4,\n    dresTolLInf = 1e-4\n)\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S3_examples/LeastL1Norm/#Benchmark-Methodology","page":"Least L1 Norm","title":"Benchmark Methodology","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"The benchmark compares multiple ADMM solution approaches:","category":"page"},{"location":"S3_examples/LeastL1Norm/#ADMM-Variants","page":"Least L1 Norm","title":"ADMM Variants","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Original ADMM: Standard ADMM with different configurations\nAccelerated ADMM: With Anderson and Auto-Halpern accelerators\nAdaptive ADMM: With RB and SRA adapters\nLinearized ADMM: Adaptive and doubly linearized variants","category":"page"},{"location":"S3_examples/LeastL1Norm/#Problem-Scales","page":"Least L1 Norm","title":"Problem Scales","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Small: 500 × 250 (500 constraints, 250 variables)\nMedium: 1000 × 500 (1000 constraints, 500 variables)\nLarge: 2000 × 1000 (2000 constraints, 1000 variables)","category":"page"},{"location":"S3_examples/LeastL1Norm/#Performance-Results","page":"Least L1 Norm","title":"Performance Results","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/#Small-Scale-(500-250)","page":"Least L1 Norm","title":"Small Scale (500 × 250)","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Method Time (s) Iterations Status\nOriginal ADMM + Halpern only 1.77 6,350 OPTIMAL\nOriginal ADMM + RB + Halpern 2.10 8,552 OPTIMAL\nOriginal ADMM + Anderson only 2.46 3,186 OPTIMAL\nOriginal ADMM + RB Adapter 4.07 16,697 OPTIMAL\nOriginal ADMM (baseline) 6.66 21,774 OPTIMAL","category":"page"},{"location":"S3_examples/LeastL1Norm/#Medium-Scale-(1000-500)","page":"Least L1 Norm","title":"Medium Scale (1000 × 500)","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Method Time (s) Iterations Status\nOriginal ADMM + RB + Halpern 7.70 8,143 OPTIMAL\nOriginal ADMM + Halpern only 11.79 12,412 OPTIMAL\nOriginal ADMM + Anderson only 14.38 4,864 OPTIMAL\nOriginal ADMM + RB Adapter 22.03 23,514 OPTIMAL\nOriginal ADMM (baseline) 24.91 26,234 OPTIMAL","category":"page"},{"location":"S3_examples/LeastL1Norm/#Large-Scale-(2000-1000)","page":"Least L1 Norm","title":"Large Scale (2000 × 1000)","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Method Time (s) Iterations Status\nOriginal ADMM + RB Adapter 54.91 9,742 OPTIMAL\nOriginal ADMM + RB + Halpern 63.96 11,493 OPTIMAL\nOriginal ADMM + Anderson only 159.97 8,127 OPTIMAL\nOriginal ADMM (baseline) 282.29 51,226 OPTIMAL\nOriginal ADMM + Halpern only 283.16 46,426 OPTIMAL","category":"page"},{"location":"S3_examples/LeastL1Norm/#Multi-Scale-Performance-Summary","page":"Least L1 Norm","title":"Multi-Scale Performance Summary","text":"","category":"section"},{"location":"S3_examples/LeastL1Norm/","page":"Least L1 Norm","title":"Least L1 Norm","text":"Scale Best ADMM Method Time (s) Iterations Status\nSmall (500×250) Original ADMM + Halpern only 1.77 6,350 OPTIMAL\nMedium (1000×500) Original ADMM + RB + Halpern 7.70 8,143 OPTIMAL\nLarge (2000×1000) Original ADMM + RB Adapter 54.91 9,742 OPTIMAL","category":"page"},{"location":"S4_api/main/#Main-Interface","page":"Main","title":"Main Interface","text":"","category":"section"},{"location":"S4_api/main/","page":"Main","title":"Main","text":"This page documents the main solver interfaces and high-level functions in PDMO.jl.","category":"page"},{"location":"S4_api/main/#Main-Solver-Functions","page":"Main","title":"Main Solver Functions","text":"","category":"section"},{"location":"S4_api/main/#PDMO.runBipartiteADMM","page":"Main","title":"PDMO.runBipartiteADMM","text":"runBipartiteADMM(mbp::MultiblockProblem, param::ADMMParam; kwargs...)\n\nSolve a multiblock optimization problem using the bipartite ADMM algorithm.\n\nThis function implements a bipartite ADMM approach for solving large-scale multiblock optimization problems. It includes automatic bipartization of the problem graph, scaling options, and comprehensive solution validation.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock optimization problem to solve\nparam::ADMMParam: ADMM algorithm parameters including penalty parameter, tolerances, max iterations,  scaling options (applyScaling), and algorithmic components (solver, adapter, accelerator)\n\nKeyword Arguments\n\nsaveSolutionInMultiblockProblem::Bool = true: Whether to save the solution back to the problem structure\nbipartizationAlgorithm::BipartizationAlgorithm = BFS_BIPARTIZATION: Algorithm for graph bipartization\nBFS_BIPARTIZATION: Breadth-first search bipartization (default)\nDFS_BIPARTIZATION: Depth-first search bipartization\nMILP_BIPARTIZATION: Mixed-integer linear programming bipartization\nSPANNING_TREE_BIPARTIZATION: Spanning tree bipartization\ntrueObj::Float64 = Inf: Known true objective value for validation (if available)\ntryJuMP::Bool = true: Whether to verify solution using JuMP solver\n\nReturns\n\nNamedTuple: Contains solution and iteration information\nsolution::Dict{BlockID, NumericVariable}: Optimal solution for each block\niterationInfo::ADMMIterationInfo: Detailed iteration information including convergence history\n\nExamples\n\n# Basic usage\nmbp = MultiblockProblem()\n# ... add blocks and constraints ...\nparam = ADMMParam()\nparam.initialRho = 1.0\nparam.maxIter = 1000\nparam.presTolL2 = 1e-6\nparam.dresTolL2 = 1e-6\nresult = runBipartiteADMM(mbp, param)\n\n# With scaling and custom bipartization\nparam = ADMMParam()\nparam.applyScaling = true\nparam.solver = OriginalADMMSubproblemSolver()\nresult = runBipartiteADMM(mbp, param; \n    bipartizationAlgorithm=MILP_BIPARTIZATION)\n\n# Access solution\nsolution = result.solution\ninfo = result.iterationInfo\n\nAlgorithm Steps\n\nValidation: Checks multiblock problem validity\nScaling: Optionally applies problem scaling for numerical stability (controlled by param.applyScaling)\nGraph Construction: Creates multiblock graph representation\nBipartization: Applies bipartization algorithm to create bipartite graph\nADMM Execution: Runs bipartite ADMM algorithm\nSolution Retrieval: Extracts and unscales primal solution\nVerification: Validates solution feasibility and objective value\n\nSee also: runAdaPDM, MultiblockProblem, ADMMParam\n\n\n\n\n\n","category":"function"},{"location":"S4_api/main/#PDMO.runAdaPDM","page":"Main","title":"PDMO.runAdaPDM","text":"runAdaPDM(mbp::MultiblockProblem, param::AbstractAdaPDMParam; kwargs...)\n\nSolve a composite multiblock optimization problem using the Adaptive Primal-Dual Method (AdaPDM).\n\nThis function implements various adaptive primal-dual methods for solving composite optimization problems with the structure: minimize f(x) + g(Ax), where some blocks may be proximal-only (g functions).\n\nArguments\n\nmbp::MultiblockProblem: The composite multiblock optimization problem to solve\nparam::AbstractAdaPDMParam: AdaPDM algorithm parameters. Can be one of:\nAdaPDMParam: Standard adaptive primal-dual method parameters\nAdaPDMPlusParam: Enhanced AdaPDM with additional features\nMalitskyPockParam: Malitsky-Pock algorithm parameters\nCondatVuParam: Condat-Vũ algorithm parameters\n\nKeyword Arguments\n\nsaveSolutionInMultiblockProblem::Bool = true: Whether to save the solution back to the problem structure\ntrueObj::Float64 = Inf: Known true objective value for validation (if available)\ntryJuMP::Bool = true: Whether to verify solution using JuMP solver\n\nReturns\n\nNamedTuple: Contains solution and iteration information\nsolution::Dict{BlockID, NumericVariable}: Optimal solution for each block\niterationInfo::AdaPDMIterationInfo: Detailed iteration information including convergence history\n\nExamples\n\n# Basic AdaPDM usage\nmbp = MultiblockProblem()\n# ... add blocks with composite structure ...\nparam = AdaPDMParam(mbp; maxIter=1000, primalTol=1e-6, dualTol=1e-6)\nresult = runAdaPDM(mbp, param)\n\n# Using Condat-Vũ algorithm\nparam = CondatVuParam(mbp; maxIter=1000)\nresult = runAdaPDM(mbp, param)\n\n# Access solution\nsolution = result.solution\ninfo = result.iterationInfo\n\nProblem Structure\n\nThe method is designed for composite problems where:\n\nSome blocks have smooth functions (f) with gradient oracles\nSome blocks have proximal functions (g) with proximal oracles\nThe last block typically represents the coupling constraint variable\n\nAlgorithm Variants\n\nAdaPDM: Standard adaptive primal-dual method\nAdaPDM+: Enhanced version with additional acceleration\nMalitsky-Pock: Malitsky-Pock primal-dual algorithm\nCondat-Vũ: Condat-Vũ primal-dual algorithm\n\nSee also: runBipartiteADMM, MultiblockProblem, AdaPDMParam\n\n\n\n\n\n","category":"function"},{"location":"S3_examples/DualSVM/#Dual-SVM-Benchmark-Results","page":"Dual SVM","title":"Dual SVM Benchmark Results","text":"","category":"section"},{"location":"S3_examples/DualSVM/#Problem-Formulation","page":"Dual SVM","title":"Problem Formulation","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"The Dual Support Vector Machine problem is a quadratic programming problem with linear constraints:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"beginaligned\nmin_x quad  frac12langle Qx x rangle - langle e x rangle \nmathrmst quad  langle b x rangle = 0 \n 0 leq x_i leq C quad forall i\nendaligned","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"where:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Q is an n × n positive definite matrix (kernel matrix)\nb is an n-dimensional vector (class labels)\ne is the vector of all ones\nx is the n-dimensional optimization variable (dual variables)\nC is the regularization parameter","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"For ADMM decomposition, this is reformulated as a two-block problem:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"beginaligned\nmin_x z quad  frac12langle Qx x rangle - langle e x rangle \nmathrmst quad  x - z = 0 quad 0 leq z_i leq C quad forall i \n x in x langle b x rangle = 0\nendaligned","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"This reformulation separates the equality constraint from the box constraints, enabling efficient distributed solving using bipartite ADMM.","category":"page"},{"location":"S3_examples/DualSVM/#Implementation-Examples","page":"Dual SVM","title":"Implementation Examples","text":"","category":"section"},{"location":"S3_examples/DualSVM/#JuMP-Implementation","page":"Dual SVM","title":"JuMP Implementation","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Here's how to implement the Dual SVM problem using JuMP:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"function solveDualSVMByJuMP(Q::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, C::Float64, solver_optimizer=SCS.Optimizer; warmup=false)\n\n    n = length(b)\n\n    # Create JuMP model with specified optimizer\n    model = Model(solver_optimizer)\n    set_silent(model)\n    \n    # Define optimization variable\n    @variable(model, x[1:n])\n    \n    # Define objective: 0.5 * <Qx, x> - <e, x>\n    @objective(model, Min, 0.5 * x' * Q * x - sum(x))\n    \n    # Define constraints: <b, x> = 0, 0 <= x <= C\n    @constraint(model, dot(b, x) == 0)\n    @constraint(model, x .>= 0)\n    @constraint(model, x .<= C)\n    \n    # Solve the problem and get solver time directly from JuMP\n    optimize!(model)\n    solver_time = solve_time(model)\n    \n    status = termination_status(model)\n    optimal_value = objective_value(model)\n\n    return optimal_value, solver_time\nend","category":"page"},{"location":"S3_examples/DualSVM/#Multiblock-Problem-Implementation","page":"Dual SVM","title":"Multiblock Problem Implementation","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Here's how to formulate the same problem as a multiblock problem using PDMO:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"function generateDualSVM(Q::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, C::Float64)\n    \n    numberVars = length(b)\n    @assert(numberVars == size(Q,1) == size(Q, 2), \"DualSVM: input dimension mismatch. \")\n\n    mbp = MultiblockProblem() \n\n    # x block\n    block_x = BlockVariable() \n    block_x.f = QuadraticFunction(0.5 * Q, -ones(numberVars), 0.0)\n    block_x.g = IndicatorHyperplane(b, 0.0)\n    block_x.val = zeros(numberVars) # initial point\n    xID = addBlockVariable!(mbp, block_x)\n\n    # z block\n    block_z = BlockVariable() \n    block_z.g = IndicatorBox(zeros(numberVars), ones(numberVars) * C)\n    block_z.val = zeros(numberVars) # initial point\n    zID = addBlockVariable!(mbp, block_z)\n\n    # constraint: x - z = 0 \n    constr = BlockConstraint() \n    addBlockMappingToConstraint!(constr, xID, LinearMappingIdentity(1.0))\n    addBlockMappingToConstraint!(constr, zID, LinearMappingIdentity(-1.0))\n    constr.rhs = spzeros(numberVars)\n    addBlockConstraint!(mbp, constr)\n    \n    return mbp \nend","category":"page"},{"location":"S3_examples/DualSVM/#Running-ADMM-with-Different-Parameters","page":"Dual SVM","title":"Running ADMM with Different Parameters","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"After generating the multiblock problem, you can solve it using various ADMM configurations:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"# ADMM with RB Adapter\nparam = ADMMParam(\n    adapter = RBAdapter(testRatio=10.0, adapterRatio=2.0),\n    presTolL2 = Inf,\n    dresTolL2 = Inf,\n    presTolLInf = 1e-4,\n    dresTolLInf = 1e-4\n)\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S3_examples/DualSVM/#Benchmark-Methodology","page":"Dual SVM","title":"Benchmark Methodology","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"The benchmark compares multiple ADMM solution approaches:","category":"page"},{"location":"S3_examples/DualSVM/#ADMM-Variants","page":"Dual SVM","title":"ADMM Variants","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Original ADMM: Basic consensus ADMM with no acceleration\nAnderson Accelerator: History-based acceleration method\nAuto-Halpern Accelerator: Adaptive step-size acceleration\nRB Adapter: Residual Balancing for automatic parameter tuning\nSRA Adapter: Spectral Residual Adaptation\nLinearized Solvers: Adaptive and doubly linearized variants","category":"page"},{"location":"S3_examples/DualSVM/#Problem-Scales","page":"Dual SVM","title":"Problem Scales","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Three different problem sizes are tested:","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Small: n = 500 (number of variables)\nMedium: n = 1000  \nLarge: n = 4000","category":"page"},{"location":"S3_examples/DualSVM/#Performance-Results","page":"Dual SVM","title":"Performance Results","text":"","category":"section"},{"location":"S3_examples/DualSVM/#Small-Scale-(n-500)","page":"Dual SVM","title":"Small Scale (n = 500)","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Method Time (s) Iterations Status\nDoubly Linearized (baseline) 1.35 879 OPTIMAL\nAdaptive Linearized (γ=1.0, r=1000) 1.45 2753 OPTIMAL\nAdaptive Linearized Simple (γ=1.0, r=1000) 1.58 2625 OPTIMAL\nAdaptive Linearized (γ=2.0, r=1000) 1.60 3030 OPTIMAL\nAdaptive Linearized (γ=0.5, r=1000) 1.78 2756 OPTIMAL","category":"page"},{"location":"S3_examples/DualSVM/#Medium-Scale-(n-1000)","page":"Dual SVM","title":"Medium Scale (n = 1000)","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Method Time (s) Iterations Status\nDoubly Linearized (baseline) 5.86 1357 OPTIMAL\nOriginal ADMM + SRA Adapter 15.76 61 OPTIMAL\nOriginal ADMM + SRA + Halpern 15.84 61 OPTIMAL\nAdaptive Linearized (γ=0.5, r=1000) 17.95 9568 OPTIMAL\nAdaptive Linearized Simple (γ=1.0, r=1000) 18.38 10030 OPTIMAL","category":"page"},{"location":"S3_examples/DualSVM/#Large-Scale-(n-4000)","page":"Dual SVM","title":"Large Scale (n = 4000)","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Method Time (s) Iterations Status\nDoubly Linearized (baseline) 285.60 3348 OPTIMAL\nOriginal ADMM + SRA Adapter 1034.30 69 OPTIMAL\nOriginal ADMM + SRA + Halpern 1050.25 69 OPTIMAL\nAdaptive Linearized Simple (γ=1.0, r=1000) 3600.01 98037 TIME_LIMIT\nAdaptive Linearized (γ=2.0, r=1000) 3600.01 96944 TIME_LIMIT","category":"page"},{"location":"S3_examples/DualSVM/#Multi-Scale-Performance-Summary","page":"Dual SVM","title":"Multi-Scale Performance Summary","text":"","category":"section"},{"location":"S3_examples/DualSVM/","page":"Dual SVM","title":"Dual SVM","text":"Scale Best ADMM Method Time (s) Iterations Status\nSmall (n=500) Doubly Linearized (baseline) 1.35 879 OPTIMAL\nMedium (n=1000) Doubly Linearized (baseline) 5.86 1357 OPTIMAL\nLarge (n=4000) Doubly Linearized (baseline) 285.60 3348 OPTIMAL","category":"page"},{"location":"S3_examples/FusedLasso/#Fused-Lasso-Benchmark-Results","page":"Fused Lasso","title":"Fused Lasso Benchmark Results","text":"","category":"section"},{"location":"S3_examples/FusedLasso/#Problem-Formulation","page":"Fused Lasso","title":"Problem Formulation","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"The Fused Lasso problem combines a quadratic data fidelity term with an L1 penalty on the differences between adjacent variables:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"min_x quad frac12Ax - b^2 + lambdaDx_1","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"where:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"A is an m × n matrix\nb is an m-dimensional vector  \nx is the n-dimensional optimization variable\nλ is the regularization parameter\nD is the difference matrix:\nD = [-1  1  0 ...  0  0\n      0 -1  1 ...  0  0\n      ...\n      0  0  0 ... -1  1]","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"For ADMM decomposition, this is reformulated as a two-block problem:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"beginaligned\nmin_x z quad  frac12Ax - b^2 + lambdaz_1 \nmathrmst quad  Dx - z = 0\nendaligned","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"This reformulation enables efficient distributed solving using bipartite ADMM.","category":"page"},{"location":"S3_examples/FusedLasso/#Implementation-Examples","page":"Fused Lasso","title":"Implementation Examples","text":"","category":"section"},{"location":"S3_examples/FusedLasso/#JuMP-Implementation","page":"Fused Lasso","title":"JuMP Implementation","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Here's how to implement the Fused Lasso problem using JuMP with the epigraph formulation:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"function solveFusedLassoByJuMP(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, D::SparseMatrixCSC{Float64, Int64}, lambda::Float64, solver_optimizer=SCS.Optimizer; warmup=false)\n\n    m, n = size(A)\n\n    # Create JuMP model with specified optimizer\n    model = Model(solver_optimizer)\n    set_silent(model)\n    \n    # Define optimization variable\n    @variable(model, x[1:n])\n    \n    # Define auxiliary variables for L1 norm ||Dx||_1\n    # We use the epigraph formulation: ||Dx||_1 <= sum(t), -t <= Dx <= t\n    d_size = size(D, 1)\n    @variable(model, t[1:d_size] >= 0)\n    \n    # Define objective: (1/2) ||Ax - b||^2 + lambda * sum(t)\n    residual = A * x - b\n    @objective(model, Min, 0.5 * sum(residual[i]^2 for i in 1:length(residual)) + lambda * sum(t))\n    \n    # Define constraints for L1 norm: -t <= Dx <= t (split into two constraints)\n    Dx = D * x\n    @constraint(model, Dx .<= t)\n    @constraint(model, Dx .>= -t)\n    \n    # Solve the problem and get solver time directly from JuMP\n    optimize!(model)\n    solver_time = solve_time(model)\n    \n    status = termination_status(model)\n    optimal_value = objective_value(model)\n\n    return optimal_value, solver_time\nend","category":"page"},{"location":"S3_examples/FusedLasso/#Multiblock-Problem-Implementation","page":"Fused Lasso","title":"Multiblock Problem Implementation","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Here's how to formulate the same problem as a multiblock problem using PDMO:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"function generateFusedLasso(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, lambda::Float64)\n    mbp = MultiblockProblem() \n    \n    numberRows, numberCols = size(A)\n    @assert(numberRows == length(b), \"FusedLasso: Dimension mismatch. \")\n\n    # x block\n    block_x = BlockVariable() \n    # block_x.f = FrobeniusNormSquare(Matrix(A), b, numberCols, 1, 0.5)\n    block_x.f = QuadraticFunction(0.5 * A' * A, -1.0 * A' * b, 0.5 * dot(b, b))\n    block_x.val = zeros(numberCols)\n    xID = addBlockVariable!(mbp, block_x)\n\n    # z block \n    block_z = BlockVariable() \n    block_z.g = ElementwiseL1Norm(lambda)\n    block_z.val = zeros(numberCols-1)\n    zID = addBlockVariable!(mbp, block_z)\n\n    # constraint \n    constr = BlockConstraint() \n    D = spdiagm(0 => -ones(numberCols - 1), 1 => ones(numberCols - 1))\n    D = D[1:end-1, :]\n    addBlockMappingToConstraint!(constr, xID, LinearMappingMatrix(D))\n    addBlockMappingToConstraint!(constr, zID, LinearMappingIdentity(-1.0))\n    constr.rhs = zeros(numberCols-1)\n    addBlockConstraint!(mbp, constr)\n\n    return mbp\nend","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"After generating the multiblock problem, you can solve it using various ADMM configurations:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"# ADMM with RB Adapter\nparam = ADMMParam(\n    adapter = RBAdapter(testRatio=10.0, adapterRatio=2.0),\n    presTolL2 = Inf,\n    dresTolL2 = Inf,\n    presTolLInf = 1e-4,\n    dresTolLInf = 1e-4\n)\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S3_examples/FusedLasso/#Benchmark-Methodology","page":"Fused Lasso","title":"Benchmark Methodology","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"The benchmark compares multiple ADMM solution approaches:","category":"page"},{"location":"S3_examples/FusedLasso/#ADMM-Variants","page":"Fused Lasso","title":"ADMM Variants","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Original ADMM: Basic consensus ADMM with no acceleration\nAnderson Accelerator: History-based acceleration method\nAuto-Halpern Accelerator: Adaptive step-size acceleration\nRB Adapter: Residual Balancing for automatic parameter tuning\nSRA Adapter: Spectral Residual Adaptation\nLinearized Solvers: Adaptive and doubly linearized variants","category":"page"},{"location":"S3_examples/FusedLasso/#Problem-Scales","page":"Fused Lasso","title":"Problem Scales","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Three different problem sizes are tested:","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Small: 500 × 250 (A matrix dimensions)\nMedium: 1000 × 500  \nLarge: 4000 × 2000","category":"page"},{"location":"S3_examples/FusedLasso/#Performance-Results","page":"Fused Lasso","title":"Performance Results","text":"","category":"section"},{"location":"S3_examples/FusedLasso/#Small-Scale-(500-250)","page":"Fused Lasso","title":"Small Scale (500 × 250)","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + RB + Halpern 0.08 856 OPTIMAL\nOriginal ADMM + RB Adapter 0.10 454 OPTIMAL\nOriginal ADMM + SRA + Halpern 0.11 63 OPTIMAL\nOriginal ADMM + SRA Adapter 0.15 63 OPTIMAL\nOriginal ADMM + Anderson only 0.23 159 OPTIMAL","category":"page"},{"location":"S3_examples/FusedLasso/#Medium-Scale-(1000-500)","page":"Fused Lasso","title":"Medium Scale (1000 × 500)","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + Anderson only 0.13 229 OPTIMAL\nOriginal ADMM (baseline) 0.13 880 OPTIMAL\nOriginal ADMM + Halpern only 0.28 1707 OPTIMAL\nOriginal ADMM + RB Adapter 0.45 867 OPTIMAL\nOriginal ADMM + RB + Halpern 0.60 1681 OPTIMAL","category":"page"},{"location":"S3_examples/FusedLasso/#Large-Scale-(4000-2000)","page":"Fused Lasso","title":"Large Scale (4000 × 2000)","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + Anderson only 7.32 474 OPTIMAL\nOriginal ADMM (baseline) 8.21 1949 OPTIMAL\nOriginal ADMM + Halpern only 14.97 3839 OPTIMAL\nOriginal ADMM + RB Adapter 21.57 968 OPTIMAL\nOriginal ADMM + RB + Halpern 25.05 1873 OPTIMAL","category":"page"},{"location":"S3_examples/FusedLasso/#Multi-Scale-Performance-Summary","page":"Fused Lasso","title":"Multi-Scale Performance Summary","text":"","category":"section"},{"location":"S3_examples/FusedLasso/","page":"Fused Lasso","title":"Fused Lasso","text":"Scale Best ADMM Method Time (s) Iterations Status\nSmall (500×250) Original ADMM + RB + Halpern 0.08 856 OPTIMAL\nMedium (1000×500) Original ADMM + Anderson only 0.13 229 OPTIMAL\nLarge (4000×2000) Original ADMM + Anderson only 7.32 474 OPTIMAL","category":"page"},{"location":"S3_examples/DualLasso/#Dual-Lasso-Benchmark-Results","page":"Dual Lasso","title":"Dual Lasso Benchmark Results","text":"","category":"section"},{"location":"S3_examples/DualLasso/#Problem-Formulation","page":"Dual Lasso","title":"Problem Formulation","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"The Dual Lasso problem is a quadratic programming problem with infinity norm constraints:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"beginaligned\nmin_x quad  frac14x^2 - b^top x \nmathrmst quad   Ax_infty leq lambda\nendaligned","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"where:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"A is an m × n matrix\nb is an n-dimensional vector\nx is the n-dimensional optimization variable  \nλ is the regularization parameter","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"For ADMM decomposition, this is reformulated as a two-block problem:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"beginaligned\nmin_x z quad  frac14x^2 - b^top x \nmathrmst quad   Ax - z = 0 z_infty leq lambda\nendaligned","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"This reformulation enables efficient distributed solving using bipartite ADMM.","category":"page"},{"location":"S3_examples/DualLasso/#Implementation-Examples","page":"Dual Lasso","title":"Implementation Examples","text":"","category":"section"},{"location":"S3_examples/DualLasso/#JuMP-Implementation","page":"Dual Lasso","title":"JuMP Implementation","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Here's how to implement the Dual Lasso problem using JuMP:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"using JuMP, SCS\n\nfunction solveDualLassoByJuMP(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, lambda::Float64, solver_optimizer=SCS.Optimizer; warmup=false)\n\n    m, n = size(A)\n\n    # Create JuMP model with specified optimizer\n    model = Model(solver_optimizer)\n    set_silent(model)\n    \n    # Define optimization variable\n    @variable(model, x[1:n])\n    \n    # Define objective: (1/4) ||x||^2 - b'x\n    @objective(model, Min, 0.25 * sum(x[i]^2 for i in 1:n) - dot(b, x))\n    \n    # Define constraint: ||Ax||_{inf} <= lambda\n    # This is equivalent to: -lambda <= (Ax)_i <= lambda for all i\n    Ax = A * x\n    @constraint(model, -lambda .<= Ax .<= lambda)\n    \n    # Solve the problem and get solver time directly from JuMP\n    optimize!(model)\n    solver_time = solve_time(model)\n    \n    status = termination_status(model)\n    optimal_value = objective_value(model)\n    optimal_x = value.(x)\n\n    return optimal_value, solver_time\nend","category":"page"},{"location":"S3_examples/DualLasso/#Multiblock-Problem-Implementation","page":"Dual Lasso","title":"Multiblock Problem Implementation","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Here's how to formulate the same problem as a multiblock problem for ADMM:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"using PDMO\n\nfunction generateDualLasso(A::SparseMatrixCSC{Float64, Int64}, b::Vector{Float64}, lambda::Float64)\n    mbp = MultiblockProblem() \n    \n    numberRows, numberCols = size(A)\n    @assert(numberCols == length(b), \"DualLasso: Dimension mismatch. \")\n\n    # x block\n    block_x = BlockVariable() \n    block_x.f = QuadraticFunction(0.25 * spdiagm(0 => ones(numberCols)), -b, 0.0)\n    block_x.val = zeros(numberCols)\n    xID = addBlockVariable!(mbp, block_x)\n\n    # z block \n    block_z = BlockVariable() \n    block_z.g = IndicatorBox(-lambda * ones(numberRows), ones(numberRows) * lambda)\n    block_z.val = zeros(numberRows)\n    zID = addBlockVariable!(mbp, block_z)\n\n    # constraint \n    constr = BlockConstraint() \n    addBlockMappingToConstraint!(constr, xID, LinearMappingMatrix(A))\n    addBlockMappingToConstraint!(constr, zID, LinearMappingIdentity(-1.0))\n    constr.rhs = zeros(numberRows)\n    addBlockConstraint!(mbp, constr)\n\n    return mbp\nend ","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"After generating the multiblock problem, you can solve it using various ADMM configurations:","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"# ADMM with RB Adapter\nparam = ADMMParam(\n    adapter = RBAdapter(testRatio=10.0, adapterRatio=2.0),\n    presTolL2 = Inf,\n    dresTolL2 = Inf,\n    presTolLInf = 1e-4,\n    dresTolLInf = 1e-4\n)\nresult = runBipartiteADMM(mbp, param)","category":"page"},{"location":"S3_examples/DualLasso/#Benchmark-Methodology","page":"Dual Lasso","title":"Benchmark Methodology","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"The benchmark compares multiple ADMM solution approaches:","category":"page"},{"location":"S3_examples/DualLasso/#ADMM-Variants","page":"Dual Lasso","title":"ADMM Variants","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Original ADMM: Baseline implementation\nAccelerated Methods: Anderson and Auto-Halpern acceleration\nAdaptive Methods: RB and SRA adapters\nLinearized Methods: Adaptive and doubly linearized solvers","category":"page"},{"location":"S3_examples/DualLasso/#Problem-Scales","page":"Dual Lasso","title":"Problem Scales","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Small Scale: 100 × 200 (λ = 2.0)\nMedium Scale: 2000 × 3000 (λ = 2.0)  \nLarge Scale: 4000 × 8000 (λ = 2.0)","category":"page"},{"location":"S3_examples/DualLasso/#Performance-Results","page":"Dual Lasso","title":"Performance Results","text":"","category":"section"},{"location":"S3_examples/DualLasso/#Small-Scale-(100-200)","page":"Dual Lasso","title":"Small Scale (100 × 200)","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + RB + Halpern 0.02 77 OPTIMAL\nOriginal ADMM + SRA + Halpern 0.02 61 OPTIMAL\nOriginal ADMM + SRA Adapter 0.03 61 OPTIMAL\nOriginal ADMM + RB Adapter 0.07 77 OPTIMAL\nOriginal ADMM + Halpern only 1.91 26909 OPTIMAL","category":"page"},{"location":"S3_examples/DualLasso/#Medium-Scale-(2000-3000)","page":"Dual Lasso","title":"Medium Scale (2000 × 3000)","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + RB Adapter 15.24 158 OPTIMAL\nOriginal ADMM + SRA Adapter 15.41 169 OPTIMAL\nOriginal ADMM + RB + Halpern 17.04 248 OPTIMAL\nOriginal ADMM + SRA + Halpern 18.17 283 OPTIMAL\nOriginal ADMM + Anderson only 356.66 9061 OPTIMAL","category":"page"},{"location":"S3_examples/DualLasso/#Large-Scale-(4000-8000)","page":"Dual Lasso","title":"Large Scale (4000 × 8000)","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"ADMM Performance (Top 5):","category":"page"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Method Time (s) Iterations Status\nOriginal ADMM + SRA Adapter 103.91 350 OPTIMAL\nOriginal ADMM + SRA + Halpern 140.49 644 OPTIMAL\nOriginal ADMM + RB Adapter 147.68 347 OPTIMAL\nOriginal ADMM + RB + Halpern 186.31 638 OPTIMAL\nAdaptive Linearized Simple (γ=1.0, r=1000) 3600.03 31305 TIME_LIMIT","category":"page"},{"location":"S3_examples/DualLasso/#Multi-Scale-Performance-Summary","page":"Dual Lasso","title":"Multi-Scale Performance Summary","text":"","category":"section"},{"location":"S3_examples/DualLasso/","page":"Dual Lasso","title":"Dual Lasso","text":"Scale Best ADMM Method Time (s) Iterations Status\nSmall (100×200) Original ADMM + RB + Halpern 0.02 77 OPTIMAL\nMedium (2000×3000) Original ADMM + RB Adapter 15.24 158 OPTIMAL\nLarge (4000×8000) Original ADMM + SRA Adapter 103.91 350 OPTIMAL","category":"page"},{"location":"S2_algorithms/AdaPDM/#Adaptive-Primal-Dual-Methods-(AdaPDM)","page":"AdaPDM","title":"Adaptive Primal-Dual Methods (AdaPDM)","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/#Introduction-and-Solution-Process-Overview","page":"AdaPDM","title":"Introduction and Solution Process Overview","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Adaptive Primal-Dual Methods (AdaPDM) are optimization algorithms designed to solve composite optimization problems with specific structural constraints. Unlike ADMM which requires bipartization for multiblock problems, AdaPDM methods work directly on problems that naturally satisfy a composite structure.","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"The AdaPDM framework solves problems of the form:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"beginaligned\nmin_mathbfx quad   sum_j=1^n-1 left( f_j(x_j) + g_j(x_j) right) + g_n(x_n) \ntextst quad  mathbfA_11x_1 + mathbfA_2x_2 + cdots + mathbfA_1n-1x_n-1 - x_n+1 = 0\nendaligned","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"or equivalently, ","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"beginaligned\nmin_mathbfx quad   sum_j=1^n-1 left( f_j(x_j) + g_j(x_j) right) + g_n(mathbfA_11x_1 + mathbfA_12x_2 + cdots + mathbfA_1n-1x_n-1)\nendaligned","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"The algorithm alternates between dual and primal updates:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"beginalign\n   y^k+1 = textprox_sigma g_n^*(y^k + sigma Abarx^k) notag \n   x_j^k+1 = textprox_gamma g_j(x_j^k - gamma(nabla f_j(x_j^k) + mathbfA_1j^top y^k+1)) quad j=1ldotsn-1 notag \nendalign","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"where barx^k includes extrapolation terms, and sigma and gamma are adaptive step sizes.","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"PDMO.jl implements a comprehensive AdaPDM framework with automated three-stage process:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Problem Validation: Verify the MultiblockProblem conforms to the required composite structure\nAlgorithm Selection: Initialize the selected algorithm with user-specified parameters\nIterative Solution: Apply primal-dual updates until convergence","category":"page"},{"location":"S2_algorithms/AdaPDM/#Composite-Problem-Structure-and-Validation","page":"AdaPDM","title":"Composite Problem Structure and Validation","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"AdaPDM methods require problems to satisfy a specific composite structure:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Validation Criteria:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Single Constraint: Exactly one constraint of the form sum_i=1^n A_i x_i - x_n+1 = 0\nZero Right-Hand Side: The constraint must have zero right-hand side\nProximal-Only Block: One block must have f_n+1 = 0 (proximal-only) with mapping coefficient -10\nFunction Properties: f_i must be smooth convex functions, g_i must be proximal-friendly","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Automatic Processing:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Block reordering to ensure required (x_1 ldots x_p x_p+1) structure\nOperator norm estimation when not provided","category":"page"},{"location":"S2_algorithms/AdaPDM/#AdaPDM-Algorithm-Variants","page":"AdaPDM","title":"AdaPDM Algorithm Variants","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"PDMO.jl implements four distinct AdaPDM variants with different step size strategies. Unlike in ADMM where a specific variant is selected through solver field in ADMMParameter, we directly pass different algorithmic parameters to specify a particular variant. ","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Algorithm Parameter Reference\nCondat-Vũ CondatVuParam [1], [2]\nAdaPDM AdaPDMParam Algorithm 3.1 of [3]\nAdaPDM+ AdaPDMPlusParam Algorithm 3.2 of [3]\nMalitsky-Pock MalitskyPockParam [4]","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Please refer to original papers on how to tune each method. ","category":"page"},{"location":"S2_algorithms/AdaPDM/#Condat-Vũ-(CondatVuParam)","page":"AdaPDM","title":"Condat-Vũ (CondatVuParam)","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Classical primal-dual method with fixed step sizes:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"param = CondatVuParam(mbp;\n    alphaProvided = Inf,                     # Primal step size (auto-computed if Inf)\n    betaProvided = Inf,                      # Dual step size (auto-computed if Inf)\n    opNormEstimateProvided = Inf,            # Operator norm estimate\n    LipschitzConstantEstimateProvided = Inf, # Lipschitz constant estimate\n    presTolL2 = 1e-4,                        # Primal residual tolerance in L2 norm\n    dresTolL2 = 1e-4,                        # Dual residual tolerance in L2 norm\n    presTolLInf = 1e-6,                      # Primal residual tolerance in L∞ norm\n    dresTolLInf = 1e-6,                      # Dual residual tolerance in L∞ norm\n    lineSearchMaxIter = 1000,                # Maximum iterations for line search\n    maxIter = 10000,                         # Maximum iterations\n    logInterval = 1000,                      # Logging interval\n    timeLimit = 3600.0                       # Time limit in seconds\n)\nresult = runAdaPDM(mbp, param)","category":"page"},{"location":"S2_algorithms/AdaPDM/#AdaPDM-(AdaPDMParam)","page":"AdaPDM","title":"AdaPDM (AdaPDMParam)","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Basic adaptive primal-dual method with automatic step size selection:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"param = AdaPDMParam(mbp;\n    t = 1.0,                        # Primal/dual step size ratio\n    stepSizeEpsilon = 1e-6,         # Step size parameter epsilon  \n    stepSizeNu = 1.2,               # Step size parameter nu\n    opNormEstimateProvided = Inf,   # Operator norm estimate (auto-computed if Inf)\n    presTolL2 = 1e-4,               # Primal residual tolerance in L2 norm\n    dresTolL2 = 1e-4,               # Dual residual tolerance in L2 norm\n    presTolLInf = 1e-6,             # Primal residual tolerance in L∞ norm\n    dresTolLInf = 1e-6,             # Dual residual tolerance in L∞ norm\n    maxIter = 10000,                # Maximum iterations\n    logInterval = 1000,             # Logging interval\n    timeLimit = 3600.0              # Time limit in seconds\n)\nresult = runAdaPDM(mbp, param)","category":"page"},{"location":"S2_algorithms/AdaPDM/#AdaPDM-(AdaPDMPlusParam)","page":"AdaPDM","title":"AdaPDM+ (AdaPDMPlusParam)","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Enhanced version with line search and adaptive operator norm estimation:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"param = AdaPDMPlusParam(mbp;\n    t = 1.0,                              # Primal/dual step size ratio\n    stepSizeEpsilon = 1e-6,               # Step size parameter epsilon\n    stepSizeNu = 1.2,                     # Step size parameter nu\n    opNormEstimateProvided = Inf,         # Operator norm estimate (auto-computed if Inf)\n    backtrackingFactor = 2.0,             # Line search backtracking factor\n    normEstimateShrinkingFactor = 0.95,   # Operator norm shrinking factor\n    presTolL2 = 1e-4,                     # Primal residual tolerance in L2 norm\n    dresTolL2 = 1e-4,                     # Dual residual tolerance in L2 norm\n    presTolLInf = 1e-6,                   # Primal residual tolerance in L∞ norm\n    dresTolLInf = 1e-6,                   # Dual residual tolerance in L∞ norm\n    lineSearchMaxIter = 1000,             # Maximum iterations for line search\n    maxIter = 10000,                      # Maximum iterations\n    logInterval = 1000,                   # Logging interval\n    timeLimit = 3600.0                    # Time limit in seconds\n)\nresult = runAdaPDM(mbp, param)","category":"page"},{"location":"S2_algorithms/AdaPDM/#Malitsky-Pock-(MalitskyPockParam)","page":"AdaPDM","title":"Malitsky-Pock (MalitskyPockParam)","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Specialized method with backtracking and theoretical convergence guarantees:","category":"page"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"param = MalitskyPockParam(mbp;\n    initialTheta = 1.0,                # Initial primal step size ratio\n    initialSigma = 1.0,                # Initial dual step size\n    backtrackDescentRatio = 0.95,      # Backtrack descent ratio in (0,1)\n    t = 1.0,                           # Ratio for primal dual step sizes\n    mu = 0.8,                          # Backtrack parameter in (0,1)\n    presTolL2 = 1e-4,                  # Primal residual tolerance in L2 norm\n    dresTolL2 = 1e-4,                  # Dual residual tolerance in L2 norm\n    presTolLInf = 1e-6,                # Primal residual tolerance in L∞ norm\n    dresTolLInf = 1e-6,                # Dual residual tolerance in L∞ norm\n    lineSearchMaxIter = 1000,          # Maximum iterations for line search\n    maxIter = 10000,                   # Maximum iterations\n    logInterval = 1000,                # Logging interval\n    timeLimit = 3600.0                 # Time limit in seconds\n)\nresult = runAdaPDM(mbp, param)","category":"page"},{"location":"S2_algorithms/AdaPDM/#References","page":"AdaPDM","title":"References","text":"","category":"section"},{"location":"S2_algorithms/AdaPDM/","page":"AdaPDM","title":"AdaPDM","text":"Condat, L. (2013). A primal–dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms. Journal of Optimization Theory and Applications, 158(2), 460-479.\nVũ, B. C. (2013). A splitting algorithm for dual monotone inclusions involving cocoercive operators. Advances in Computational Mathematics, 38(3), 667-681.\nLatafat, P., Themelis, A., Stella, L., & Patrinos, P. (2024). Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient. Mathematical Programming, 1-39.\nMalitsky, Y., & Pock, T. (2018). A first-order primal-dual algorithm with linesearch. SIAM Journal on Optimization, 28(1), 411-432.","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Formulations","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"This page documents the problem formulation utilities in PDMO.jl.","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Core Data Types","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Block Components","category":"page"},{"location":"S4_api/formulations/#PDMO.BlockID","page":"Formulations","title":"PDMO.BlockID","text":"BlockVariable\n\nA container representing a block variable in a larger optimization problem.\n\nFields\n\nid::Union{Int, String}: A unique identifier for the block. This can be either an integer or a string.\nf::AbstractFunction: A function that should be smooth.\ng::AbstractFunction: A function that should be proximal.\nval::NumericVariable: The variable associated with this block.\n\nA default constructor is provided with BlockVariable(idx::Int64=0), which initializes f and g with Zero()  and val with 0.0. A block is considered uninitialized when block_idx ≤ 0.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.BlockVariable","page":"Formulations","title":"PDMO.BlockVariable","text":"BlockVariable(id::BlockID=\"\")\n\nConstruct a new block variable with the specified ID.\n\nArguments\n\nid::BlockID=\"\": The identifier for the block. Can be an integer or string. Default is an empty string.\n\nReturns\n\nBlockVariable: A new BlockVariable instance with the given ID and default Zero functions.\n\nThrows\n\nErrorException: If id is a negative integer.\n\nNotes\n\nIf id is an empty string, a warning is issued that it might be overwritten later.\nThe block is initialized with Zero() functions for both f and g, and 0.0 for val.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.BlockConstraint","page":"Formulations","title":"PDMO.BlockConstraint","text":"BlockConstraint\n\nMaintains a collection of block variable indices and associated mappings.\n\nFields\n\nid::BlockID: A unique identifier for this constraint.\ninvolvedBlocks::Vector{BlockID}: A vector of block IDs; these indices are kept in increasing order.\nmappings::Dict{BlockID, AbstractMapping}: A dictionary mapping block IDs to their corresponding linear mappings.\nrhs::NumericVariable: The right-hand side of the constraint.\n\nThe constraint enforces a relationship of the form\n\nΣ (mapping(x[id])) = rhs\n\nover the blocks indexed in involvedBlocks. A default constructor is provided with id = -1, indicating an uninitialized state.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.MultiblockProblem","page":"Formulations","title":"PDMO.MultiblockProblem","text":"MultiblockProblem\n\nA container for a multiblock optimization problem. This structure maintains a collection of block variables and a collection of block constraints.\n\nFields\n\nblocks::Vector{BlockVariable}: A vector of block variables.\nconstraints::Vector{BlockConstraint}: A vector of block constraints.\n\nA default constructor is provided that initializes both collections as empty.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Block Component Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.addBlockVariable!","page":"Formulations","title":"PDMO.addBlockVariable!","text":"addBlockVariable!(mbp::MultiblockProblem, block::BlockVariable)\n\nAdd a block variable to the multiblock problem. If the block has a default ID, a new unique ID is assigned. Otherwise, the ID is checked for uniqueness.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.addBlockConstraint!","page":"Formulations","title":"PDMO.addBlockConstraint!","text":"addBlockConstraint!(mbp::MultiblockProblem, constraint::BlockConstraint)\n\nAdd a block constraint to the multiblock problem. If the constraint has a default ID, a new unique ID is assigned. Otherwise, the ID is checked for uniqueness.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.checkBlockVariableValidity","page":"Formulations","title":"PDMO.checkBlockVariableValidity","text":"checkBlockVariableValidity(block::BlockVariable; addScalarFunctionWrapper::Bool=true) -> Bool\n\nCheck if a block variable is valid for use in optimization problems.\n\nThis function performs several validation steps:\n\nChecks if the block ID is properly initialized (not negative for integer IDs)\nVerifies that f is a smooth function\nVerifies that g is a proximal function\nOptionally adds scalar function wrappers if needed\nTests function evaluations, gradients, and proximal operators\n\nArguments\n\nblock::BlockVariable: The block variable to validate\naddScalarFunctionWrapper::Bool=true: Whether to automatically add scalar function wrappers if needed\n\nReturns\n\nBool: true if the block is valid, false otherwise\n\nNotes\n\nErrors encountered during function evaluations are caught and reported, returning false\nDetailed error messages are printed to help diagnose issues\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.checkBlockConstraintValidity","page":"Formulations","title":"PDMO.checkBlockConstraintValidity","text":"checkBlockConstraintValidity(constr::BlockConstraint) -> Bool\n\nCheck if a block constraint is valid.\n\nArguments\n\nconstr::BlockConstraint: The constraint to check.\n\nReturns\n\nBool: true if the constraint is valid, false otherwise.\n\nNotes\n\nA valid constraint must:\nHave a properly initialized ID (not negative for integer IDs)\nHave matching lengths for involvedBlocks and mappings\nHave at least 2 blocks in involvedBlocks\nHave a mapping for each block in involvedBlocks\nDetailed error messages are printed for invalid constraints.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.checkMultiblockProblemValidity","page":"Formulations","title":"PDMO.checkMultiblockProblemValidity","text":"checkMultiblockProblemValidity(mbp::MultiblockProblem; addWrapper::Bool=true)\n\nCheck whether the given multiblock problem is valid.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem to check.\naddWrapper::Bool=true: Flag to add wrapper for scalar input functions.\n\nReturns\n\nBool: true if the problem is valid, false otherwise.\n\nImplementation Details\n\nThe function checks the validity of each block variable and constraint. It also attempts to evaluate the primal residuals of each constraint with the initial solution to ensure the problem is well-formed.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.checkMultiblockProblemFeasibility","page":"Formulations","title":"PDMO.checkMultiblockProblemFeasibility","text":"checkMultiblockProblemFeasibility(mbp::MultiblockProblem)\n\nCheck the feasibility of the current solution in the multiblock problem.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem to check.\n\nReturns\n\nFloat64: The L2 norm of the constraint violations.\nFloat64: The infinity norm of the constraint violations.\n\nImplementation Details\n\nThe function computes the constraint violations for each constraint using the current values in the block variables, and returns both the L2 and infinity norms of these violations.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.checkCompositeProblemValidity!","page":"Formulations","title":"PDMO.checkCompositeProblemValidity!","text":"checkCompositeProblemValidity!(mbp::MultiblockProblem)\n\nThe input is a mbp with (p+1) blocks: \n\nmin sum_{i=1}^p (f_i(x_i) + g_i(x_i)) + g_{p+1}(x_{p+1})\ns.t. A1x1 + A2x2 + ... + Apxp - x_{p+1} = 0\n\nThe function checks that if the input mbp is of the form described above. If x_{p+1} is      identified, this block wil be moved to the end of mbp.blocks. \n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem to check.\n\nReturns\n\nBool: true if the problem is a valid composite form, false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Graph Formulations","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Graph Data Types","category":"page"},{"location":"S4_api/formulations/#PDMO.MultiblockGraph","page":"Formulations","title":"PDMO.MultiblockGraph","text":"MultiblockGraph\n\nGraph representation of a multi-block optimization problem. Contains nodes representing variables and constraints, connected by edges.\n\nFields\n\nnodes::Dict{String, Node}: Dictionary mapping node IDs to Node objects\nedges::Dict{String, Edge}: Dictionary mapping edge IDs to Edge objects\ncolors::Dict{String, Int64}: Node coloring used for bipartiteness testing (0 or 1)\nisBipartite::Bool: Whether the graph is bipartite (updated by analysis functions)\nisConnected::Bool: Whether the graph is connected (updated by analysis functions)\n\nConstructors\n\nMultiblockGraph()\nMultiblockGraph(mbp::MultiblockProblem)\n\nGraph Structure\n\nThe graph represents the constraint structure of a multiblock optimization problem:\n\nVariable nodes: Represent optimization variable blocks\nConstraint nodes: Represent constraints involving more than 2 blocks\nTwo-block edges: Direct connections between variable nodes (2-block constraints)\nMulti-block edges: Connections from variable nodes to constraint nodes (>2-block constraints)\n\nExample\n\n# Create graph from a multiblock problem\nmbp = MultiblockProblem()\n# ... add blocks and constraints to mbp ...\ngraph = MultiblockGraph(mbp)\n\n# Analyze graph properties\nis_bipartite = isMultiblockGraphBipartite(graph)\nis_connected = isMultiblockGraphConnected(graph)\n\nUsage\n\nThis representation is particularly useful for:\n\nAlgorithm selection (e.g., choosing between different decomposition methods)\nProblem analysis (connectivity, bipartiteness)\nGraph-based optimization algorithms like ADMM\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.NodeType","page":"Formulations","title":"PDMO.NodeType","text":"NodeType\n\nAn enumeration defining the types of nodes in a multiblock graph.\n\nValues\n\nVARIABLE_NODE: Node representing a variable block from the original problem\nCONSTRAINT_NODE: Node representing a constraint from the original problem\n\nUsage\n\nNode types are used to distinguish between variable blocks and constraints in the graph representation. Variable nodes correspond to optimization variables, while constraint nodes represent constraints that involve more than two blocks.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.EdgeType","page":"Formulations","title":"PDMO.EdgeType","text":"EdgeType\n\nAn enumeration defining the types of edges in a multiblock graph.\n\nValues\n\nTWO_BLOCK_EDGE: Edge connecting exactly two variable nodes (representing a constraint involving two blocks)\nMULTIBLOCK_EDGE: Edge connecting a variable node to a constraint node (representing participation in a multi-block constraint)\n\nUsage\n\nEdge types distinguish between constraints that involve exactly two blocks (represented as direct edges between variable nodes) and constraints that involve more than two blocks (represented as edges from variable nodes to constraint nodes).\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.Node","page":"Formulations","title":"PDMO.Node","text":"Node\n\nA node in the multi-block graph representing either a variable block or a constraint.\n\nFields\n\nneighbors::Vector{String}: List of edge IDs connected to this node\ntype::NodeType: Type of node (VARIABLENODE or CONSTRAINTNODE)\nsource::BlockID: ID of the original variable block or constraint from the MultiblockProblem\n\nConstructor\n\nNode(neighbors::Vector{String}, type::NodeType, source::BlockID)\n\nUsage\n\nNodes are automatically created when constructing a MultiblockGraph from a MultiblockProblem. Variable nodes are created for each block variable, and constraint nodes are created for constraints involving more than two blocks.\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.Edge","page":"Formulations","title":"PDMO.Edge","text":"Edge\n\nAn edge in the multi-block graph connecting two nodes.\n\nFields\n\nnodeID1::String: ID of the first node connected by this edge\nnodeID2::String: ID of the second node connected by this edge (constraint node for MULTIBLOCK_EDGE)\ntype::EdgeType: Type of edge (TWOBLOCKEDGE or MULTIBLOCK_EDGE)\nsourceBlockConstraint::BlockID: ID of the constraint from the original problem that this edge represents\nsourceBlockVariable::BlockID: For MULTIBLOCKEDGE, the variable block ID; empty string for TWOBLOCK_EDGE\n\nConstructor\n\nEdge(nodeID1::String, nodeID2::String, type::EdgeType, sourceBlockConstraint::BlockID, sourceBlockVariable::BlockID)\n\nUsage\n\nEdges are automatically created when constructing a MultiblockGraph from a MultiblockProblem.\n\nTWOBLOCKEDGE connects two variable nodes directly (constraint involves exactly 2 blocks)\nMULTIBLOCK_EDGE connects a variable node to a constraint node (constraint involves >2 blocks)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Graph Construction Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.createNodeID","page":"Formulations","title":"PDMO.createNodeID","text":"createNodeID(id::BlockID; isConstraint::Bool=false) -> String\n\nGenerate a unique node ID string from a block or constraint ID.\n\nArguments\n\nid::BlockID: The block or constraint ID from the original MultiblockProblem\nisConstraint::Bool=false: If true, generates a constraint node ID; otherwise generates a variable node ID\n\nReturns\n\nA string ID that uniquely identifies the node in the graph\n\nExamples\n\n# Create a variable node ID\nvar_node_id = createNodeID(\"Block1\")  # Returns \"VariableNode(Block1)\"\n\n# Create a constraint node ID\nconstr_node_id = createNodeID(\"Constraint1\", isConstraint=true)  # Returns \"ConstraintNode(Constraint1)\"\n\nUsage\n\nThis function is used internally when constructing a MultiblockGraph to create unique identifiers for nodes. The generated IDs follow a consistent naming convention to distinguish between variable nodes and constraint nodes.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.createEdgeID","page":"Formulations","title":"PDMO.createEdgeID","text":"createEdgeID(constrID::BlockID; variableID::BlockID=\"\") -> String\n\nGenerate a unique edge ID string from constraint and variable IDs.\n\nArguments\n\nconstrID::BlockID: The constraint ID from the original MultiblockProblem\nvariableID::BlockID=\"\": The variable ID (empty for TWOBLOCKEDGE, specified for MULTIBLOCK_EDGE)\n\nReturns\n\nA string ID that uniquely identifies the edge in the graph\n\nExamples\n\n# Create a two-block edge ID (constraint involves exactly 2 blocks)\ntwo_block_edge_id = createEdgeID(\"Constraint1\")  # Returns \"TwoBlockEdge(Constraint1)\"\n\n# Create a multiblock edge ID (constraint involves >2 blocks)\nmulti_edge_id = createEdgeID(\"Constraint2\", variableID=\"Block1\")  # Returns \"MultiblockEdge(Constraint2, Block1)\"\n\nUsage\n\nThis function is used internally when constructing a MultiblockGraph to create unique identifiers for edges. The type of edge ID depends on whether a variableID is provided:\n\nNo variableID: TWOBLOCKEDGE (direct connection between two variable nodes)\nWith variableID: MULTIBLOCK_EDGE (connection from variable node to constraint node)\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Graph Analysis Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.numberNodes","page":"Formulations","title":"PDMO.numberNodes","text":"numberNodes(graph::MultiblockGraph) -> Int\n\nReturns the total number of nodes in the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\nThe number of nodes\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.numberEdges","page":"Formulations","title":"PDMO.numberEdges","text":"numberEdges(graph::MultiblockGraph) -> Int\n\nReturns the total number of edges in the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\nThe number of edges\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.numberEdgesByTypes","page":"Formulations","title":"PDMO.numberEdgesByTypes","text":"numberEdgesByTypes(graph::MultiblockGraph) -> Tuple{Int, Int}\n\nCounts the number of edges of each type in the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\nA tuple containing (count of two-block edges, count of multi-block edges)\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.getNodelNeighbors","page":"Formulations","title":"PDMO.getNodelNeighbors","text":"getNodelNeighbors(graph::MultiblockGraph) -> Dict{String, Set{String}}\n\nConstructs an adjacency list representation of the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\nA dictionary mapping each node ID to a set of its neighboring node IDs\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.isMultiblockGraphBipartite","page":"Formulations","title":"PDMO.isMultiblockGraphBipartite","text":"isMultiblockGraphBipartite(graph::MultiblockGraph) -> Bool\n\nDetermines if the graph is bipartite using a breadth-first search coloring algorithm. Updates the colors and isBipartite fields in the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\ntrue if the graph is bipartite, false otherwise\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.isMultiblockGraphConnected","page":"Formulations","title":"PDMO.isMultiblockGraphConnected","text":"isMultiblockGraphConnected(graph::MultiblockGraph) -> Bool\n\nDetermines if the graph is connected using a breadth-first search traversal. Updates the isConnected field in the graph.\n\nArguments\n\ngraph::MultiblockGraph: The graph to analyze\n\nReturns\n\ntrue if the graph is connected, false otherwise\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Bipartization Algorithms","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Bipartization Types","category":"page"},{"location":"S4_api/formulations/#PDMO.BipartizationAlgorithm","page":"Formulations","title":"PDMO.BipartizationAlgorithm","text":"BipartizationAlgorithm\n\nAn enumeration defining the available bipartization algorithms.\n\nValues\n\nMILP_BIPARTIZATION: Mixed Integer Linear Programming approach\nOptimal solution that minimizes operator norms and graph complexity\nSlowest but highest quality results\nUses HiGHS solver for optimization\nBFS_BIPARTIZATION: Breadth-First Search based approach\nFast heuristic using BFS traversal\nAssigns nodes to alternating partitions\nSplits edges when conflicts are detected\nDFS_BIPARTIZATION: Depth-First Search based approach  \nFast heuristic using DFS traversal\nSimilar to BFS but with different traversal order\nMay produce different partitioning results\nSPANNING_TREE_BIPARTIZATION: Spanning tree with smart back edge processing\nBalanced approach between quality and speed\nBuilds spanning tree and processes back edges intelligently\nMinimizes unnecessary edge splits\n\nUsage\n\nThese enum values are used with getBipartizationAlgorithmName() and as identifiers for selecting bipartization algorithms in optimization routines.\n\nAlgorithm Selection Guidelines\n\nUse MILP_BIPARTIZATION for optimal results when computational time is not critical\nUse BFS_BIPARTIZATION or DFS_BIPARTIZATION for fast heuristic solutions\nUse SPANNING_TREE_BIPARTIZATION for a good balance between speed and quality\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.BfsBipartization","page":"Formulations","title":"PDMO.BfsBipartization","text":"BfsBipartization(graph::MultiblockGraph, mbp::MultiblockProblem, \n                nodesAssignment::Dict{String, Int64}, \n                edgesSplitting::Dict{String, Tuple{Int64, Int64}})\n\nBipartization algorithm using Breadth-First Search (BFS).\n\nThis algorithm uses BFS traversal to quickly assign nodes to partitions and create a bipartite graph by splitting edges when conflicts are detected.\n\nArguments\n\ngraph::MultiblockGraph: The graph to bipartize\nmbp::MultiblockProblem: The original multiblock problem (not used in algorithm, included for interface consistency)\nnodesAssignment::Dict{String, Int64}: Dictionary to store node partition assignments (0 for left, 1 for right)\nedgesSplitting::Dict{String, Tuple{Int64, Int64}}: Dictionary to store edge splitting decisions (a,b) where:\na = 0: edge not split, a = 1: edge split\nb = 0: assigned to left partition, b = 1: assigned to right partition\n\nAlgorithm Steps\n\nNeighbor Construction: Build adjacency list from graph edges\nConnected Components: Process each connected component separately\nBFS Traversal: Starting from unvisited nodes, perform BFS with alternating partition assignment\nConflict Detection: When adjacent nodes would have the same partition, split the connecting edge\nEdge Processing: Set splitting decisions for all edges (split or keep)\n\nBFS Process\n\nQueue-Based: Uses FIFO queue for breadth-first traversal\nAlternating Assignment: Assigns nodes to alternating partitions (0, 1, 0, 1, ...)\nConflict Resolution: Splits edges when both endpoints would be in the same partition\nComponent Handling: Alternates starting partition for different connected components\n\nAdvantages\n\nFast Execution: O(V + E) time complexity where V = vertices, E = edges\nSimple Implementation: Straightforward algorithm with predictable behavior\nMemory Efficient: Minimal memory overhead beyond input graph\nHandles Disconnected Graphs: Automatically processes all connected components\n\nDisadvantages\n\nSuboptimal Results: May split more edges than necessary\nNo Operator Norm Consideration: Ignores numerical properties of the original problem\nTraversal Order Dependence: Results may depend on node iteration order\n\nUsage\n\nThis algorithm is recommended when:\n\nFast execution is more important than optimal results\nThe graph is large and MILP would be too slow\nA reasonable bipartite approximation is sufficient\nOperator norm considerations are not critical\n\nImplementation Notes\n\nProcesses connected components independently\nAlternates starting partition (0, 1, 0, 1, ...) for different components\nAll unprocessed edges are marked as not split (0, 0)\nThe dictionaries nodesAssignment and edgesSplitting are cleared and populated by the algorithm\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.MilpBipartization","page":"Formulations","title":"PDMO.MilpBipartization","text":"MilpBipartization(graph::MultiblockGraph, mbp::MultiblockProblem, \n                 nodesAssignment::Dict{String, Int64}, \n                 edgesSplitting::Dict{String, Tuple{Int64, Int64}})\n\nBipartization algorithm using Mixed Integer Linear Programming (MILP).\n\nThis algorithm formulates the graph bipartization problem as an optimization problem that  finds the optimal bipartite structure while minimizing operator norms and graph complexity.\n\nArguments\n\ngraph::MultiblockGraph: The graph to bipartize\nmbp::MultiblockProblem: The original multiblock problem (used for operator norm calculations)\nnodesAssignment::Dict{String, Int64}: Dictionary to store node partition assignments (0 for left, 1 for right)\nedgesSplitting::Dict{String, Tuple{Int64, Int64}}: Dictionary to store edge splitting decisions (a,b) where:\na = 0: edge not split, a = 1: edge split\nb = 0: assigned to left partition, b = 1: assigned to right partition\n\nAlgorithm Steps\n\nVariable Creation: Binary variables for node assignments and edge splitting decisions\nConstraint Formulation: Ensures each node belongs to exactly one partition\nBipartite Constraints: Prevents adjacent nodes from being in the same partition (or splits the edge)\nObjective Optimization: Minimizes operator norms and graph complexity\nSolution Extraction: Converts optimal solution to node assignments and edge splitting decisions\n\nMathematical Formulation\n\nVariables: x_node_L[i], x_node_R[i] (node assignments), z_edge[e] (edge splitting), x_edge_L[e], x_edge_R[e] (edge assignments)\nConstraints: Partition constraints, bipartite constraints, edge splitting logic\nObjective: min t_L + t_R + complexity_terms where t_L, t_R are operator norm bounds\n\nAdvantages\n\nOptimal Solution: Finds the best bipartization according to the objective function\nOperator Norm Awareness: Considers the numerical properties of the original problem\nPrincipled Approach: Mathematical optimization rather than heuristic\n\nDisadvantages\n\nComputational Cost: Slower than heuristic methods, especially for large graphs\nSolver Dependency: Requires HiGHS or another MILP solver\nMemory Usage: May require significant memory for large problems\n\nUsage\n\nThis algorithm is recommended when solution quality is more important than computational speed, particularly for problems where operator norm considerations are critical for numerical stability.\n\nImplementation Notes\n\nUses HiGHS solver with silent mode enabled\nAutomatically handles both TWOBLOCKEDGE and MULTIBLOCK_EDGE types\nOperator norms are computed using the operatorNorm2 function from the mappings\nThe dictionaries nodesAssignment and edgesSplitting are cleared and populated by the algorithm\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.DfsBipartization","page":"Formulations","title":"PDMO.DfsBipartization","text":"DfsBipartization(graph::MultiblockGraph, mbp::MultiblockProblem, \n                nodesAssignment::Dict{String, Int64}, \n                edgesSplitting::Dict{String, Tuple{Int64, Int64}})\n\nBipartization algorithm using Depth-First Search (DFS).\n\nThis algorithm uses DFS traversal to assign nodes to partitions and create a bipartite graph by splitting edges when conflicts are detected. Similar to BFS but with different traversal order.\n\nArguments\n\ngraph::MultiblockGraph: The graph to bipartize\nmbp::MultiblockProblem: The original multiblock problem (not used in algorithm, included for interface consistency)\nnodesAssignment::Dict{String, Int64}: Dictionary to store node partition assignments (0 for left, 1 for right)\nedgesSplitting::Dict{String, Tuple{Int64, Int64}}: Dictionary to store edge splitting decisions (a,b) where:\na = 0: edge not split, a = 1: edge split\nb = 0: assigned to left partition, b = 1: assigned to right partition\n\nAlgorithm Steps\n\nNeighbor Construction: Build adjacency list from graph edges\nConnected Components: Process each connected component separately\nDFS Traversal: Starting from unvisited nodes, perform DFS with alternating partition assignment\nConflict Detection: When adjacent nodes would have the same partition, split the connecting edge\nEdge Processing: Set splitting decisions for all edges (split or keep)\n\nDFS Process\n\nStack-Based: Uses LIFO stack for depth-first traversal\nAlternating Assignment: Assigns nodes to alternating partitions (0, 1, 0, 1, ...)\nConflict Resolution: Splits edges when both endpoints would be in the same partition\nComponent Handling: Alternates starting partition for different connected components\n\nAdvantages\n\nFast Execution: O(V + E) time complexity where V = vertices, E = edges\nSimple Implementation: Straightforward algorithm with predictable behavior\nMemory Efficient: Minimal memory overhead beyond input graph\nHandles Disconnected Graphs: Automatically processes all connected components\nDifferent Traversal Pattern: May produce different (sometimes better) results than BFS\n\nDisadvantages\n\nSuboptimal Results: May split more edges than necessary\nNo Operator Norm Consideration: Ignores numerical properties of the original problem\nTraversal Order Dependence: Results may depend on node iteration order\nStack Depth: May require significant stack space for deep graphs\n\nComparison with BFS\n\nTraversal Order: DFS explores deeply before backtracking, BFS explores level by level\nMemory Usage: DFS uses implicit recursion stack, BFS uses explicit queue\nResults: May produce different partitioning results for the same graph\nPerformance: Similar time complexity, but different memory access patterns\n\nUsage\n\nThis algorithm is recommended when:\n\nFast execution is more important than optimal results\nYou want to try a different heuristic than BFS\nThe graph structure might benefit from depth-first exploration\nOperator norm considerations are not critical\n\nImplementation Notes\n\nUses explicit stack with pop!() for LIFO behavior (vs popfirst!() for BFS)\nProcesses connected components independently\nAlternates starting partition (0, 1, 0, 1, ...) for different components\nAll unprocessed edges are marked as not split (0, 0)\nThe dictionaries nodesAssignment and edgesSplitting are cleared and populated by the algorithm\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.SpanningTreeBipartization","page":"Formulations","title":"PDMO.SpanningTreeBipartization","text":"SpanningTreeBipartization(graph::MultiblockGraph, mbp::MultiblockProblem, \n                         nodesAssignment::Dict{String, Int64}, \n                         edgesSplitting::Dict{String, Tuple{Int64, Int64}})\n\nBipartization algorithm using Spanning Tree with Smart Back Edge Processing.\n\nThis algorithm provides a balanced approach between solution quality and computational efficiency  by intelligently processing edges based on their role in the graph structure.\n\nArguments\n\ngraph::MultiblockGraph: The graph to bipartize\nmbp::MultiblockProblem: The original multiblock problem (not used in algorithm, included for interface consistency)\nnodesAssignment::Dict{String, Int64}: Dictionary to store node partition assignments (0 for left, 1 for right)\nedgesSplitting::Dict{String, Tuple{Int64, Int64}}: Dictionary to store edge splitting decisions (a,b) where:\na = 0: edge not split, a = 1: edge split\nb = 0: assigned to left partition, b = 1: assigned to right partition\n\nAlgorithm Steps\n\nSpanning Tree Construction: Build spanning tree using DFS traversal\nTree Coloring: 2-color the spanning tree (trees are always bipartite)\nEdge Classification: Classify edges as tree edges or back edges\nSmart Back Edge Processing: Only split back edges if endpoints have the same color\nEdge Decision Assignment: Set splitting decisions for all edges\n\nMathematical Foundation\n\nSpanning Tree Property: Any tree is bipartite and can be 2-colored\nBack Edge Analysis: A back edge preserves bipartiteness if its endpoints have different colors\nOptimal Splitting: Only splits edges that would violate bipartiteness\n\nAlgorithm Process\n\nTree Edges: Always kept intact (never split) since they maintain bipartite structure\nBack Edges with Different Colors: Kept intact (already valid for bipartite graph)\nBack Edges with Same Color: Split to avoid violating bipartiteness\nConnected Components: Each component processed independently\n\nAdvantages\n\nBalanced Approach: Good compromise between speed and solution quality\nMinimal Splitting: Splits only edges that truly violate bipartiteness\nDeterministic: Always produces the same result for the same graph\nHandles Disconnected Graphs: Automatically processes all connected components\nMathematically Sound: Based on solid graph theory principles\n\nDisadvantages\n\nNo Operator Norm Consideration: Ignores numerical properties of the original problem\nSpanning Tree Dependence: Results may depend on which spanning tree is chosen\nNot Globally Optimal: May not find the absolute minimum number of splits\n\nComparison with Other Algorithms\n\nvs MILP: Faster but may not be globally optimal\nvs BFS/DFS: Usually produces fewer edge splits due to intelligent edge processing\nvs Naive Spanning Tree: Much more efficient (doesn't split all back edges)\n\nUsage\n\nThis algorithm is recommended when:\n\nYou want a balance between solution quality and computational speed\nThe graph has many cycles (where back edge processing provides benefits)\nYou prefer a deterministic algorithm over heuristics\nOperator norm considerations are not critical\n\nImplementation Notes\n\nUses DFS to build spanning tree for each connected component\nMaintains separate sets for visited nodes and tree edges\n2-colors the spanning tree using BFS\nAll back edges are analyzed for potential splitting\nThe dictionaries nodesAssignment and edgesSplitting are cleared and populated by the algorithm\n\nTime Complexity\n\nSpanning Tree Construction: O(V + E)\nTree Coloring: O(V + E)\nBack Edge Processing: O(E)\nOverall: O(V + E) where V = vertices, E = edges\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"Bipartization Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.getBipartizationAlgorithmName","page":"Formulations","title":"PDMO.getBipartizationAlgorithmName","text":"getBipartizationAlgorithmName(alg::BipartizationAlgorithm) -> String\n\nReturns a human-readable string representation of a bipartization algorithm.\n\nArguments\n\nalg::BipartizationAlgorithm: The bipartization algorithm enum value\n\nReturns\n\nA string representing the algorithm name, or \"Unknown bipartization algorithm\" for invalid inputs\n\nExamples\n\nalg = BFS_BIPARTIZATION\nname = getBipartizationAlgorithmName(alg)  # Returns \"BFS_BIPARTIZATION\"\n\nalg = MILP_BIPARTIZATION\nname = getBipartizationAlgorithmName(alg)  # Returns \"MILP_BIPARTIZATION\"\n\nUsage\n\nThis function is useful for:\n\nLogging and debugging (identifying which algorithm is being used)\nUser interfaces (displaying algorithm names)\nReport generation (documenting algorithm choices)\nError messages and diagnostics\n\nSupported Algorithms\n\nMILP_BIPARTIZATION → \"MILP_BIPARTIZATION\"\nBFS_BIPARTIZATION → \"BFS_BIPARTIZATION\"  \nDFS_BIPARTIZATION → \"DFS_BIPARTIZATION\"\nSPANNING_TREE_BIPARTIZATION → \"SPANNINGTREEBIPARTIZATION\"\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"ADMM Bipartite Graph","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"ADMM Data Types","category":"page"},{"location":"S4_api/formulations/#PDMO.ADMMBipartiteGraph","page":"Formulations","title":"PDMO.ADMMBipartiteGraph","text":"ADMMBipartiteGraph\n\nA bipartite graph representation specifically designed for the Alternating Direction Method of Multipliers (ADMM) algorithm.\n\nThis structure transforms a general multiblock optimization problem into a bipartite graph that enables efficient ADMM decomposition. The bipartite structure ensures that variables can be updated in alternating fashion between the two partitions, which is essential for ADMM convergence properties.\n\nFields\n\nnodes::Dict{String, ADMMNode}: Dictionary mapping node IDs to ADMMNode objects\nContains original variable nodes, constraint nodes, and auxiliary split nodes\nEach node has associated optimization functions and partition assignment\nedges::Dict{String, ADMMEdge}: Dictionary mapping edge IDs to ADMMEdge objects\nRepresents linear constraints between nodes in the bipartite graph\nEach edge defines coupling relationships for ADMM algorithm\nmbpBlockID2admmNodeID::Dict{BlockID, String}: Mapping from original MultiblockProblem block IDs to ADMM node IDs\nEnables traceability between original problem formulation and ADMM representation\nUsed for solution extraction and result interpretation\nleft::Vector{String}: Node IDs assigned to the left partition (typically assignment = 0)\nright::Vector{String}: Node IDs assigned to the right partition (typically assignment = 1)\n\nBipartite Structure Properties\n\nPartition Guarantee: No edges exist between nodes in the same partition\nADMM Compatibility: Structure enables alternating updates between partitions\nConstraint Preservation: All original constraints are represented through edges\nAuxiliary Nodes: May contain additional nodes created during bipartization\n\nGraph Construction Process\n\nNode Creation: Transform MultiblockProblem blocks into ADMM nodes\nEdge Creation: Transform constraints into ADMM edges\nBipartization: Apply bipartization algorithm if necessary\nEdge Splitting: Create auxiliary nodes and edges to maintain bipartite structure\nPartition Assignment: Assign nodes to left/right partitions\nValidation: Verify bipartite property and constraint preservation\n\nConstructors\n\nADMMBipartiteGraph()  # Empty graph\nADMMBipartiteGraph(graph::MultiblockGraph, mbp::MultiblockProblem, nodesAssignment, edgesSplitting)\nADMMBipartiteGraph(graph::MultiblockGraph, mbp::MultiblockProblem, algorithm::BipartizationAlgorithm)\n\nUsage in ADMM Algorithm\n\nx-update: Update variables in one partition (typically left)\nz-update: Update variables in other partition (typically right)\nDual update: Update Lagrange multipliers associated with edges\nResidual computation: Compute primal and dual residuals using edge constraints\nConvergence check: Monitor constraint violations and variable changes\n\nMathematical Representation\n\nFor a bipartite graph with partitions L (left) and R (right):\n\nVariables: xL (left partition), xR (right partition)\nConstraints: Each edge (i,j) with i∈L, j∈R represents Ai xi + Aj xj = b_{ij}\nADMM updates alternate between optimizing over xL and xR\n\nExamples\n\n# Create from MultiblockProblem with specific algorithm\nmbp = MultiblockProblem()\n# ... add blocks and constraints ...\ngraph = MultiblockGraph(mbp)\nadmm_graph = ADMMBipartiteGraph(graph, mbp, BFS_BIPARTIZATION)\n\n# Access graph properties\nprintln(\"Left partition size: \", length(admm_graph.left))\nprintln(\"Right partition size: \", length(admm_graph.right))\nprintln(\"Number of constraints: \", length(admm_graph.edges))\n\nRelated Types\n\nADMMNode: Individual nodes in the bipartite graph\nADMMEdge: Edges representing constraints between nodes\nMultiblockGraph: Original graph representation before bipartization\nBipartizationAlgorithm: Algorithms for ensuring bipartite structure\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.ADMMNode","page":"Formulations","title":"PDMO.ADMMNode","text":"ADMMNode\n\nA node in the ADMM bipartite graph representation for the Alternating Direction Method of Multipliers.\n\nADMMNode represents either an original variable block from the multiblock problem or an auxiliary node created from edge splitting during bipartization. Each node contains optimization functions and  maintains connectivity information for the ADMM algorithm.\n\nFields\n\nf::AbstractFunction: Primary objective function component (smooth part of the objective)\ng::AbstractFunction: Secondary objective function component (non-smooth part, often regularization or constraint indicators)\nval::NumericVariable: Current value/estimate of the variable associated with this node\nneighbors::Set{String}: Set of edge IDs connected to this node in the bipartite graph\nconvertedEdgeID::String: Original edge ID from MultiblockGraph if this node was created by splitting an edge; empty string for original variable nodes\nassignment::Int: Partition assignment in the bipartite graph (0 for left partition, 1 for right partition)\n\nNode Types\n\nOriginal Variable Nodes: Represent variable blocks from the original MultiblockProblem\nHave non-trivial f and g functions from the original block\nconvertedEdgeID is empty string\nval initialized from original block value\nConstraint Nodes: Represent multi-block constraints from the original problem\nHave f = Zero() and g = IndicatorSumOfNVariables(...)\nCreated for constraints involving more than 2 blocks\nconvertedEdgeID is empty string\nSplit Edge Nodes: Auxiliary nodes created during bipartization\nCreated when edges need to be split to maintain bipartite structure\nHave specific function configurations depending on split type\nconvertedEdgeID contains the original edge ID\n\nConstructor\n\nADMMNode(f, g, val, neighbors, convertedEdgeID, assignment)\n\nUsage in ADMM\n\nLeft Partition: Typically contains variable nodes for x-update step\nRight Partition: Typically contains constraint nodes and auxiliary nodes for z-update step\nFunctions: Used in proximal operators during ADMM iterations\nConnectivity: Determines the coupling structure in ADMM decomposition\n\nExamples\n\n# Original variable node\nnode = ADMMNode(QuadraticFunction(...), IndicatorBox(...), x0, Set{String}(), \"\", 0)\n\n# Constraint node  \nnode = ADMMNode(Zero(), IndicatorSumOfNVariables(...), z0, Set{String}(), \"\", 1)\n\n# Split edge auxiliary node\nnode = ADMMNode(Zero(), IndicatorSumOfNVariables(2, rhs), aux_val, Set{String}(), \"EdgeID123\", 1)\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/#PDMO.ADMMEdge","page":"Formulations","title":"PDMO.ADMMEdge","text":"ADMMEdge\n\nAn edge in the ADMM bipartite graph connecting two nodes and representing a constraint in the ADMM decomposition.\n\nADMMEdge represents a linear constraint between two nodes in the bipartite graph. Each edge corresponds to either an original constraint from the multiblock problem or a constraint created during edge splitting for bipartization. The edge defines the coupling between variables in the ADMM algorithm.\n\nFields\n\nnodeID1::String: ID of the first node connected by this edge (typically from left partition)\nnodeID2::String: ID of the second node connected by this edge (typically from right partition)\nmappings::Dict{String, AbstractMapping}: Linear mappings for each node involved in the constraint\nKey: node ID, Value: linear mapping applied to that node's variable\nRepresents the coefficient matrices in the linear constraint\nrhs::NumericVariable: Right-hand side vector/value of the constraint represented by this edge\nsplittedEdgeID::String: Original edge ID from MultiblockGraph if this edge was created by splitting; empty string for original edges\n\nEdge Types\n\nOriginal Two-Block Edges: Direct constraints between two variable blocks\nRepresent constraints of the form A₁x₁ + A₂x₂ = b\nConnect two original variable nodes\nsplittedEdgeID is empty string\nMulti-Block Edges: Constraints involving constraint nodes\nRepresent constraints of the form Aᵢxᵢ - zⱼ = 0\nConnect a variable node to a constraint node\nsplittedEdgeID is empty string\nSplit Edges: Edges created from splitting original edges\nCreated during bipartization to maintain bipartite structure\nConnect original nodes to auxiliary split nodes\nsplittedEdgeID contains the original edge ID\n\nMathematical Representation\n\nEach edge represents a constraint: mapping[nodeID1] * x₁ + mapping[nodeID2] * x₂ = rhs\n\nFor node i with variable xᵢ, the constraint contribution is mappings[nodeIDᵢ](xᵢ)\nThe complete constraint equation must equal the rhs value\n\nConstructor\n\nADMMEdge(nodeID1, nodeID2, mappings, rhs, splittedEdgeID)\n\nUsage in ADMM\n\nConstraint Coupling: Defines how variables are coupled in the optimization problem\nDual Variables: Each edge corresponds to dual variables (Lagrange multipliers) in ADMM\nUpdate Steps: Used in both primal and dual update steps of the ADMM algorithm\nConvergence: Residuals computed using these constraint definitions\n\nExamples\n\n# Original two-block constraint: A₁x₁ + A₂x₂ = b\nmappings = Dict(\"node1\" => LinearMappingMatrix(A1), \"node2\" => LinearMappingMatrix(A2))\nedge = ADMMEdge(\"node1\", \"node2\", mappings, b, \"\")\n\n# Multi-block constraint connection: Aᵢxᵢ - zⱼ = 0\nmappings = Dict(\"var_node\" => LinearMappingMatrix(A), \"constr_node\" => LinearMappingExtraction(...))\nedge = ADMMEdge(\"var_node\", \"constr_node\", mappings, zeros(m), \"\")\n\n# Split edge constraint: Aᵢxᵢ - z₁ = 0 (from splitting)\nmappings = Dict(\"original_node\" => LinearMappingMatrix(A), \"split_node\" => LinearMappingExtraction(...))\nedge = ADMMEdge(\"original_node\", \"split_node\", mappings, zeros(m), \"OriginalEdgeID\")\n\n\n\n\n\n","category":"type"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"ADMM Construction Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.createADMMNodeID","page":"Formulations","title":"PDMO.createADMMNodeID","text":"createADMMNodeID(edgeID::String) -> String\n\nGenerate a unique node ID for an auxiliary node created from splitting an edge during bipartization.\n\nWhen an edge needs to be split to maintain bipartite structure, this function creates a consistent naming scheme for the new auxiliary node that will be inserted at the split point.\n\nArguments\n\nedgeID::String: The original edge ID from the MultiblockGraph that is being split\n\nReturns\n\nA string ID for the new ADMM auxiliary node following the pattern \"ADMMNodeConvertedFromEdge(edgeID)\"\n\nUsage\n\nThis function is used internally during the construction of ADMMBipartiteGraph when:\n\nA TWOBLOCKEDGE needs to be split due to bipartization decisions\nA MULTIBLOCK_EDGE needs to be split to maintain bipartite structure\nThe bipartization algorithm determines that an edge violates bipartiteness\n\nExamples\n\noriginal_edge_id = \"TwoBlockEdge(Constraint1)\"\nnew_node_id = createADMMNodeID(original_edge_id)\n# Returns: \"ADMMNodeConvertedFromEdge(TwoBlockEdge(Constraint1))\"\n\nmultiblock_edge_id = \"MultiblockEdge(Constraint2, Block1)\"  \naux_node_id = createADMMNodeID(multiblock_edge_id)\n# Returns: \"ADMMNodeConvertedFromEdge(MultiblockEdge(Constraint2, Block1))\"\n\nImplementation Notes\n\nThe generated ID uniquely identifies the auxiliary node\nThe ID preserves traceability back to the original edge\nUsed consistently across edge splitting operations\nPrevents ID conflicts with original variable and constraint nodes\n\nRelated Functions\n\ncreateADMMEdgeID: Creates IDs for new edges connected to split nodes\nADMMBipartiteGraph: Uses this function during edge splitting operations\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.createADMMEdgeID","page":"Formulations","title":"PDMO.createADMMEdgeID","text":"createADMMEdgeID(edgeID::String, nodeID::String) -> String\n\nGenerate a unique edge ID for a new edge created when splitting an original edge during bipartization.\n\nWhen an original edge is split to maintain bipartite structure, it is replaced by two or more new edges that connect through an auxiliary node. This function creates consistent IDs for these new edges.\n\nArguments\n\nedgeID::String: The original edge ID from the MultiblockGraph that was split\nnodeID::String: The ID of the node that this new edge connects to (either original node or auxiliary node)\n\nReturns\n\nA string ID for the new ADMM edge following the pattern \"ADMMEdgeSplittedFrom(edgeID, nodeID)\"\n\nUsage\n\nThis function is used internally during edge splitting operations when:\n\nAn original TWOBLOCKEDGE is split into two edges through an auxiliary node\nA MULTIBLOCK_EDGE is split to resolve bipartite violations\nMultiple new edges need to be created with consistent, traceable naming\n\nEdge Splitting Scenarios\n\nTWOBLOCKEDGE Split: Original edge (node1, node2) becomes:\nEdge1: (node1, auxNode) with ID \"ADMMEdgeSplittedFrom(originalEdgeID, node1)\"\nEdge2: (node2, auxNode) with ID \"ADMMEdgeSplittedFrom(originalEdgeID, node2)\"\nMULTIBLOCK_EDGE Split: Original edge (varNode, constrNode) becomes:\nEdge1: (varNode, auxNode) with ID \"ADMMEdgeSplittedFrom(originalEdgeID, varNode)\"\nEdge2: (constrNode, auxNode) with ID \"ADMMEdgeSplittedFrom(originalEdgeID, constrNode)\"\n\nExamples\n\n# Splitting a two-block edge\noriginal_edge = \"TwoBlockEdge(Constraint1)\"\nedge1_id = createADMMEdgeID(original_edge, \"VariableNode(Block1)\")\n# Returns: \"ADMMEdgeSplittedFrom(TwoBlockEdge(Constraint1), VariableNode(Block1))\"\n\nedge2_id = createADMMEdgeID(original_edge, \"VariableNode(Block2)\")  \n# Returns: \"ADMMEdgeSplittedFrom(TwoBlockEdge(Constraint1), VariableNode(Block2))\"\n\n# Connection to auxiliary node\naux_node_id = \"ADMMNodeConvertedFromEdge(TwoBlockEdge(Constraint1))\"\naux_edge_id = createADMMEdgeID(original_edge, aux_node_id)\n# Returns: \"ADMMEdgeSplittedFrom(TwoBlockEdge(Constraint1), ADMMNodeConvertedFromEdge(...))\"\n\nImplementation Notes\n\nPreserves full traceability back to the original edge and connected node\nEnsures unique IDs even when multiple edges are created from the same original edge\nUsed consistently across all edge splitting operations\nEnables reconstruction of the splitting history for debugging and analysis\n\nRelated Functions\n\ncreateADMMNodeID: Creates IDs for auxiliary nodes in edge splits\nADMMBipartiteGraph: Uses this function extensively during bipartization\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"JuMP Interface","category":"page"},{"location":"S4_api/formulations/","page":"Formulations","title":"Formulations","text":"JuMP Interface Functions","category":"page"},{"location":"S4_api/formulations/#PDMO.solveMultiblockProblemByJuMP","page":"Formulations","title":"PDMO.solveMultiblockProblemByJuMP","text":"solveMultiblockProblemByJuMP(mbp::MultiblockProblem)\n\nSolve a multiblock problem using JuMP and Ipopt.\n\nArguments\n\nmbp::MultiblockProblem: The multiblock problem to solve.\n\nReturns\n\nFloat64: The optimal objective value found by the solver.\n\nImplementation Details\n\nThe function:\n\nConverts the multiblock problem to a JuMP model\nSets up the Ipopt solver with HSL linear solver\nCreates JuMP variables for each block variable\nAdds constraints from the multiblock problem\nFormulates the objective function (handling both linear/quadratic and nonlinear terms)\nSolves the problem and returns the objective value\n\nCurrently, only LinearMappingMatrix and LinearMappingIdentity are supported for mappings.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.isSupportedObjectiveFunction","page":"Formulations","title":"PDMO.isSupportedObjectiveFunction","text":"isSupportedObjectiveFunction(f::AbstractFunction) -> Bool\n\nCheck if a function type is supported for JuMP objective formulation, including wrapped functions.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.isSupportedProximalFunction","page":"Formulations","title":"PDMO.isSupportedProximalFunction","text":"isSupportedProximalFunction(g::AbstractFunction) -> Bool\n\nCheck if a function type is supported for JuMP constraint formulation, including wrapped functions.\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.unwrapFunction","page":"Formulations","title":"PDMO.unwrapFunction","text":"unwrapFunction(f::AbstractFunction) -> (originalFunction, scaling, translation)\n\nUnwrap a function, returning the original function and transformation parameters. For non-wrapped functions, returns (f, 1.0, 0.0).\n\n\n\n\n\n","category":"function"},{"location":"S4_api/formulations/#PDMO.addBlockVariableToJuMPModel!","page":"Formulations","title":"PDMO.addBlockVariableToJuMPModel!","text":"addBlockVariableToJuMPModel!(model::JuMP.Model, \n                           f::AbstractFunction,\n                           g::AbstractFunction,\n                           x::NumericVariable,\n                           blockID::BlockID, \n                           var::Union{Dict{BlockID, Vector{JuMP.VariableRef}}, Dict{String, Vector{JuMP.VariableRef}}}, \n                           aux::Union{Dict{BlockID, Vector{JuMP.VariableRef}}, Dict{String, Vector{JuMP.VariableRef}}}, \n                           objExpressions::Vector{Union{JuMP.AffExpr, JuMP.QuadExpr}})\n\nAdd a block variable to a JuMP model.\n\nArguments\n\nmodel::JuMP.Model: The JuMP model to which the variable will be added.\nf::AbstractFunction: The smooth function component of the block variable.\ng::AbstractFunction: The nonsmooth function component of the block variable.\nx::NumericVariable: The current value of the block variable.\nblockID::BlockID: The ID of the block variable.\nvar::Union{Dict{BlockID, Vector{JuMP.VariableRef}}, Dict{String, Vector{JuMP.VariableRef}}}: Dictionary to store the created JuMP variables.\naux::Union{Dict{BlockID, Vector{JuMP.VariableRef}}, Dict{String, Vector{JuMP.VariableRef}}}: Dictionary to store auxiliary JuMP variables.\nobjExpressions::Vector{Union{JuMP.AffExpr, JuMP.QuadExpr}}: Vector to collect objective expressions.\n\nReturns\n\nBool: true if the block has a nonlinear smooth function, false otherwise.\n\nImplementation Details\n\nThe function creates JuMP variables and constraints based on the types of f and g.  It supports various function types including QuadraticFunction, AffineFunction,  ComponentwiseExponentialFunction for smooth functions, and IndicatorBox, IndicatorBallL2,  IndicatorSumOfNVariables, ElementwiseL1Norm, IndicatorHyperplane, IndicatorSOC for nonsmooth functions.\n\n\n\n\n\n","category":"function"},{"location":"#PDMO.jl-**Primal-Dual-Methods-for-Optimization**","page":"Home","title":"PDMO.jl - Primal-Dual Methods for Optimization","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PDMO.jl is a powerful Julia framework for primal-dual multiblock optimization, built for rapid prototyping and high-performance computing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Solve Complex Problems: Model and solve problems with multiple variable blocks and linear coupling constriants. \nHighly Customizable: An open-source toolkit that is easy to adapt for your applications and specific algorithms.\nAccelerate Research: Benchmark your methods against classic and","category":"page"},{"location":"","page":"Home","title":"Home","text":"state-of-the-art solvers.","category":"page"},{"location":"#Problem-Formulation","page":"Home","title":"Problem Formulation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PDMO.jl presents a unified framework for formulating and solving a MultiblockProblem of the form: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nmin_mathbfx quad  sum_j=1^n left( f_j(x_j) + g_j(x_j) right) \nmathrmst quad   mathbfA mathbfx = mathbfb\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where we have the following problem variables and data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginarrayccc\nntextbfBlock Variables quad  mtextbf Block Constraints quad  textbfBlock Matrix (m times n  textbflinear operators) \nmathbfx = beginbmatrix x_1  x_2  vdots  x_n endbmatrix quad  mathbfb = beginbmatrix b_1  b_2  vdots  b_m endbmatrix quad  mathbfA = beginbmatrix mathbfA_11  mathbfA_12  cdots  mathbfA_1n  mathbfA_21  mathbfA_22  cdots  mathbfA_2n  vdots  vdots  ddots  vdots  mathbfA_m1  mathbfA_m2  cdots  mathbfA_mn endbmatrix \nendarray","category":"page"},{"location":"","page":"Home","title":"Home","text":"More specifically, ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For each jin 1cdotsn, a BlockVariable x_j represents a numeric array (i.e., scalar, vector, matrix, etc.), and is associated with two objective functions: \neach f_j is differentiable, and f_j(cdot) and nabla f_j(cdot) are available; \neach g_j is proximable, and g_j(cdot) and textprox_gamma g_j(cdot) are available.\nFor each i in 1cdotsm, a BlockConstraint is defined by some linear operators and a right-hand side array: \nthe linear operator mathbfA_ij is non-zero if and only if constraint i involves blocks x_j;\nthe adjoint operator of mathbfA_ij is available;\nthe right-hand side b_i can be a numeric array of any shape. ","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PDMO.jl provides various algorithms to solve problems of the above form.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternating Direction Method of Multipliers (ADMM)\nGraph-based bipartization methods automatically generate ADMM-ready reformulations of MultiblockProblem.\nVarious ADMM variants are available: \nOriginal ADMM \nDoubly linearized ADMM \nAdaptive linearized ADMM \nVarious algorithmic component can be selected: \nPenalty adapters, e.g., Residual Balancing, Spectral Radius Approximation\nAccelerators, e.g., Halpern (with or without restart), Filtered Anderson\nAdaptive Primal-Dual Method (AdaPDM)\nA suite of efficient and adaptive methods for problems with simpler coupling, i.e., m=1, f_n = 0, and mathbfA_1 n = -mathrmId. \n  beginaligned\n  min_mathbfx quad  sum_j=1^n-1 left( f_j(x_j) + g_j(x_j) right) + g_n(mathbfA_11x_1 + cdots + mathbfA_1n-1x_n-1)\n  endaligned\nVarious methods can be selected: \nOriginal Condat-Vu\nAdaPDM \nAdaPDM+\nMalitsky-Pock","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"🧱 Unified Modeling: A versatile interface for structured problems.\n🔄 Automatic Decomposition: Intelligently analyzes and reformulates problems for supported algorithms.\n🧩 Extensible by Design: Easily add custom functions, constraints, and algorithms.\n📊 Modular Solvers: A rich library of classic and modern algorithms.\n⚡  Non-Convex Ready: Equipped with features to tackle non-convexity.","category":"page"},{"location":"#Roadmap-(Work-in-Progress)","page":"Home","title":"Roadmap (Work in Progress)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"🔍 Classification and detection for pathological problems\n🚀 Advanced acceleration techniques for first-order methods \n🤖 AI coding assistant for user-defined functions\n🛣️ Parallel, distributed, and GPU support.","category":"page"},{"location":"#Use-Cases","page":"Home","title":"Use Cases","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For Researchers Developing New Methods:\nA testbed for easily experimenting with your methods against existing ones \nA platform to track academic advances in first-order methods\nFor Optimization Users:\nAn open-sourced producet that is higly competitive\nEasy to use: unifying formulation, intuitive APIs, and comprehensive documentation\nModular design: minimal effort required to customize applications and algorithms","category":"page"},{"location":"#This-Documentation","page":"Home","title":"This Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Check out Getting Started for installation guide and your first optimization problem with PDMO.jl.\nLearn more about the theoretical foundations of ADMM and AdaPDM, and how to explore different algorithmic components for better performance. \nSee Examples for pre-defined templates of some classic applications and benchmark results.\nCheck out API References to implement and customize your own algorithms","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PDMO.jl is open source and welcomes contributions! Please contact info@mindopt.tech for more details.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently PDMO.jl is under the MIT License.","category":"page"}]
}
